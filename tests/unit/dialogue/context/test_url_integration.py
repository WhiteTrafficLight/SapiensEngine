"""
ì‹¤ì œ URLê³¼ LLMì„ ì‚¬ìš©í•œ í†µí•© í…ŒìŠ¤íŠ¸

Yale Journal URLì—ì„œ ì‹¤ì œë¡œ ë¶ˆë ›í¬ì¸íŠ¸ë¥¼ ì œëŒ€ë¡œ ì¶”ì¶œí•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
"""

import unittest
import sys
import os
import requests
from pathlib import Path
from bs4 import BeautifulSoup

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from src.dialogue.context.debate_context_manager import DebateContextManager
from src.models.llm.llm_manager import LLMManager


class TestURLIntegration(unittest.TestCase):
    """ì‹¤ì œ URLê³¼ LLMì„ ì‚¬ìš©í•œ í†µí•© í…ŒìŠ¤íŠ¸"""
    
    def setUp(self):
        """í…ŒìŠ¤íŠ¸ ì„¤ì •"""
        self.cnbc_url = "https://www.cnbc.com/2025/06/18/iran-threatens-irreparable-damage-if-us-enters-israel-conflict.html"
        
        # OpenAI API í‚¤ í™•ì¸
        if not os.environ.get('OPENAI_API_KEY'):
            self.skipTest("OpenAI API key not found - set OPENAI_API_KEY environment variable")
    
    def test_cnbc_news_bullet_point_extraction(self):
        """CNBC ë‰´ìŠ¤ì—ì„œ ì‹¤ì œ ë¶ˆë ›í¬ì¸íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""
        print(f"\n=== CNBC ë‰´ìŠ¤ ë¶ˆë ›í¬ì¸íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ===")
        print(f"URL: {self.cnbc_url}")
        
        try:
            # 1. URLì—ì„œ ì»¨í…ì¸  ì¶”ì¶œ
            print("\nğŸ“„ URL ì»¨í…ì¸  ì¶”ì¶œ ì¤‘...")
            content = self._extract_url_content(self.cnbc_url)
            print(f"âœ… ì»¨í…ì¸  ì¶”ì¶œ ì™„ë£Œ: {len(content):,}ì")
            
            # ì»¨í…ì¸ ê°€ ì‹¤ì œë¡œ ì¶”ì¶œë˜ì—ˆëŠ”ì§€ í™•ì¸
            self.assertGreater(len(content), 500, "ì¶”ì¶œëœ ì»¨í…ì¸ ê°€ ë„ˆë¬´ ì§§ìŒ")
            
            # CNBC ë‰´ìŠ¤ ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨ í™•ì¸
            content_lower = content.lower()
            self.assertTrue(
                any(keyword in content_lower for keyword in ["iran", "israel", "conflict", "trump"]),
                "CNBC ë‰´ìŠ¤ ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì§€ ì•ŠìŒ"
            )
            
            # 2. ContextManagerë¡œ ê°ê´€ì  ìš”ì•½ ìƒì„±
            print("\nğŸ“Š ê°ê´€ì  ìš”ì•½ ìƒì„± ì¤‘...")
            llm_manager = LLMManager()
            context_manager = DebateContextManager(llm_manager)
            
            # ì»¨í…ì¸ ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì¶”ê°€
            context_manager.add_text_context(content, "CNBC News Article")
            
            # í† ë¡  ì£¼ì œ ì„¤ì •
            topic = "Should the U.S. take military action in Middle East conflicts?"
            
            # ê°ê´€ì  ìš”ì•½ ìƒì„±
            summary = context_manager.get_objective_summary(
                topic=topic,
                context_type="news_article"
            )
            
            print("âœ… ê°ê´€ì  ìš”ì•½ ìƒì„± ì™„ë£Œ")
            
            # 3. ê²°ê³¼ ê²€ì¦
            self._validate_summary_results(summary, content, topic)
            
            # 4. ë¶ˆë ›í¬ì¸íŠ¸ í˜•íƒœ í™•ì¸
            bullet_points = self._extract_bullet_points_from_summary(summary)
            self._validate_bullet_points(bullet_points)
            
            # 5. ê²°ê³¼ ì¶œë ¥
            self._print_results(content, summary, bullet_points)
            
            print("\nâœ… ëª¨ë“  ê²€ì¦ ì™„ë£Œ!")
            
        except Exception as e:
            self.fail(f"í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
    
    def _extract_url_content(self, url: str) -> str:
        """URLì—ì„œ í…ìŠ¤íŠ¸ ì»¨í…ì¸  ì¶”ì¶œ"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }
            
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ íƒœê·¸ ì œê±°
            for script in soup(["script", "style"]):
                script.extract()
            
            text = soup.get_text(separator='\n')
            
            # ì—¬ëŸ¬ ì¤„ë°”ê¿ˆ ì •ë¦¬
            import re
            text = re.sub(r'\n{3,}', '\n\n', text)
            text = re.sub(r'\s{3,}', ' ', text)
            
            return text.strip()
            
        except Exception as e:
            raise Exception(f"URL ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
    
    def _validate_summary_results(self, summary: str, original_content: str, topic: str):
        """ìš”ì•½ ê²°ê³¼ ê²€ì¦"""
        # ìš”ì•½ì´ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
        self.assertIsInstance(summary, str)
        self.assertGreater(len(summary), 100, "ìš”ì•½ì´ ë„ˆë¬´ ì§§ìŒ")
        self.assertLess(len(summary), len(original_content), "ìš”ì•½ì´ ì›ë³¸ë³´ë‹¤ ê¸¸ë©´ ì•ˆë¨")
        
        # í•µì‹¬ í‚¤ì›Œë“œ í¬í•¨ í™•ì¸ (ë‰´ìŠ¤ ê¸°ì‚¬ í‚¤ì›Œë“œë¡œ ë³€ê²½)
        summary_lower = summary.lower()
        expected_keywords = ["iran", "israel", "conflict", "military", "trump", "khamenei"]
        found_keywords = [kw for kw in expected_keywords if kw in summary_lower]
        
        self.assertGreater(len(found_keywords), 1, 
                         f"ê´€ë ¨ í‚¤ì›Œë“œê°€ ì¶©ë¶„íˆ í¬í•¨ë˜ì§€ ì•ŠìŒ. ë°œê²¬: {found_keywords}")
    
    def _extract_bullet_points_from_summary(self, summary: str) -> list:
        """ìš”ì•½ì—ì„œ ë¶ˆë ›í¬ì¸íŠ¸ ì¶”ì¶œ"""
        import re
        
        bullet_points = []
        for line in summary.split('\n'):
            line = line.strip()
            if line and (line.startswith('â€¢') or line.startswith('-') or line.startswith('*')):
                bullet_points.append(line)
        
        return bullet_points
    
    def _validate_bullet_points(self, bullet_points: list):
        """ë¶ˆë ›í¬ì¸íŠ¸ ê²€ì¦ (ë‰´ìŠ¤ ê¸°ì‚¬ìš©)"""
        # ì ì ˆí•œ ê°œìˆ˜ì˜ ë¶ˆë ›í¬ì¸íŠ¸
        self.assertGreater(len(bullet_points), 2, "ìµœì†Œ 3ê°œ ì´ìƒì˜ ë¶ˆë ›í¬ì¸íŠ¸ê°€ í•„ìš”")
        self.assertLess(len(bullet_points), 8, "ë¶ˆë ›í¬ì¸íŠ¸ê°€ ë„ˆë¬´ ë§ìŒ")
        
        # ê° ë¶ˆë ›í¬ì¸íŠ¸ì˜ ë‚´ìš© ê²€ì¦ (ë‰´ìŠ¤ ê¸°ì‚¬ íŠ¹ì„± ê³ ë ¤)
        for point in bullet_points:
            self.assertGreater(len(point), 15, f"ë¶ˆë ›í¬ì¸íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ: {point}")
            self.assertLess(len(point), 500, f"ë¶ˆë ›í¬ì¸íŠ¸ê°€ ë„ˆë¬´ ê¹€: {point}")  # ë‰´ìŠ¤ ê¸°ì‚¬ë„ ìƒì„¸í•œ ì„¤ëª… í—ˆìš©
    
    def _print_results(self, content: str, summary: str, bullet_points: list):
        """ê²°ê³¼ ì¶œë ¥"""
        print(f"\n=== ê²°ê³¼ ìš”ì•½ ===")
        print(f"ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(content):,}ì")
        print(f"ìš”ì•½ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(summary):,}ì")
        print(f"ì••ì¶• ë¹„ìœ¨: {len(summary)/len(content)*100:.1f}%")
        print(f"ë¶ˆë ›í¬ì¸íŠ¸ ê°œìˆ˜: {len(bullet_points)}ê°œ")
        
        print(f"\n=== ìƒì„±ëœ ìš”ì•½ ===")
        # ìš”ì•½ì´ ê¸¸ë©´ ì¼ë¶€ë§Œ ì¶œë ¥
        if len(summary) > 1000:
            print(summary[:1000] + "\n... (ìš”ì•½ ê³„ì†)")
        else:
            print(summary)
        
        print(f"\n=== ì¶”ì¶œëœ ë¶ˆë ›í¬ì¸íŠ¸ ===")
        for i, point in enumerate(bullet_points, 1):
            print(f"{i}. {point}")


class TestURLExtractionOnly(unittest.TestCase):
    """URL ì¶”ì¶œë§Œ í…ŒìŠ¤íŠ¸ (LLM ì—†ì´)"""
    
    def test_cnbc_content_extraction(self):
        """CNBC ì»¨í…ì¸  ì¶”ì¶œë§Œ í…ŒìŠ¤íŠ¸"""
        url = "https://www.cnbc.com/2025/06/18/iran-threatens-irreparable-damage-if-us-enters-israel-conflict.html"
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            text = soup.get_text()
            
            # ê¸°ë³¸ ê²€ì¦
            self.assertGreater(len(text), 1000)
            self.assertIn("iran", text.lower())
            self.assertIn("israel", text.lower())
            
            print(f"âœ… CNBC ì»¨í…ì¸  ì¶”ì¶œ ì„±ê³µ: {len(text):,}ì")
            
        except Exception as e:
            self.fail(f"URL ì»¨í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")


if __name__ == '__main__':
    # API í‚¤ í™•ì¸
    if not os.environ.get('OPENAI_API_KEY'):
        print("âš ï¸  OpenAI API key not found")
        print("Set OPENAI_API_KEY environment variable to run LLM tests")
        print("Example: export OPENAI_API_KEY='your-api-key-here'")
        print("\nRunning URL extraction test only...")
        
        # LLM ì—†ì´ URL ì¶”ì¶œë§Œ í…ŒìŠ¤íŠ¸
        suite = unittest.TestSuite()
        suite.addTest(TestURLExtractionOnly('test_cnbc_content_extraction'))
        runner = unittest.TextTestRunner(verbosity=2)
        runner.run(suite)
    else:
        print("âœ… OpenAI API key found")
        print("Running full integration test...")
        
        # ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        unittest.main(verbosity=2) 