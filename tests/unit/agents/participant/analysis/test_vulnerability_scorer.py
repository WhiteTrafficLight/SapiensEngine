"""
Unit tests for VulnerabilityScorer class.
"""

import pytest
import json
from unittest.mock import Mock, patch
from typing import Dict, Any

from src.agents.participant.analysis.vulnerability_scorer import VulnerabilityScorer


class TestVulnerabilityScorer:
    """Test cases for VulnerabilityScorer class."""
    
    @pytest.fixture
    def mock_llm_manager(self):
        """Mock LLM manager fixture."""
        mock_llm = Mock()
        mock_llm.generate_response.return_value = '{"conceptual_clarity": 0.6, "logical_leap": 0.4, "overgeneralization": 0.5, "emotional_appeal": 0.3, "lack_of_concrete_evidence": 0.7}'
        return mock_llm
    
    @pytest.fixture
    def philosopher_data(self):
        """Sample philosopher data fixture."""
        return {
            "name": "Socrates",
            "essence": "Know thyself",
            "debate_style": "Questioning",
            "personality": "Curious",
            "key_traits": ["wisdom", "humility"],
            "quote": "I know that I know nothing",
            "vulnerability_sensitivity": {
                "conceptual_clarity": 0.8,
                "logical_leap": 0.7,
                "overgeneralization": 0.6,
                "emotional_appeal": 0.5,
                "lack_of_concrete_evidence": 0.9
            }
        }
    
    @pytest.fixture
    def vulnerability_scorer(self, mock_llm_manager, philosopher_data):
        """VulnerabilityScorer instance fixture."""
        return VulnerabilityScorer(
            llm_manager=mock_llm_manager,
            agent_id="test_socrates",
            philosopher_name="Socrates",
            philosopher_data=philosopher_data
        )
    
    @pytest.fixture
    def sample_argument(self):
        """Sample argument fixture."""
        return {
            "claim": "AI will replace all human jobs",
            "evidence": "Automation is increasing in many industries",
            "reasoning": "Technology always replaces human labor",
            "assumptions": ["technological progress is inevitable", "jobs cannot adapt"]
        }
    
    def test_initialization(self, vulnerability_scorer, mock_llm_manager, philosopher_data):
        """Test VulnerabilityScorer initialization."""
        assert vulnerability_scorer.llm_manager == mock_llm_manager
        assert vulnerability_scorer.agent_id == "test_socrates"
        assert vulnerability_scorer.philosopher_name == "Socrates"
        assert vulnerability_scorer.philosopher_data == philosopher_data
    
    def test_score_single_argument_success(self, vulnerability_scorer, sample_argument):
        """Test successful single argument scoring."""
        full_context = "AI technology is advancing rapidly and will impact employment."
        
        # Mock the detailed vulnerabilities analysis
        mock_detailed_vulnerabilities = {
            "conceptual_clarity": 0.6,
            "logical_leap": 0.4,
            "overgeneralization": 0.5,
            "emotional_appeal": 0.3,
            "lack_of_concrete_evidence": 0.7
        }
        vulnerability_scorer.analyze_detailed_vulnerabilities = Mock(
            return_value=mock_detailed_vulnerabilities
        )
        
        # Mock the personalized vulnerability calculation
        vulnerability_scorer.calculate_personalized_vulnerability = Mock(
            return_value=0.55
        )
        
        # Mock the basic scores
        mock_basic_scores = {
            "logical_strength": 0.6,
            "evidence_quality": 0.5,
            "relevance": 0.8
        }
        vulnerability_scorer.get_basic_argument_scores = Mock(
            return_value=mock_basic_scores
        )
        
        result = vulnerability_scorer.score_single_argument(sample_argument, full_context)
        
        assert "logical_strength" in result
        assert "evidence_quality" in result
        assert "relevance" in result
        assert "final_vulnerability" in result
        assert "overall_score" in result
        assert result["final_vulnerability"] == 0.55
        
        # Verify method calls
        vulnerability_scorer.analyze_detailed_vulnerabilities.assert_called_once_with(
            sample_argument, full_context
        )
        vulnerability_scorer.calculate_personalized_vulnerability.assert_called_once_with(
            mock_detailed_vulnerabilities
        )
        vulnerability_scorer.get_basic_argument_scores.assert_called_once_with(
            sample_argument, full_context
        )
    
    def test_analyze_detailed_vulnerabilities_success(self, vulnerability_scorer, sample_argument):
        """Test successful detailed vulnerabilities analysis."""
        full_context = "AI technology discussion context"
        
        result = vulnerability_scorer.analyze_detailed_vulnerabilities(sample_argument, full_context)
        
        assert "conceptual_clarity" in result
        assert "logical_leap" in result
        assert "overgeneralization" in result
        assert "emotional_appeal" in result
        assert "lack_of_concrete_evidence" in result
        
        # All values should be between 0.0 and 1.0
        for key, value in result.items():
            assert 0.0 <= value <= 1.0
        
        # Verify LLM was called
        vulnerability_scorer.llm_manager.generate_response.assert_called_once()
    
    def test_analyze_detailed_vulnerabilities_json_parse_error(self, vulnerability_scorer, sample_argument):
        """Test detailed vulnerabilities analysis with JSON parse error."""
        # Mock LLM to return invalid JSON
        vulnerability_scorer.llm_manager.generate_response.return_value = "Invalid JSON response"
        
        result = vulnerability_scorer.analyze_detailed_vulnerabilities(sample_argument, "context")
        
        # Should return default values
        expected_defaults = {
            "conceptual_clarity": 0.5,
            "logical_leap": 0.5,
            "overgeneralization": 0.5,
            "emotional_appeal": 0.5,
            "lack_of_concrete_evidence": 0.5
        }
        assert result == expected_defaults
    
    def test_analyze_detailed_vulnerabilities_llm_error(self, vulnerability_scorer, sample_argument):
        """Test detailed vulnerabilities analysis with LLM error."""
        # Mock LLM to raise an exception
        vulnerability_scorer.llm_manager.generate_response.side_effect = Exception("LLM failed")
        
        result = vulnerability_scorer.analyze_detailed_vulnerabilities(sample_argument, "context")
        
        # Should return default values
        expected_defaults = {
            "conceptual_clarity": 0.5,
            "logical_leap": 0.5,
            "overgeneralization": 0.5,
            "emotional_appeal": 0.5,
            "lack_of_concrete_evidence": 0.5
        }
        assert result == expected_defaults
    
    def test_calculate_personalized_vulnerability_with_sensitivity(self, vulnerability_scorer):
        """Test personalized vulnerability calculation with sensitivity data."""
        detailed_vulnerabilities = {
            "conceptual_clarity": 0.6,
            "logical_leap": 0.4,
            "overgeneralization": 0.5,
            "emotional_appeal": 0.3,
            "lack_of_concrete_evidence": 0.7
        }
        
        result = vulnerability_scorer.calculate_personalized_vulnerability(detailed_vulnerabilities)
        
        # Should be a weighted average based on philosopher's sensitivity
        assert 0.0 <= result <= 1.0
        assert isinstance(result, float)
    
    def test_calculate_personalized_vulnerability_no_sensitivity(self, vulnerability_scorer):
        """Test personalized vulnerability calculation without sensitivity data."""
        # Remove sensitivity data
        vulnerability_scorer.philosopher_data = {"name": "Test"}
        
        detailed_vulnerabilities = {
            "conceptual_clarity": 0.6,
            "logical_leap": 0.4,
            "overgeneralization": 0.5,
            "emotional_appeal": 0.3,
            "lack_of_concrete_evidence": 0.7
        }
        
        result = vulnerability_scorer.calculate_personalized_vulnerability(detailed_vulnerabilities)
        
        # Should return average of all vulnerabilities
        expected_avg = sum(detailed_vulnerabilities.values()) / len(detailed_vulnerabilities)
        assert result == expected_avg
    
    def test_calculate_personalized_vulnerability_error(self, vulnerability_scorer):
        """Test personalized vulnerability calculation with error handling."""
        # Pass data that will cause division by zero in the fallback
        detailed_vulnerabilities = {}
        
        result = vulnerability_scorer.calculate_personalized_vulnerability(detailed_vulnerabilities)
        
        # Should handle error gracefully and return a default value
        assert 0.0 <= result <= 1.0
    
    def test_get_basic_argument_scores_success(self, vulnerability_scorer, sample_argument):
        """Test successful basic argument scores retrieval."""
        # Mock LLM response for basic scores
        vulnerability_scorer.llm_manager.generate_response.return_value = '{"logical_strength": 0.7, "evidence_quality": 0.6, "relevance": 0.8}'
        
        result = vulnerability_scorer.get_basic_argument_scores(sample_argument, "context")
        
        assert "logical_strength" in result
        assert "evidence_quality" in result
        assert "relevance" in result
        
        # All values should be between 0.0 and 1.0
        for key, value in result.items():
            assert 0.0 <= value <= 1.0
    
    def test_get_basic_argument_scores_json_parse_error(self, vulnerability_scorer, sample_argument):
        """Test basic argument scores with JSON parse error."""
        # Mock LLM to return invalid JSON
        vulnerability_scorer.llm_manager.generate_response.return_value = "Invalid JSON"
        
        result = vulnerability_scorer.get_basic_argument_scores(sample_argument, "context")
        
        # Should return default values
        expected_defaults = {
            "logical_strength": 0.5,
            "evidence_quality": 0.5,
            "relevance": 0.5
        }
        assert result == expected_defaults
    
    def test_get_basic_argument_scores_llm_error(self, vulnerability_scorer, sample_argument):
        """Test basic argument scores with LLM error."""
        # Mock LLM to raise an exception
        vulnerability_scorer.llm_manager.generate_response.side_effect = Exception("LLM failed")
        
        result = vulnerability_scorer.get_basic_argument_scores(sample_argument, "context")
        
        # Should return default values
        expected_defaults = {
            "logical_strength": 0.5,
            "evidence_quality": 0.5,
            "relevance": 0.5
        }
        assert result == expected_defaults
    
    @pytest.mark.parametrize("vulnerability_scores,expected_range", [
        ({"conceptual_clarity": 0.0, "logical_leap": 0.0, "overgeneralization": 0.0, "emotional_appeal": 0.0, "lack_of_concrete_evidence": 0.0}, (0.0, 0.1)),
        ({"conceptual_clarity": 1.0, "logical_leap": 1.0, "overgeneralization": 1.0, "emotional_appeal": 1.0, "lack_of_concrete_evidence": 1.0}, (0.9, 1.0)),
        ({"conceptual_clarity": 0.5, "logical_leap": 0.5, "overgeneralization": 0.5, "emotional_appeal": 0.5, "lack_of_concrete_evidence": 0.5}, (0.4, 0.6)),
    ])
    def test_calculate_personalized_vulnerability_ranges(self, vulnerability_scorer, vulnerability_scores, expected_range):
        """Test personalized vulnerability calculation with different score ranges."""
        result = vulnerability_scorer.calculate_personalized_vulnerability(vulnerability_scores)
        
        assert expected_range[0] <= result <= expected_range[1]
    
    def test_score_single_argument_integration(self, vulnerability_scorer, sample_argument):
        """Test complete integration of score_single_argument method."""
        full_context = "Complete context for testing"
        
        result = vulnerability_scorer.score_single_argument(sample_argument, full_context)
        
        # Check all required fields are present
        required_fields = [
            "logical_strength", "evidence_quality", "relevance",
            "conceptual_clarity", "logical_leap", "overgeneralization",
            "emotional_appeal", "lack_of_concrete_evidence",
            "final_vulnerability", "overall_score"
        ]
        
        for field in required_fields:
            assert field in result
            assert 0.0 <= result[field] <= 1.0 