import os
import json
import time
import logging
import re
import requests
from typing import Dict, Any, List, Optional, Union, Tuple
import openai
import anthropic
from dotenv import load_dotenv, dotenv_values
import chromadb
from chromadb.utils import embedding_functions

from src.utils.config.config_loader import ConfigLoader
from src.utils.context_manager import UserContextManager

# Load environment variables
load_dotenv(override=True)  # Force override existing environment variables with .env values

logger = logging.getLogger(__name__)

class LLMManager:
    """
    Manages interactions with language models (OpenAI or Anthropic)
    for philosophical dialogue generation
    """
    
    def __init__(self, config: Union[Dict[str, Any], ConfigLoader] = None):
        """Initialize the LLM Manager"""
        # Get configuration
        if isinstance(config, ConfigLoader):
            self.config_loader = config
            self.llm_config = self.config_loader.get_main_config().get("llm", {})
        elif isinstance(config, dict):
            self.config_loader = ConfigLoader()
            self.llm_config = config
        else:
            self.config_loader = ConfigLoader()
            self.llm_config = self.config_loader.get_main_config().get("llm", {})
            
        # ğŸ†• ê°•ì œ ì¤‘ë‹¨ ì‹œê·¸ë„
        self._force_stop_signal = False
        self._active_requests = set()  # ì§„í–‰ ì¤‘ì¸ ìš”ì²­ë“¤ ì¶”ì 
        
        # âœ… ì»¨í…ìŠ¤íŠ¸ë³„ LLM ì„¤ì • (í† í° ìµœì í™”ìš©)
        self.context_configs = {
            # ê³ í’ˆì§ˆ í•„ìš” (ì°½ì˜ì„±, ë…¼ë¦¬ì„±)
            "opening_argument": {"model": "gpt-4o", "max_tokens": 8000},
            "conclusion": {"model": "gpt-4o", "max_tokens": 6000},
            
            # ì¤‘ê°„ í’ˆì§ˆ (ë¶„ì„, ì „ëµ)
            "attack_strategy": {"model": "gpt-4o", "max_tokens": 4000},
            "defense_response": {"model": "gpt-4o", "max_tokens": 3000},
            "argument_analysis": {"model": "gpt-4o", "max_tokens": 3000},
            
            # ë¹ ë¥¸ ì²˜ë¦¬ (ê°„ë‹¨í•œ ì‘ì—…)
            "interactive_response": {"model": "gpt-4o", "max_tokens": 1500},
            "rag_query_generation": {"model": "gpt-4o", "max_tokens": 500},
            "keyword_extraction": {"model": "gpt-4o", "max_tokens": 200},
            
            # ê¸°ë³¸ê°’
            "default": {"model": "gpt-4o", "max_tokens": 4000}
        }
            
        # Get API keys from .env file directly to ensure we use the correct values
        env_values = dotenv_values()
        self.openai_api_key = env_values.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY")
        self.anthropic_api_key = env_values.get("ANTHROPIC_API_KEY") or os.getenv("ANTHROPIC_API_KEY")
            
        # Set up RAG paths
        self.rag_paths = {
            "aristotle": os.path.join("data", "vectordbs", "aristotle"),
            "plato": os.path.join("data", "vectordbs", "plato"),
            "socrates": os.path.join("data", "vectordbs", "socrates"),
            "kant": os.path.join("data", "vectordbs", "kant"),
            "hegel": os.path.join("data", "vectordbs", "hegel"),
            "nietzsche": os.path.join("data", "vectordbs", "nietzsche"),
            "beauvoir": os.path.join("data", "vectordbs", "beauvoir"),
            "arendt": os.path.join("data", "vectordbs", "arendt"),
            "butler": os.path.join("data", "vectordbs", "butler"),
        }
        
        # Set up RAG collection names
        self.rag_collections = {
            "aristotle": "langchain",
            "plato": "langchain",
            "socrates": "langchain",
            "kant": "langchain",
            "hegel": "langchain", 
            "nietzsche": "langchain",
            "beauvoir": "langchain",
            "arendt": "langchain",
            "butler": "langchain",
        }
        
        # Setup LLM clients
        self._setup_clients()
        
        # Initialize context manager
        self.context_manager = UserContextManager()
        
        # NPC cache for RAG data - key is NPC ID, value is a dict with RAG config
        self.npc_rag_cache = {}
        
        logger.info(f"Initialized LLM Manager with provider: {self.llm_config.get('provider', 'openai')}, model: {self.llm_config.get('model', 'gpt-4')}")
        
        # Print a masked version of the API key for debugging
        if self.openai_api_key:
            masked_key = self.openai_api_key[:4] + '*' * (len(self.openai_api_key) - 8) + self.openai_api_key[-4:]
            logger.info(f"Using OpenAI API key: {masked_key}")
        elif self.anthropic_api_key:
            masked_key = self.anthropic_api_key[:4] + '*' * (len(self.anthropic_api_key) - 8) + self.anthropic_api_key[-4:]
            logger.info(f"Using Anthropic API key: {masked_key}")
        else:
            logger.warning(f"No API key found for provider: {self.llm_config.get('provider', 'openai')}")
    
    def cancel_all_requests(self):
        """ëª¨ë“  ì§„í–‰ ì¤‘ì¸ LLM ìš”ì²­ì„ ê°•ì œ ì·¨ì†Œ"""
        logger.info(f"ğŸ›‘ Cancelling all LLM requests")
        
        # ê°•ì œ ì¤‘ë‹¨ ì‹œê·¸ë„ ì„¤ì •
        self._force_stop_signal = True
        
        # ì§„í–‰ ì¤‘ì¸ ìš”ì²­ë“¤ ê°•ì œ ì·¨ì†Œ
        for request_id in list(self._active_requests):
            try:
                logger.info(f"ğŸ›‘ Cancelling request: {request_id}")
            except Exception as e:
                logger.warning(f"âš ï¸ Error cancelling request {request_id}: {e}")
        
        self._active_requests.clear()
        
        # HTTP ì—°ê²° ê°•ì œ ì¢…ë£Œ ì‹œë„
        try:
            import httpx
            import httpcore
            
            # ê¸°ì¡´ OpenAI í´ë¼ì´ì–¸íŠ¸ì˜ HTTP ì—°ê²°ë“¤ ê°•ì œ ì¢…ë£Œ
            if hasattr(self, 'client') and hasattr(self.client, '_client'):
                try:
                    if hasattr(self.client._client, 'close'):
                        self.client._client.close()
                        logger.info(f"ğŸ›‘ Closed OpenAI client HTTP connections")
                except Exception as e:
                    logger.warning(f"âš ï¸ Error closing OpenAI client: {e}")
                    
            # ê¸€ë¡œë²Œ HTTP ì—°ê²° í’€ ì •ë¦¬
            try:
                # httpxì˜ ê¸°ë³¸ í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ë“¤ ì¢…ë£Œ
                import asyncio
                if hasattr(httpx, '_client'):
                    logger.info(f"ğŸ›‘ Attempting to close httpx connections")
            except Exception as e:
                logger.warning(f"âš ï¸ Error in httpx cleanup: {e}")
                
        except Exception as e:
            logger.warning(f"âš ï¸ Error in HTTP cleanup: {e}")
        
        logger.info(f"ğŸ›‘ LLM request cancellation completed")
        
    def _setup_clients(self):
        """Set up API clients based on configured provider"""
        provider = self.llm_config.get("provider", "openai").lower()
        
        if provider == "openai":
            if not self.openai_api_key:
                raise ValueError("OpenAI API key not found. Set OPENAI_API_KEY environment variable.")
            openai.api_key = self.openai_api_key
            self.client = openai.Client(api_key=self.openai_api_key)
            print(f"Using OpenAI with API key: {self.openai_api_key[:5]}...{self.openai_api_key[-5:]}")
        elif provider == "anthropic":
            if not self.anthropic_api_key:
                raise ValueError("Anthropic API key not found. Set ANTHROPIC_API_KEY environment variable.")
            self.client = anthropic.Anthropic(api_key=self.anthropic_api_key)
        else:
            raise ValueError(f"Unsupported LLM provider: {provider}")
            
    def generate_response(self, system_prompt: str, user_prompt: str, 
                        context_type: str = "default",
                        llm_provider: str = "openai", llm_model: str = None,
                        max_tokens: int = None, temperature: float = 0.7) -> str:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            user_prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            context_type: ì»¨í…ìŠ¤íŠ¸ íƒ€ì… (ìë™ ìµœì í™”ìš©)
            llm_provider: LLM ì œê³µì (ê¸°ë³¸ê°’: "openai")
            llm_model: ì‚¬ìš©í•  ëª¨ë¸ (Noneì´ë©´ ì»¨í…ìŠ¤íŠ¸ë³„ ìµœì  ëª¨ë¸ ìë™ ì„ íƒ)
            max_tokens: ìµœëŒ€ í† í° ìˆ˜ (Noneì´ë©´ ì»¨í…ìŠ¤íŠ¸ë³„ ìµœì ê°’ ìë™ ì„ íƒ)
            temperature: ì˜¨ë„ (ê¸°ë³¸ê°’: 0.7)
            
        Returns:
            ìƒì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        # ğŸ›‘ ê°•ì œ ì¤‘ë‹¨ ì‹œê·¸ë„ ì²´í¬ (ìµœìš°ì„ )
        if self._force_stop_signal:
            logger.info(f"ğŸ›‘ LLM request cancelled due to force stop signal")
            return ""
        
        # âœ… ì»¨í…ìŠ¤íŠ¸ë³„ ìµœì  ì„¤ì • ìë™ ì ìš©
        context_config = self.context_configs.get(context_type, self.context_configs["default"])
        
        # íŒŒë¼ë¯¸í„°ê°€ ëª…ì‹œì ìœ¼ë¡œ ì œê³µë˜ì§€ ì•Šìœ¼ë©´ ì»¨í…ìŠ¤íŠ¸ ì„¤ì • ì‚¬ìš©
        if llm_model is None:
            llm_model = context_config["model"]
        if max_tokens is None:
            max_tokens = context_config["max_tokens"]
            
        # ë””ë²„ê¹… ë¡œê·¸ (ì»¨í…ìŠ¤íŠ¸ ìµœì í™” í™•ì¸ìš©)
        if context_type != "default":
            logger.info(f"[LLM_CONTEXT] {context_type} -> Model: {llm_model}, Tokens: {max_tokens}")
        
        try:
            # logger.info("[LLM_DEBUG] LLM ì‘ë‹µ ìƒì„± ì‹œì‘")
            # logger.info(f"[LLM_DEBUG] Provider: {llm_provider}, Model: {llm_model}")
            # logger.info(f"[LLM_DEBUG] System prompt ê¸¸ì´: {len(system_prompt)}")
            # logger.info(f"[LLM_DEBUG] User prompt ê¸¸ì´: {len(user_prompt)}")
            
            # OpenAI ì‚¬ìš©
            if llm_provider == "openai":
                # í™˜ê²½ ë³€ìˆ˜ ì²´í¬
                if not os.environ.get("OPENAI_API_KEY"):
                    api_key = self.openai_api_key
                    if api_key:
                        os.environ["OPENAI_API_KEY"] = api_key
                        # logger.info("[LLM_DEBUG] OpenAI API í‚¤ë¥¼ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤")
                    else:
                        logger.error("[LLM_DEBUG] OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                        return "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                
                # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
                from openai import OpenAI
                client = OpenAI()
                
                # logger.info(f"[LLM_DEBUG] OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
                # logger.info(f"[LLM_DEBUG] API ìš”ì²­ ì‹œì‘ - max_tokens: {max_tokens}, temperature: {temperature}")
                
                # API ìš”ì²­
                try:
                    # ğŸ›‘ API í˜¸ì¶œ ì§ì „ ì¤‘ë‹¨ ì‹œê·¸ë„ ì¬ì²´í¬
                    if self._force_stop_signal:
                        logger.info(f"ğŸ›‘ LLM request cancelled before API call")
                        return ""
                    
                    # ìš”ì²­ ID ìƒì„± ë° ì¶”ì  ì‹œì‘
                    import uuid
                    request_id = f"req_{uuid.uuid4().hex[:8]}"
                    self._active_requests.add(request_id)
                    
                    logger.info(f"ğŸ”„ Starting OpenAI API request: {request_id}")
                    
                    response = client.chat.completions.create(
                        model=llm_model,
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ],
                        max_tokens=max_tokens,
                        temperature=temperature
                    )
                    
                    # ğŸ›‘ API ì‘ë‹µ í›„ ì¤‘ë‹¨ ì‹œê·¸ë„ ì¬ì²´í¬
                    if self._force_stop_signal:
                        logger.info(f"ğŸ›‘ LLM request cancelled after API call")
                        return ""
                    
                    # ìš”ì²­ ì™„ë£Œ í›„ ì¶”ì ì—ì„œ ì œê±°
                    self._active_requests.discard(request_id)
                    
                    logger.info(f"âœ… Completed OpenAI API request: {request_id}")
                    result = response.choices[0].message.content
                    
                    # ğŸ›‘ ê²°ê³¼ ë°˜í™˜ ì§ì „ ìµœì¢… ì²´í¬
                    if self._force_stop_signal:
                        logger.info(f"ğŸ›‘ LLM response discarded due to force stop signal")
                        return ""
                    
                    return result
                
                except Exception as api_error:
                    logger.error(f"[LLM_DEBUG] OpenAI API í˜¸ì¶œ ì˜¤ë¥˜: {str(api_error)}", exc_info=True)
                    return ""
            
            # Ollama ì‚¬ìš©
            elif llm_provider == "ollama":
                # Ollama ì—”ë“œí¬ì¸íŠ¸ ì„¤ì • (ê¸°ë³¸ê°’: localhost:11434)
                ollama_endpoint = os.getenv("OLLAMA_ENDPOINT", "http://localhost:11434")
                
                # logger.info(f"[LLM_DEBUG] Ollama ì—”ë“œí¬ì¸íŠ¸: {ollama_endpoint}")
                # logger.info(f"[LLM_DEBUG] Ollama ëª¨ë¸: {llm_model}")
                
                try:
                    # Ollama API ìš”ì²­
                    payload = {
                        "model": llm_model,
                        "messages": [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ],
                        "options": {
                            "num_predict": max_tokens,
                            "temperature": temperature
                        },
                        "stream": False
                    }
                    
                    response = requests.post(
                        f"{ollama_endpoint}/api/chat",
                        json=payload,
                        timeout=120  # 2ë¶„ íƒ€ì„ì•„ì›ƒ
                    )
                    
                    if response.status_code != 200:
                        logger.error(f"[LLM_DEBUG] Ollama API ì˜¤ë¥˜: {response.status_code} - {response.text}")
                        return ""
                    
                    result = response.json()
                    
                    if "message" not in result or "content" not in result["message"]:
                        logger.error(f"[LLM_DEBUG] ìœ íš¨í•˜ì§€ ì•Šì€ Ollama ì‘ë‹µ í˜•ì‹: {result}")
                        return ""
                    
                    content = result["message"]["content"]
                    
                    if not content:
                        logger.error("[LLM_DEBUG] Ollamaì—ì„œ ë¹ˆ ì‘ë‹µì„ ë°›ì•˜ìŠµë‹ˆë‹¤")
                        return ""
                    
                    # logger.info(f"[LLM_DEBUG] Ollama ì‘ë‹µ ê¸¸ì´: {len(content)}")
                    # logger.info(f"[LLM_DEBUG] Ollama ì‘ë‹µ ë‚´ìš©: {content[:100]}..." if len(content) > 100 else f"[LLM_DEBUG] Ollama ì‘ë‹µ ë‚´ìš©: {content}")
                    
                    return content
                
                except requests.exceptions.RequestException as req_error:
                    logger.error(f"[LLM_DEBUG] Ollama ì—°ê²° ì˜¤ë¥˜: {str(req_error)}")
                    return ""
                except Exception as ollama_error:
                    logger.error(f"[LLM_DEBUG] Ollama API í˜¸ì¶œ ì˜¤ë¥˜: {str(ollama_error)}", exc_info=True)
                    return ""
            
            else:
                logger.error(f"[LLM_DEBUG] ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM ì œê³µì: {llm_provider}")
                return ""
                
        except Exception as e:
            logger.error(f"[LLM_DEBUG] LLM ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
            return ""
        
    def generate_philosophical_response(self, 
                                      npc_description: str, 
                                      topic: str,
                                      context: str = "",
                                      previous_dialogue: str = "",
                                      source_materials: List[Dict[str, str]] = None,
                                      user_contexts: List[Dict[str, Any]] = None,
                                      references: List[Dict[str, Any]] = None,
                                      llm_provider: str = None,
                                      llm_model: str = None,
                                      use_rag: bool = False,
                                      npc_id: str = None) -> Tuple[str, Dict[str, Any]]:
        """
        Generate a philosophical response from a specific perspective
        
        Args:
            npc_description: Description of the philosophical character/perspective to simulate
            topic: The philosophical topic to discuss
            context: Additional context about the discussion
            previous_dialogue: Previous messages in the dialogue
            source_materials: Relevant philosophical source materials
            user_contexts: User-provided context for the conversation
            references: References for the philosophical topics
            llm_provider: Override the default LLM provider
            llm_model: Override the default LLM model
            use_rag: Whether to use RAG to enhance the response
            npc_id: ID of the philosopher for RAG
            
        Returns:
            Tuple of (response_text, metadata)
        """
        # Extract the philosopher's style if available in the description
        philosopher_style = ""
        philosopher_name = ""
        
        # Try to extract philosopher's name and style from description
        if npc_description:
            # Extract name - typically the first part before the colon
            if ":" in npc_description:
                philosopher_name = npc_description.split(":")[0].strip()
            
            # Look for style information in the description
            if "style:" in npc_description.lower():
                style_parts = npc_description.lower().split("style:")
                if len(style_parts) > 1:
                    philosopher_style = style_parts[1].split(".")[0].strip()
                    
                    # If we have multiple sentences in style, get them all
                    full_style = style_parts[1].strip()
                    dot_idx = full_style.find(".")
                    if dot_idx >= 0 and len(full_style) > dot_idx + 1:
                        # Check if there are more sentences after the first one
                        rest_of_text = full_style[dot_idx+1:].strip()
                        if rest_of_text and rest_of_text[0].isupper():
                            # There's likely more content - try to get until next section
                            next_section_markers = ["key_concepts:", "major_works:", "philosophical_stance:", "influenced_by:", "voice style:", "debate approach:"]
                            end_idx = len(full_style)
                            
                            for marker in next_section_markers:
                                marker_pos = full_style.lower().find(marker)
                                if marker_pos > 0 and marker_pos < end_idx:
                                    end_idx = marker_pos
                            
                            philosopher_style = full_style[:end_idx].strip()
            
            # Also check for voice style which might be used for custom NPCs
            if "voice style:" in npc_description.lower():
                style_parts = npc_description.lower().split("voice style:")
                if len(style_parts) > 1:
                    philosopher_style = style_parts[1].split(".")[0].strip()
                
        # Build sources context
        sources_context = ""
        if source_materials:
            sources_context = "# Relevant Source Materials\n\n"
            for source in source_materials:
                if "title" in source and "excerpt" in source:
                    sources_context += f"**{source['title']}**\n"
                    sources_context += f"Excerpt: {source['excerpt']}\n\n"
        
        # Build user context
        user_context_str = ""
        latest_user_message = ""
        
        # Extract latest user message
        if previous_dialogue:
            dialogue_lines = previous_dialogue.strip().split("\n")
            for line in reversed(dialogue_lines):
                if line.lower().startswith("user:"):
                    latest_user_message = line.split(":", 1)[1].strip()
                    break
                
        if user_contexts:
            user_context_str = "# User-Provided References\n\n"
            for ctx in user_contexts:
                user_context_str += f"**{ctx['title']}**\n"
                user_context_str += f"Source: {ctx['source']}\n"
                user_context_str += f"Excerpt: {ctx['excerpt']}\n\n"
                
        # Build the system prompt with improved conversational flow and philosopher-specific style
        system_prompt = f"""You are an AI simulating the philosophical thinking of {philosopher_name or "a specific philosopher"} in an interactive dialogue.
Your goal is to respond to philosophical topics exactly as this philosopher would, while engaging naturally with other participants.
Maintain the philosophical terminology, worldview, and most importantly the UNIQUE SPEAKING STYLE consistent with this philosopher.

This is a philosophical simulation where different perspectives interact with each other.
Don't break character. Don't refer to yourself as an AI. Don't explain your thinking process.
Respond directly as if you truly are this philosopher.

PHILOSOPHER'S SPECIFIC STYLE AND MANNER OF SPEAKING:
{philosopher_style}

IMPORTANT GUIDELINES FOR NATURAL INTERACTIVE DIALOGUE:
1. Be concise and direct - keep responses to 2-3 sentences maximum
2. DIRECTLY RESPOND TO THE PREVIOUS SPEAKER - reference their specific points, ideas, or questions
3. Be conversational, as if you're having a real-time discussion with the previous speaker
4. If there are multiple speakers, address the most recent message or the most relevant point
5. NEVER start with "Indeed" or simple agreement - use varied ways to engage
6. If appropriate, ask follow-up questions or challenge assumptions made by others
7. RESPOND IN THE SAME LANGUAGE AS THE TOPIC - if the topic is in Korean, respond in Korean; if in English, respond in English

VERY IMPORTANT - ABOUT USING NAMES:
1. DO NOT address other speakers by name in most of your responses
2. AVOID starting your responses with another person's name
3. Only mention names when absolutely necessary for clarity (like when distinguishing between multiple viewpoints)
4. Focus on responding to ideas rather than to people
5. Use phrases like "That perspective..." or "This view..." instead of "Person's name, your perspective..."
6. When you do need to use names, use proper names (never IDs or codes)

The response should feel like a natural conversation that TRULY CAPTURES THE DISTINCT VOICE AND PHILOSOPHICAL APPROACH of {philosopher_name or "this philosopher"}.
"""

        # Build the user prompt with enhanced dialogue focus and philosopher-specific guidance
        user_prompt = f"""# Your Philosophical Persona
{npc_description}

# Your Unique Speaking Style
{philosopher_style}

# Topic of Discussion
{topic}

{sources_context}

{user_context_str}

# Additional Context
{context}

# Previous Dialogue (Most Recent First)
{previous_dialogue}

RESPOND DIRECTLY TO THE MOST RECENT MESSAGE IN THE DIALOGUE, as {philosopher_name or "your philosophical character"} would.
Your response should feel like a natural continuation of the conversation while TRULY EMBODYING YOUR UNIQUE PHILOSOPHICAL VOICE.

KEEP YOUR RESPONSE BRIEF (2-3 SENTENCES) as if speaking in a real-time dialogue.

IMPORTANT GUIDELINES: 
1. DO NOT ADDRESS THE PREVIOUS SPEAKER BY NAME in your response
2. DO NOT START YOUR RESPONSE WITH SOMEONE'S NAME
3. Directly address or engage with what was just said without using names
4. Express your philosophical perspective in YOUR UNIQUE STYLE
5. NEVER start with "Indeed" or generic acknowledgments
6. RESPOND IN THE SAME LANGUAGE AS THE TOPIC AND PREVIOUS MESSAGES

Create a natural flowing dialogue that genuinely captures how YOU as this specific philosopher would speak.
"""

        # Generate the initial response
        start_time = time.time()
        response_text = self.generate_response(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            llm_provider=llm_provider,
            llm_model=llm_model
        )
        
        # RAG ì‚¬ìš©ì„ ìœ„í•œ ë³€ìˆ˜ ì´ˆê¸°í™”
        rag_used = False
        rag_query = latest_user_message if latest_user_message else topic
        rag_detailed_results = []
        citations = []  # ì¸ìš© ì •ë³´ë¥¼ ì €ì¥í•  ë°°ì—´ ì¶”ê°€
        
        # ìƒì„±ëœ ì‘ë‹µì— RAGë¥¼ ì ìš©í•˜ì—¬ ê°•í™”
        if use_rag and npc_id:
            try:
                # ì–¸ì–´ ê°ì§€
                original_language = self.detect_language(response_text)
                logger.info(f"ğŸ” ì›ë³¸ ì‘ë‹µ ì–¸ì–´: {original_language}")
                
                # RAG ì¿¼ë¦¬ ì¤€ë¹„ - í•œêµ­ì–´ì¸ ê²½ìš° ì˜ì–´ë¡œ ë²ˆì—­
                translated_query = rag_query
                if original_language == 'ko':
                    translated_query = self.translate_korean_to_english(rag_query)
                    logger.info(f"ğŸ”„ RAG ì¿¼ë¦¬ê°€ ì˜ì–´ë¡œ ë²ˆì—­ë˜ì—ˆìŠµë‹ˆë‹¤: {translated_query[:50]}...")
                
                # RAGë¡œ ê´€ë ¨ ì½˜í…ì¸  ê²€ìƒ‰
                logger.info(f"ğŸ” ì² í•™ì {npc_id}ì˜ ì €ì‘ë¬¼ì—ì„œ ê´€ë ¨ ë‚´ìš© ê²€ìƒ‰ ì¤‘...")
                rag_result, rag_metadata = self.get_relevant_content_with_rag(
                    npc_id=npc_id, 
                    topic=topic, 
                    query=translated_query
                )
                
                # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš° ì‘ë‹µ ê°•í™”
                if rag_result and "documents" in rag_metadata:
                    logger.info(f"âœ… {len(rag_metadata['documents'])}ê°œì˜ ê´€ë ¨ ì €ì‘ë¬¼ ì²­í¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                    
                    # ì›ë³¸ ì‘ë‹µ ì €ì¥
                    original_response = response_text
                    
                    # RAGë¡œ ê°•í™”ëœ ì‘ë‹µ ìƒì„± ë° ì¸ìš© ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                    response_text, citations = self.enhance_message_with_rag(
                        message=response_text,
                        rag_results=rag_metadata,
                        original_language=original_language
                    )
                    
                    # ìƒì„¸ ê²€ìƒ‰ ê²°ê³¼ ì €ì¥
                    if "documents" in rag_metadata and "distances" in rag_metadata:
                        documents = rag_metadata["documents"]
                        distances = rag_metadata["distances"]
                        metadatas = rag_metadata.get("metadatas", [{}] * len(documents))
                        
                        for i, (doc, distance) in enumerate(zip(documents, distances)):
                            doc_metadata = metadatas[i] if i < len(metadatas) else {}
                            source = doc_metadata.get("source", "Unknown source")
                            
                            # ìœ ì‚¬ë„ ë³€í™˜
                            if distance is not None:
                                similarity = max(0, min(1, 1 - (distance / 2)))
                            else:
                                similarity = None
                                
                            rag_detailed_results.append({
                                "chunk": doc,
                                "similarity": similarity,
                                "source": source
                            })
                    
                    rag_used = True
                    logger.info(f"âœ… RAGë¥¼ í†µí•´ ì² í•™ì  ì‘ë‹µì´ ê°•í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
                else:
                    logger.warning(f"âš ï¸ {npc_id}ì˜ ì €ì‘ë¬¼ì—ì„œ ê´€ë ¨ ì½˜í…ì¸ ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                logger.error(f"âŒ RAG ê°•í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ë³¸ ì‘ë‹µ ìœ ì§€
        
        elapsed_time = time.time() - start_time
        
        # ë©”íƒ€ë°ì´í„° êµ¬ì„± - ì¸ìš© ì •ë³´ ì¶”ê°€
        metadata = {
            "elapsed_time": f"{elapsed_time:.2f}s",
            "rag_used": rag_used,
            "rag_results": rag_detailed_results if rag_used else [],
            "citations": citations  # ì¸ìš© ì •ë³´ ì¶”ê°€
        }
        
        return response_text, metadata

    def get_relevant_content_with_rag(self, npc_id: str, topic: str, query: str) -> Tuple[str, Dict[str, Any]]:
        """
        Retrieve relevant content for an NPC using RAG
        
        Args:
            npc_id: The ID of the NPC
            topic: The current discussion topic
            query: The query to search for (usually the most recent message)
            
        Returns:
            Tuple containing (relevant content, metadata)
        """
        try:
            # Lowercase NPC ID for standard format
            npc_id_lower = npc_id.lower()
            
            # Get RAG path for this NPC
            rag_path = None
            collection_name = None
            
            # Check if in standard mapping
            if npc_id_lower in self.rag_paths:
                rag_path = self.rag_paths[npc_id_lower]
                collection_name = self.rag_collections.get(npc_id_lower, "langchain")
            
            # If no RAG path found, return empty result
            if not rag_path or not os.path.exists(rag_path):
                logger.warning(f"âŒ No RAG data path found for NPC: {npc_id}")
                return "", {"status": "no_rag_data"}
            
            logger.info(f"ğŸ” Using RAG path: {rag_path} with collection: {collection_name}")
            
            # Initialize ChromaDB client
            chroma_client = chromadb.PersistentClient(path=rag_path)
            embedding_function = embedding_functions.OpenAIEmbeddingFunction(
                api_key=self.openai_api_key,
                model_name="text-embedding-3-small"
            )
            
            # Get collection
            collection = chroma_client.get_collection(name=collection_name, embedding_function=embedding_function)
            
            # Log collection info
            logger.info(f"ğŸ“Š Collection count: {collection.count()}")
            
            # Clean query - remove any special formatting
            clean_query = re.sub(r'\s+', ' ', query).strip()
            
            # If query is too short, use topic as fallback
            if len(clean_query) < 10:
                clean_query = topic
                logger.info(f"âš ï¸ Query too short, using topic instead: {clean_query}")
            
            # Query for relevant documents
            results = collection.query(
                query_texts=[clean_query],
                n_results=5,  # 5ê°œì˜ ê´€ë ¨ ì²­í¬ ê²€ìƒ‰
                include=["documents", "distances", "metadatas"]  # ê±°ë¦¬ ë° ë©”íƒ€ë°ì´í„° í¬í•¨
            )
            
            # Process results
            if results and "documents" in results and results["documents"]:
                documents = results["documents"][0]  # ì²« ë²ˆì§¸ ì¿¼ë¦¬ ê²°ê³¼
                distances = results.get("distances", [[]])[0] if "distances" in results else []
                metadatas = results.get("metadatas", [[]])[0] if "metadatas" in results else []
                
                # ê±°ë¦¬ê°’ ë¡œê¹…
                logger.info(f"ğŸ” ê²€ìƒ‰ëœ ê±°ë¦¬ê°’(distances): {distances}")
                
                if not documents:
                    logger.warning("âŒ No documents found in query results")
                    return "", {"status": "no_results"}
                
                # RAG ë©”íƒ€ë°ì´í„° ìƒì„±
                metadata = {
                    "status": "success",
                    "result_count": len(documents),
                    "query": clean_query,
                    "collection": collection_name,
                    "documents": documents,
                    "distances": distances,
                    "metadatas": metadatas
                }
                
                # ë‹¨ìˆœ í…ìŠ¤íŠ¸ ê²°í•© (ì´í›„ enhance_message_with_ragì—ì„œ ì‹¤ì œ í¬ë§·íŒ…ë¨)
                combined_text = ""
                for i, (doc, distance) in enumerate(zip(documents, distances if distances else [None] * len(documents))):
                    combined_text += f"Excerpt {i+1}: {doc}\n\n"
                
                logger.info(f"âœ… Retrieved {len(documents)} relevant chunks")
                return combined_text, metadata
            else:
                logger.warning("âŒ No documents found in query results")
                return "", {"status": "no_results"}
                
        except Exception as e:
            logger.error(f"âŒ Error in RAG retrieval: {str(e)}")
            return "", {"status": "error", "error": str(e)}

    def should_use_rag(self, npc_id: str, user_message: str, previous_dialogue: str = "", topic: str = "") -> bool:
        """
        Automatically determines if RAG should be used based on conversation context and NPC
        
        Args:
            npc_id: The ID of the NPC that will respond
            user_message: The current user message
            previous_dialogue: Previous dialogue for context
            topic: The conversation topic
            
        Returns:
            Boolean indicating whether RAG should be used
        """
        # Check if this NPC has RAG data available
        npc_id_lower = npc_id.lower()
        
        # ì² í•™ìê°€ RAG ë°ì´í„°ë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€ í™•ì¸
        if npc_id_lower in self.rag_paths:
            rag_path = self.rag_paths[npc_id_lower]
            # RAG ë°ì´í„° ê²½ë¡œê°€ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            if os.path.exists(rag_path):
                logger.info(f"âœ… {npc_id}ì˜ RAG ë°ì´í„°ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. RAG ìë™ í™œì„±í™”ë¨.")
                return True
            else:
                logger.warning(f"âš ï¸ {npc_id}ì˜ RAG ê²½ë¡œ({rag_path})ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return False
                
        # RAG ë°ì´í„°ê°€ ì—†ëŠ” ì² í•™ìëŠ” RAGë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        logger.info(f"âš ï¸ {npc_id}ëŠ” RAG ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return False

    # ì–¸ì–´ ê°ì§€ í•¨ìˆ˜ ì¶”ê°€
    def detect_language(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.
        
        Args:
            text: ì–¸ì–´ë¥¼ ê°ì§€í•  í…ìŠ¤íŠ¸
            
        Returns:
            ê°ì§€ëœ ì–¸ì–´ ì½”ë“œ (ì˜ˆ: 'ko', 'en')
        """
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±: í•œê¸€ ê¸€ìê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        korean_pattern = re.compile('[ê°€-í£]')
        if korean_pattern.search(text):
            return 'ko'
        else:
            return 'en'
            
    # í•œêµ­ì–´ ë²ˆì—­ ê¸°ëŠ¥ ì¶”ê°€
    def translate_korean_to_english(self, korean_text: str) -> str:
        """
        í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.
        
        Args:
            korean_text: ë²ˆì—­í•  í•œêµ­ì–´ í…ìŠ¤íŠ¸
            
        Returns:
            ë²ˆì—­ëœ ì˜ì–´ í…ìŠ¤íŠ¸
        """
        try:
            logger.info(f"ğŸ”„ í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë²ˆì—­ ì‹œì‘: {korean_text[:50]}...")
            
            response = self.client.chat.completions.create(
                model="gpt-4o", # ëª¨ë¸ ì„¤ì • - ë²ˆì—­ì— ìµœì í™”ëœ ëª¨ë¸ ì‚¬ìš©
                messages=[
                    {"role": "system", "content": "You are a professional Korean to English translator. Your task is to accurately translate Korean text to English. Translate ONLY the text provided, without any additional explanation or context."},
                    {"role": "user", "content": f"Translate this Korean text to English: {korean_text}"}
                ],
                temperature=0.1,
                max_tokens=500
            )
            
            english_text = response.choices[0].message.content.strip()
            logger.info(f"âœ… ë²ˆì—­ ì™„ë£Œ: {english_text[:50]}...")
            return english_text
            
        except Exception as e:
            logger.error(f"âŒ ë²ˆì—­ ì˜¤ë¥˜: {str(e)}")
            return korean_text  # ì˜¤ë¥˜ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜

    # í…ìŠ¤íŠ¸ë¥¼ RAGë¥¼ í†µí•´ ê°•í™”í•˜ëŠ” í•¨ìˆ˜ ìˆ˜ì •
    def enhance_message_with_rag(self, 
                                message: str, 
                                rag_results: Dict[str, Any], 
                                original_language: str = 'en') -> Tuple[str, List[Dict[str, str]]]:
        """
        ê²€ìƒ‰ëœ RAG ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ ë©”ì‹œì§€ë¥¼ ê°•í™”í•©ë‹ˆë‹¤.
        
        Args:
            message: ì›ë³¸ ë©”ì‹œì§€
            rag_results: RAG ê²€ìƒ‰ ê²°ê³¼ (documents, distances, metadatas í¬í•¨)
            original_language: ì›ë³¸ ì–¸ì–´ ì½”ë“œ ('ko' ë˜ëŠ” 'en')
            
        Returns:
            Tupleì˜ í˜•íƒœë¡œ (ê°•í™”ëœ ë©”ì‹œì§€, ì¸ìš© ì •ë³´ ë¦¬ìŠ¤íŠ¸) ë°˜í™˜
        """
        try:
            logger.info(f"ğŸ“š RAG ê²€ìƒ‰ ê²°ê³¼ë¥¼ í™œìš©í•œ ë©”ì‹œì§€ ê°•í™” ì‹œì‘")
            
            # ê²€ìƒ‰ ê²°ê³¼ ì¶”ì¶œ ë° í¬ë§·íŒ…
            retrieved_contexts = ""
            
            if "documents" in rag_results and "distances" in rag_results:
                documents = rag_results["documents"]
                distances = rag_results["distances"]
                metadatas = rag_results.get("metadatas", [{}] * len(documents))
                
                # ê° ê²€ìƒ‰ ê²°ê³¼ì— ëŒ€í•œ ìƒì„¸ ì •ë³´ í¬ë§·íŒ…
                for i, (doc, distance) in enumerate(zip(documents, distances)):
                    metadata = metadatas[i] if i < len(metadatas) else {}
                    source = metadata.get("source", "Unknown source")
                    
                    # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜ (0~1 ë²”ìœ„)
                    similarity = max(0, min(1, 1 - (distance / 2))) if distance is not None else 0
                    
                    retrieved_contexts += f"Source {i+1} ({source}): {doc}\n\n"
            
            # RAG ê°•í™” í”„ë¡¬í”„íŠ¸ ì‘ì„± - ê°ì£¼ ìŠ¤íƒ€ì¼ ë³€ê²½í•˜ë˜ 1ì¸ì¹­ ì‹œì  ìœ ì§€
            system_prompt = """You are simulating a specific philosopher, speaking in first person as if you ARE that philosopher. 
Your goal is to enhance a philosophical response with source material from your own works, while maintaining your authentic voice and style.

IMPORTANT INSTRUCTIONS:
1. ALWAYS SPEAK IN FIRST PERSON as the philosopher - you ARE Kant, Hegel, etc., not someone describing their view
2. Maintain the exact same philosophical perspective and speaking style as in the original message
3. PRESERVE AND ENHANCE THE LOGICAL STRUCTURE of philosophical argumentation:
   - Present your core philosophical principle relevant to the topic
   - Connect this principle to the specific topic with clear logical reasoning
   - Provide your philosophical conclusion that follows from your principles
   - End with a brief philosophical reflection if appropriate
4. Use the retrieved excerpts from your own works to strengthen your points with specific references
5. When referencing your philosophical works, use numbered footnotes like [1], [2], etc. at the end of relevant sentences
6. Include a list of citations at the end of your response in this format:
   [1] Source: "Critique of Pure Reason", Text: "original quoted text"
   [2] Source: "Critique of Practical Reason", Text: "original quoted text"
7. Maintain the same formal philosophical tone, terminology and first-person perspective throughout
8. DO NOT change the message's main points or conclusions, but DO strengthen the logical connections
9. Output MUST be in the same language as the original message

Remember: You ARE the philosopher speaking directly, not someone explaining their views.
"""
            
            # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ì‘ì„± - ê°ì£¼ ìŠ¤íƒ€ì¼ ë³€ê²½
            user_prompt = f"""# Your Original Response (Speaking as the Philosopher)
{message}

# Relevant Source Materials from Your Own Philosophical Works
{retrieved_contexts}

Enhance your original philosophical response using the retrieved source materials from your own works. Follow these guidelines:

1. Maintain your identity as the philosopher throughout - YOU are Kant, Nietzsche, etc. speaking in first person
2. Use a clear logical structure in your enhanced response:
   - Present your core philosophical principle relevant to this topic, citing the ACTUAL SOURCE
   - Connect this principle to the specific topic with crystal-clear logical reasoning
   - Provide your philosophical conclusion based on this reasoning
   - End with a brief philosophical reflection
3. Strengthen your argument by citing your own actual works - referring to their specific titles
4. Use numbered footnotes [1], [2], etc. at the end of sentences that reference your specific works
5. Include a list of citations at the end of your response in this format:
   [1] Source: "Critique of Pure Reason", Text: "original quoted text"
   [2] Source: "Critique of Practical Reason", Text: "original quoted text"
6. Keep the same philosophical tone, speaking style and perspective as in your original response
7. Maintain the same core points as your original message but enhance the logical structure and connections

DO NOT use explicit section headers like "ì „ì œ 1" or "ê²°ë¡ ". Instead, create a naturally flowing philosophical response that contains all the logical components (principles, connections, conclusion) seamlessly integrated.
"""
            
            # ê°•í™”ëœ ë©”ì‹œì§€ ìƒì„±
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.2,
                max_tokens=1000
            )
            
            full_enhanced_message = response.choices[0].message.content.strip()
            logger.info(f"âœ… ë©”ì‹œì§€ ê°•í™” ì™„ë£Œ: {full_enhanced_message[:50]}...")
            
            # ë©”ì‹œì§€ì™€ ì¸ìš© ë¶€ë¶„ ë¶„ë¦¬
            message_parts = full_enhanced_message.split("\n\n")
            
            # ì¸ìš© ëª©ë¡ ì¶”ì¶œ
            citations = []
            enhanced_message = full_enhanced_message
            
            # ë§ˆì§€ë§‰ ë¶€ë¶„ì— ìˆëŠ” ê°ì£¼ ëª©ë¡ ì°¾ê¸°
            citation_pattern = r'\[(\d+)\]\s+Source:\s+"([^"]+)",\s+Text:\s+"([^"]+)"'
            citation_matches = re.findall(citation_pattern, full_enhanced_message)
            
            if citation_matches:
                # ê°ì£¼ ëª©ë¡ ì°¾ìŒ
                logger.info(f"ğŸ“š {len(citation_matches)}ê°œì˜ ì¸ìš© ì •ë³´ ì°¾ìŒ: {citation_matches}")
                
                for citation_id, source, text in citation_matches:
                    citation_obj = {
                        "id": citation_id,
                        "source": source,
                        "text": text
                    }
                    citations.append(citation_obj)
                    logger.debug(f"ğŸ“š ì¸ìš© ì •ë³´ ìƒì„±: {citation_obj}")
                
                # ê°ì£¼ ëª©ë¡ ë¶€ë¶„ ì œê±°í•˜ê³  ì‹¤ì œ ë©”ì‹œì§€ë§Œ ìœ ì§€
                enhanced_message = re.sub(r'\n\n\[1\]\s+Source:.+', '', full_enhanced_message, flags=re.DOTALL)
                
                logger.info(f"âœ… {len(citations)}ê°œì˜ ê°ì£¼ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
            else:
                logger.warning("âš ï¸ ê°ì£¼ ëª©ë¡ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            
            # ë°˜í™˜í•˜ê¸° ì „ì— citations êµ¬ì¡° í™•ì¸
            if citations:
                logger.info(f"ğŸ“š ë°˜í™˜í•  ì¸ìš© ì •ë³´: {citations}")
                # ê° ì¸ìš© ì •ë³´ì— í•„ìˆ˜ í•„ë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                for i, citation in enumerate(citations):
                    if not isinstance(citation, dict) or not all(k in citation for k in ["id", "source", "text"]):
                        logger.warning(f"âš ï¸ ì¸ìš© ì •ë³´ {i}ë²ˆì˜ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {citation}")
                        
            return enhanced_message, citations
            
        except Exception as e:
            logger.error(f"âŒ ë©”ì‹œì§€ ê°•í™” ì˜¤ë¥˜: {str(e)}")
            return message, []  # ì˜¤ë¥˜ ì‹œ ì›ë³¸ ë©”ì‹œì§€ì™€ ë¹ˆ ì¸ìš© ëª©ë¡ ë°˜í™˜
 