"""
ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €

UserContextManagerë¥¼ í™•ì¥í•˜ì—¬ ê°ê´€ì ì¸ ì»¨í…ìŠ¤íŠ¸ ìš”ì•½ ë° ê´€ë¦¬ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import logging
import re
import math
from typing import Dict, List, Any, Optional, Union
from ...utils.context_manager import UserContextManager
from .summary_templates import SummaryTemplates

logger = logging.getLogger(__name__)

class DebateContextManager(UserContextManager):
    """
    ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €
    
    UserContextManagerë¥¼ í™•ì¥í•˜ì—¬ ê°ê´€ì ì¸ ì»¨í…ìŠ¤íŠ¸ ìš”ì•½ ê¸°ëŠ¥ì„ ì¶”ê°€:
    - ì£¼ì œ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ìš”ì•½
    - ì ì‘ì  ìš”ì•½ ì „ëµ (ê¸¸ì´ì— ë”°ë¥¸ ê³„ì¸µì  ìš”ì•½)
    - ë¶ˆë ›í¬ì¸íŠ¸ í˜•íƒœì˜ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
    """
    
    def __init__(self, llm_manager, max_context_length: int = 4000, max_summary_points: int = 5):
        """
        ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        
        Args:
            llm_manager: LLM ê´€ë¦¬ì (ìš”ì•½ ìƒì„±ìš©)
            max_context_length: ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´
            max_summary_points: ìµœëŒ€ ìš”ì•½ í¬ì¸íŠ¸ ìˆ˜
        """
        super().__init__(max_context_length)
        self.llm_manager = llm_manager
        self.max_summary_points = max_summary_points
        
        # ì ì‘ì  ìš”ì•½ ì„¤ì •
        self.short_context_threshold = 3000  # 3000ì ë¯¸ë§Œì€ ë‹¨ì¼ ìš”ì•½
        self.chunk_size = 2000  # ì²­í¬ í¬ê¸° ì¦ëŒ€ (1200 â†’ 2000)
        self.chunk_overlap = 300  # ì²­í¬ê°„ ì˜¤ë²„ë© ì¦ëŒ€
        
        # ê³¼ë„í•œ ì»¨í…ìŠ¤íŠ¸ ë³´í˜¸ ì„¤ì •
        self.max_processable_length = 50000  # ìµœëŒ€ ì²˜ë¦¬ ê°€ëŠ¥ ê¸¸ì´ (50KB)
        self.max_chunks_per_level = 15  # ë ˆë²¨ë‹¹ ìµœëŒ€ ì²­í¬ ìˆ˜
        self.max_llm_calls = 25  # ìµœëŒ€ LLM í˜¸ì¶œ íšŸìˆ˜ ì œí•œ
        
        # ìºì‹œëœ ìš”ì•½ë“¤
        self.summaries = {}  # {cache_key: summary}
        self.context_bullet_points = {}  # {context_id: [bullet_points]}
        
        logger.info(f"ContextManager initialized with enhanced safeguards")
        logger.info(f"Max processable length: {self.max_processable_length:,} chars")
        logger.info(f"Max LLM calls per request: {self.max_llm_calls}")
    
    def generate_summary(self, topic: str, context_type: str = None) -> Dict[str, str]:
        """
        ì£¼ì œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°ê´€ì ì¸ ì»¨í…ìŠ¤íŠ¸ ìš”ì•½ ìƒì„±
        
        Args:
            topic: ì£¼ì œ
            context_type: ì»¨í…ìŠ¤íŠ¸ íƒ€ì… (Noneì´ë©´ ìë™ íŒë³„ëœ íƒ€ì… ì‚¬ìš©)
        
        Returns:
            ê°ê´€ì  ìš”ì•½ ë”•ì…”ë„ˆë¦¬ {"summary": summary_text}
        """
        if not self.active_contexts:
            logger.warning("No active contexts available for summary generation")
            return {"summary": ""}
        
        # ğŸ†• context_typeì´ Noneì´ë©´ ìë™ íŒë³„ëœ íƒ€ì… ì‚¬ìš©
        if context_type is None:
            context_type = self._determine_best_context_type()
            logger.info(f"Auto-determined context type: {context_type}")
        
        # ìºì‹œ í™•ì¸ (íƒ€ì… í¬í•¨)
        cache_key = f"{topic}_{context_type}_{hash(str(sorted(self.active_contexts)))}"
        if cache_key in self.summaries:
            logger.info(f"Using cached summary for topic: {topic} (type: {context_type})")
            return self.summaries[cache_key]
        
        # ëª¨ë“  í™œì„± ì»¨í…ìŠ¤íŠ¸ í†µí•©
        combined_context = self._combine_active_contexts()
        
        if not combined_context.strip():
            logger.warning("Combined context is empty")
            return {"summary": ""}
        
        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¸ ë° ì „ëµ ê²°ì •
        context_length = len(combined_context)
        logger.info(f"Combined context length: {context_length:,} chars")
        
        # ê³¼ë„í•œ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì²˜ë¦¬
        if context_length > self.max_processable_length:
            logger.warning(f"Context too long ({context_length:,} chars), truncating to {self.max_processable_length:,} chars")
            combined_context = self._truncate_context_intelligently(combined_context)
            context_length = len(combined_context)
            logger.info(f"Truncated context length: {context_length:,} chars")
        
        try:
            logger.info(f"Generating objective summary for topic: {topic} with context type: {context_type}")
            
            if context_length < self.short_context_threshold:
                # ì§§ì€ ì»¨í…ìŠ¤íŠ¸: ë‹¨ì¼ ìš”ì•½
                summary = self._generate_single_summary(
                    combined_context, topic, context_type
                )
            else:
                # ê¸´ ì»¨í…ìŠ¤íŠ¸: ê³„ì¸µì  ìš”ì•½
                summary = self._generate_hierarchical_summary(
                    combined_context, topic, context_type
                )
            
            result = {"summary": summary}
            logger.info(f"Generated objective summary with {len(summary.split('â€¢')) - 1} bullet points")
            
            # ìºì‹œì— ì €ì¥
            self.summaries[cache_key] = result
            
            return result
            
        except Exception as e:
            logger.error(f"Error generating objective summary: {str(e)}")
            return {"summary": f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}"}
    
    def _determine_best_context_type(self) -> str:
        """
        í™œì„± ì»¨í…ìŠ¤íŠ¸ë“¤ì—ì„œ ìµœì ì˜ ì»¨í…ìŠ¤íŠ¸ íƒ€ì… ê²°ì •
        
        Returns:
            ê²°ì •ëœ ì»¨í…ìŠ¤íŠ¸ íƒ€ì…
        """
        if not self.active_contexts:
            return None
        
        # ê°€ì¥ í”í•œ íƒ€ì… ì‚¬ìš© (UserContextManagerì˜ ë©”ì„œë“œ í™œìš©)
        most_common_type = self.get_most_common_context_type()
        
        if most_common_type and most_common_type != "general":
            logger.info(f"Using most common context type: {most_common_type}")
            return most_common_type
        
        # ëª¨ë“  íƒ€ì…ì´ generalì´ê±°ë‚˜ íƒ€ì… ì •ë³´ê°€ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ì»¨í…ìŠ¤íŠ¸ íƒ€ì… ì‚¬ìš©
        first_context = self.user_contexts.get(self.active_contexts[0])
        if first_context:
            inferred_type = first_context.get("content_type", None)
            if inferred_type:
                logger.info(f"Using first context type: {inferred_type}")
                return inferred_type
        
        logger.info("No specific context type determined, using None (general template)")
        return None
    
    def get_context_type_summary(self) -> Dict[str, Any]:
        """
        í˜„ì¬ í™œì„± ì»¨í…ìŠ¤íŠ¸ë“¤ì˜ íƒ€ì… ë¶„í¬ ìš”ì•½
        
        Returns:
            íƒ€ì…ë³„ í†µê³„ ì •ë³´
        """
        if not self.active_contexts:
            return {"total_contexts": 0, "type_distribution": {}, "determined_type": None}
        
        # íƒ€ì… ë¶„í¬ ê³„ì‚°
        type_counts = {}
        for ctx_id in self.active_contexts:
            context = self.user_contexts.get(ctx_id)
            if context:
                content_type = context.get("content_type", "unknown")
                type_counts[content_type] = type_counts.get(content_type, 0) + 1
        
        # ê²°ì •ëœ íƒ€ì…
        determined_type = self._determine_best_context_type()
        
        return {
            "total_contexts": len(self.active_contexts),
            "type_distribution": type_counts,
            "determined_type": determined_type,
            "context_details": [
                {
                    "id": ctx_id,
                    "title": self.user_contexts[ctx_id].get("title", "Unknown"),
                    "source_type": self.user_contexts[ctx_id].get("type", "unknown"),
                    "content_type": self.user_contexts[ctx_id].get("content_type", "unknown"),
                    "length": len(self.user_contexts[ctx_id].get("content", ""))
                }
                for ctx_id in self.active_contexts
                if ctx_id in self.user_contexts
            ]
        }
    
    # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
    def generate_debate_summary(self, topic: str, stance_statements: Dict[str, str] = None, 
                               context_type: str = None) -> Dict[str, str]:
        """í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œ - generate_summary í˜¸ì¶œ"""
        return self.generate_summary(topic, context_type)
    
    def _generate_single_summary(self, context: str, topic: str, context_type: str = None) -> str:
        """
        ë‹¨ì¼ ìš”ì•½ ìƒì„± (ì§§ì€ ì»¨í…ìŠ¤íŠ¸ìš©)
        """
        try:
            template = SummaryTemplates.get_template(context_type=context_type)
            prompt = template.format(context=context, topic=topic)
            
            # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë”°ë¥¸ í† í° ìˆ˜ ì¡°ì •
            estimated_tokens = self._estimate_tokens(context)
            max_tokens = min(1200, max(400, estimated_tokens // 3))  # ì…ë ¥ì˜ 1/3 ì •ë„
            
            summary = self.llm_manager.generate_response(
                system_prompt="You are a helpful assistant that creates concise, accurate summaries.",
                user_prompt=prompt,
                llm_model="gpt-4",
                max_tokens=max_tokens
            )
            
            # ë¶ˆë › í¬ì¸íŠ¸ ì¶”ì¶œ ë° ì •ì œ
            bullet_points = self._extract_bullet_points(summary)
            return "\n".join(bullet_points)
            
        except Exception as e:
            logger.error(f"Error in single summary generation: {str(e)}")
            return f"ë‹¨ì¼ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}"
    
    def _generate_hierarchical_summary(self, context: str, topic: str, context_type: str = None) -> str:
        """
        ê³„ì¸µì  ìš”ì•½ ìƒì„± (ê¸´ ì»¨í…ìŠ¤íŠ¸ìš©) - ì²­í¬ ìˆ˜ ì œí•œ ì ìš©
        """
        try:
            logger.info(f"Starting hierarchical summarization")
            
            # 1ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
            chunks = self._split_into_chunks(context)
            original_chunk_count = len(chunks)
            
            # ì²­í¬ ìˆ˜ ì œí•œ ì ìš©
            if len(chunks) > self.max_chunks_per_level:
                logger.warning(f"Too many chunks ({len(chunks)}), limiting to {self.max_chunks_per_level}")
                # ì¤‘ìš”í•œ ë¶€ë¶„ ìš°ì„ ìœ¼ë¡œ ì²­í¬ ì„ íƒ
                chunks = self._select_most_important_chunks(chunks, self.max_chunks_per_level)
            
            logger.info(f"Processing {len(chunks)}/{original_chunk_count} chunks")
            
            # LLM í˜¸ì¶œ íšŸìˆ˜ í™•ì¸
            estimated_calls = len(chunks) + 1  # ì²­í¬ë³„ ìš”ì•½ + ìµœì¢… ìš”ì•½
            if estimated_calls > self.max_llm_calls:
                logger.error(f"Estimated LLM calls ({estimated_calls}) exceeds limit ({self.max_llm_calls})")
                return f"ì»¨í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ì–´ì„œ ìš”ì•½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì˜ˆìƒ LLM í˜¸ì¶œ: {estimated_calls}íšŒ)"
            
            # 2ë‹¨ê³„: ê° ì²­í¬ë³„ ë¶€ë¶„ ìš”ì•½ ìƒì„±
            partial_summaries = []
            for i, chunk in enumerate(chunks):
                logger.info(f"Processing chunk {i+1}/{len(chunks)} (length: {len(chunk):,} chars)")
                
                # ì²­í¬ë³„ ìš”ì•½ í”„ë¡¬í”„íŠ¸ ìƒì„±
                chunk_prompt = self._create_chunk_summary_prompt(chunk, topic, context_type)
                
                chunk_summary = self.llm_manager.generate_response(
                    system_prompt="You are summarizing a part of a larger document. Focus on key facts and information.",
                    user_prompt=chunk_prompt,
                    llm_model="gpt-4",
                    max_tokens=400  # ì²­í¬ë³„ í† í° ìˆ˜ ì¶•ì†Œ (600 â†’ 400)
                )
                
                if chunk_summary.strip():
                    partial_summaries.append(chunk_summary)
            
            if not partial_summaries:
                logger.error("No partial summaries generated")
                return "ê³„ì¸µì  ìš”ì•½ ì‹¤íŒ¨: ë¶€ë¶„ ìš”ì•½ ìƒì„± ì•ˆë¨"
            
            # 3ë‹¨ê³„: ë¶€ë¶„ ìš”ì•½ë“¤ì„ í•©ì³ì„œ ìµœì¢… ìš”ì•½ ìƒì„±
            logger.info(f"Combining {len(partial_summaries)} partial summaries")
            combined_partial = "\n\n--- PARTIAL SUMMARY ---\n".join(partial_summaries)
            
            # ìµœì¢… ìš”ì•½ í”„ë¡¬í”„íŠ¸
            final_prompt = self._create_final_summary_prompt(combined_partial, topic, context_type)
            
            final_summary = self.llm_manager.generate_response(
                system_prompt="You are creating a final summary by combining multiple partial summaries. Create a coherent, structured summary.",
                user_prompt=final_prompt,
                llm_model="gpt-4",
                max_tokens=800  # ìµœì¢… ìš”ì•½ í† í° ìˆ˜ ì¶•ì†Œ (1000 â†’ 800)
            )
            
            # ë¶ˆë › í¬ì¸íŠ¸ ì¶”ì¶œ ë° ì •ì œ
            bullet_points = self._extract_bullet_points(final_summary)
            result = "\n".join(bullet_points)
            
            logger.info(f"Hierarchical summarization completed: {len(chunks)} chunks â†’ {len(bullet_points)} bullet points")
            return result
            
        except Exception as e:
            logger.error(f"Error in hierarchical summary generation: {str(e)}")
            return f"ê³„ì¸µì  ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}"
    
    def _split_into_chunks(self, text: str) -> List[str]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ì ìœ¼ë¡œ ì ì ˆí•œ ì²­í¬ë¡œ ë¶„í• 
        1. ë¬¸ë‹¨ êµ¬ì¡° ìš°ì„  ê³ ë ¤
        2. ë‹¤ì–‘í•œ êµ¬ë‘ì  ì¸ì‹
        3. ì ì ˆí•œ ì˜¤ë²„ë© ì ìš©
        """
        # ë¨¼ì € ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ì‹œë„
        paragraph_chunks = self._try_paragraph_based_chunking(text)
        if paragraph_chunks:
            return paragraph_chunks
        
        # ë¬¸ë‹¨ ê¸°ë°˜ì´ ì•ˆë˜ë©´ ê°œì„ ëœ ë¬¸ì¥ ê¸°ë°˜ ë¶„í• 
        return self._sentence_based_chunking(text)
    
    def _try_paragraph_based_chunking(self, text: str) -> List[str]:
        """
        ë¬¸ë‹¨ ê¸°ë°˜ ì²­í¬í™” ì‹œë„
        """
        # ë¬¸ë‹¨ ë¶„í•  (ë‹¤ì–‘í•œ íŒ¨í„´ ì¸ì‹)
        paragraphs = re.split(r'\n\s*\n', text)
        paragraphs = [p.strip() for p in paragraphs if p.strip()]
        
        if len(paragraphs) < 2:
            return []  # ë¬¸ë‹¨ì´ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ ì‹¤íŒ¨
        
        chunks = []
        current_chunk = ""
        
        for paragraph in paragraphs:
            # í˜„ì¬ ì²­í¬ì— ë¬¸ë‹¨ì„ ì¶”ê°€í–ˆì„ ë•Œì˜ ê¸¸ì´
            potential_length = len(current_chunk) + len(paragraph) + 2  # \n\n ì¶”ê°€
            
            if potential_length <= self.chunk_size or not current_chunk:
                # ì²­í¬ í¬ê¸° ë‚´ì´ê±°ë‚˜ ì²« ë²ˆì§¸ ë¬¸ë‹¨ì´ë©´ ì¶”ê°€
                current_chunk += ("\n\n" if current_chunk else "") + paragraph
            else:
                # ì²­í¬ í¬ê¸°ë¥¼ ì´ˆê³¼í•˜ë©´ í˜„ì¬ ì²­í¬ë¥¼ ì €ì¥í•˜ê³  ìƒˆ ì²­í¬ ì‹œì‘
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = paragraph
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk:
            chunks.append(current_chunk)
        
        # ì˜¤ë²„ë© ì ìš©
        return self._apply_overlap_to_chunks(chunks, text)
    
    def _sentence_based_chunking(self, text: str) -> List[str]:
        """
        ê°œì„ ëœ ë¬¸ì¥ ê¸°ë°˜ ì²­í¬í™”
        """
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + self.chunk_size
            
            # ë§ˆì§€ë§‰ ì²­í¬ê°€ ì•„ë‹ˆë©´ ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸°
            if end < len(text):
                # ë‹¤ì–‘í•œ ë¬¸ì¥ ë íŒ¨í„´ ì¸ì‹ (í•œêµ­ì–´ í¬í•¨)
                sentence_end = self._find_best_sentence_boundary(text, start, end)
                if sentence_end > start + self.chunk_size // 3:  # ë„ˆë¬´ ì§§ì§€ ì•Šë‹¤ë©´
                    end = sentence_end
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            # ë‹¤ìŒ ì‹œì‘ ìœ„ì¹˜ (ì˜¤ë²„ë© ê³ ë ¤)
            start = end - self.chunk_overlap
            if start >= len(text):
                break
        
        return chunks
    
    def _find_best_sentence_boundary(self, text: str, start: int, preferred_end: int) -> int:
        """
        ìµœì ì˜ ë¬¸ì¥ ê²½ê³„ ì°¾ê¸° (ë‹¤ì–‘í•œ êµ¬ë‘ì  ê³ ë ¤)
        """
        # ì„ í˜¸í•˜ëŠ” êµ¬ë‘ì  ìˆœì„œ (ê°•í•œ ë¬¸ì¥ ë â†’ ì•½í•œ ë¬¸ì¥ ë)
        sentence_endings = [
            r'[.!?ã€‚ï¼Ÿï¼]\s+',  # ê°•í•œ ë¬¸ì¥ ë + ê³µë°±
            r'[.!?ã€‚ï¼Ÿï¼][\n\r]',  # ê°•í•œ ë¬¸ì¥ ë + ì¤„ë°”ê¿ˆ
            r'[.!?ã€‚ï¼Ÿï¼]$',  # ê°•í•œ ë¬¸ì¥ ë + í…ìŠ¤íŠ¸ ë
            r'[.!?ã€‚ï¼Ÿï¼]',  # ê°•í•œ ë¬¸ì¥ ë
            r'[;:][\s\n]',  # ì„¸ë¯¸ì½œë¡ , ì½œë¡  + ê³µë°±/ì¤„ë°”ê¿ˆ
            r'\n\s*\n',  # ë¬¸ë‹¨ ê²½ê³„
        ]
        
        search_start = max(start, preferred_end - self.chunk_size // 2)
        search_end = min(len(text), preferred_end + 100)  # ì•½ê°„ì˜ ì—¬ìœ 
        
        for pattern in sentence_endings:
            for match in re.finditer(pattern, text[search_start:search_end]):
                boundary = search_start + match.end()
                if start + self.chunk_size // 3 <= boundary <= preferred_end + 100:
                    return boundary
        
        # ì ì ˆí•œ ê²½ê³„ë¥¼ ì°¾ì§€ ëª»í•˜ë©´ ì›ë˜ ìœ„ì¹˜ ë°˜í™˜
        return preferred_end
    
    def _apply_overlap_to_chunks(self, chunks: List[str], original_text: str) -> List[str]:
        """
        ì²­í¬ë“¤ì— ì˜¤ë²„ë© ì ìš© (ë¬¸ë‹¨ ê¸°ë°˜ ì²­í¬í™”ìš©)
        """
        if not chunks or len(chunks) == 1:
            return chunks
        
        overlapped_chunks = [chunks[0]]  # ì²« ë²ˆì§¸ ì²­í¬ëŠ” ê·¸ëŒ€ë¡œ
        
        for i in range(1, len(chunks)):
            current_chunk = chunks[i]
            
            # ì´ì „ ì²­í¬ì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ì„ í˜„ì¬ ì²­í¬ ì•ì— ì¶”ê°€
            prev_chunk = chunks[i-1]
            overlap_text = self._extract_overlap_text(prev_chunk)
            
            if overlap_text:
                overlapped_chunk = overlap_text + "\n\n" + current_chunk
            else:
                overlapped_chunk = current_chunk
            
            overlapped_chunks.append(overlapped_chunk)
        
        return overlapped_chunks
    
    def _extract_overlap_text(self, chunk: str) -> str:
        """
        ì²­í¬ì—ì„œ ì˜¤ë²„ë©í•  í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë§ˆì§€ë§‰ ë¬¸ì¥ë“¤)
        """
        if len(chunk) <= self.chunk_overlap:
            return chunk
        
        # ë§ˆì§€ë§‰ ë¶€ë¶„ì—ì„œ ë¬¸ì¥ ê²½ê³„ ì°¾ê¸°
        overlap_start = len(chunk) - self.chunk_overlap
        sentences = re.split(r'[.!?ã€‚ï¼Ÿï¼]\s+', chunk[overlap_start:])
        
        if len(sentences) > 1:
            # ì™„ì „í•œ ë¬¸ì¥ë“¤ë§Œ í¬í•¨
            return '. '.join(sentences[1:])  # ì²« ë²ˆì§¸ëŠ” ë¶ˆì™„ì „í•  ìˆ˜ ìˆìŒ
        else:
            # ë¬¸ì¥ ë¶„í• ì´ ì•ˆë˜ë©´ ë‹¨ìˆœíˆ ë§ˆì§€ë§‰ ë¶€ë¶„ ì‚¬ìš©
            return chunk[-self.chunk_overlap:]
    
    def _create_chunk_summary_prompt(self, chunk: str, topic: str, context_type: str = None) -> str:
        """ì²­í¬ë³„ ìš”ì•½ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        base_prompt = f"""
Summarize this text chunk for a debate on "{topic}".

TEXT CHUNK:
{chunk}

Create 2-3 bullet points focusing on:
- Key facts relevant to the debate topic
- Important data, statistics, or evidence
- Expert opinions or conclusions
"""
        
        base_prompt += """
Format as bullet points (â€¢). Be concise but informative.
"""
        
        return base_prompt
    
    def _create_final_summary_prompt(self, partial_summaries: str, topic: str, context_type: str = None) -> str:
        """ìµœì¢… ìš”ì•½ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        template = SummaryTemplates.get_template(context_type=context_type)
        return template.format(context=partial_summaries, topic=topic)
    
    def _estimate_tokens(self, text: str) -> int:
        """
        í…ìŠ¤íŠ¸ì˜ ëŒ€ëµì ì¸ í† í° ìˆ˜ ì¶”ì • (1 í† í° â‰ˆ 4ì)
        """
        return len(text) // 4
    
    def get_context_bullet_points(self, max_points: int = None) -> List[str]:
        """
        í˜„ì¬ í™œì„± ì»¨í…ìŠ¤íŠ¸ë“¤ì˜ í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ë¶ˆë › í˜•íƒœë¡œ ë°˜í™˜
        
        Args:
            max_points: ìµœëŒ€ ë°˜í™˜í•  í¬ì¸íŠ¸ ìˆ˜
        
        Returns:
            ë¶ˆë › í¬ì¸íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        max_points = max_points or self.max_summary_points
        
        if not self.active_contexts:
            return []
        
        all_bullet_points = []
        
        for ctx_id in self.active_contexts:
            if ctx_id in self.context_bullet_points:
                # ìºì‹œëœ ë¶ˆë › í¬ì¸íŠ¸ ì‚¬ìš©
                all_bullet_points.extend(self.context_bullet_points[ctx_id])
            else:
                # ìƒˆë¡œ ìƒì„±
                context = self.user_contexts.get(ctx_id)
                if context:
                    bullet_points = self._generate_bullet_points_for_context(context)
                    self.context_bullet_points[ctx_id] = bullet_points
                    all_bullet_points.extend(bullet_points)
        
        # ì¤‘ë³µ ì œê±° ë° ê¸¸ì´ ì œí•œ
        unique_points = []
        seen = set()
        
        for point in all_bullet_points:
            point_key = point.lower().strip("â€¢ ").strip()
            if point_key not in seen and len(unique_points) < max_points:
                unique_points.append(point)
                seen.add(point_key)
        
        return unique_points[:max_points]
    
    def get_context_for_prompt(self, topic: str = "", include_full_context: bool = False) -> Dict[str, Any]:
        """
        í”„ë¡¬í”„íŠ¸ì— í¬í•¨í•  ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ë°˜í™˜
        
        Args:
            topic: ì£¼ì œ  
            include_full_context: ì „ì²´ ì»¨í…ìŠ¤íŠ¸ í¬í•¨ ì—¬ë¶€
        
        Returns:
            í”„ë¡¬í”„íŠ¸ìš© ì»¨í…ìŠ¤íŠ¸ ë”•ì…”ë„ˆë¦¬
        """
        result = {
            "has_context": len(self.active_contexts) > 0,
            "summary": "",
            "bullet_points": [],
            "full_context": "",
            "context_length": 0,
            "summarization_strategy": "none"
        }
        
        if not self.active_contexts:
            return result
        
        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì •ë³´ ì¶”ê°€
        combined_context = self._combine_active_contexts()
        result["context_length"] = len(combined_context)
        result["summarization_strategy"] = "single" if len(combined_context) < self.short_context_threshold else "hierarchical"
        
        # ê°ê´€ì  ìš”ì•½
        if topic:
            result["summary"] = self.get_objective_summary(topic)
        
        # ë¶ˆë › í¬ì¸íŠ¸
        result["bullet_points"] = self.get_context_bullet_points()
        
        # ì „ì²´ ì»¨í…ìŠ¤íŠ¸ (ì˜µì…˜)
        if include_full_context:
            result["full_context"] = combined_context
        
        return result
    
    def refresh_summaries(self, topic: str = None):
        """
        ìºì‹œëœ ìš”ì•½ë“¤ ìƒˆë¡œê³ ì¹¨
        
        Args:
            topic: íŠ¹ì • ì£¼ì œë§Œ ìƒˆë¡œê³ ì¹¨ (Noneì´ë©´ ì „ì²´)
        """
        if topic:
            # íŠ¹ì • ì£¼ì œì™€ ê´€ë ¨ëœ ìºì‹œë§Œ ì‚­ì œ
            keys_to_remove = [key for key in self.summaries.keys() if key.startswith(topic)]
            for key in keys_to_remove:
                del self.summaries[key]
            logger.info(f"Refreshed summaries for topic: {topic}")
        else:
            # ì „ì²´ ìºì‹œ ì‚­ì œ
            self.summaries.clear()
            self.context_bullet_points.clear()
            logger.info("Refreshed all cached summaries")
    
    def _combine_active_contexts(self) -> str:
        """í™œì„± ì»¨í…ìŠ¤íŠ¸ë“¤ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í†µí•©"""
        combined = []
        
        for ctx_id in self.active_contexts:
            context = self.user_contexts.get(ctx_id)
            if context:
                title = context.get("title", "Unknown")
                content = context.get("content", "")
                
                # ì»¨í…ìŠ¤íŠ¸ë³„ êµ¬ë¶„ì„ ìœ„í•œ í—¤ë” ì¶”ê°€
                combined.append(f"=== {title} ===")
                combined.append(content)
                combined.append("")  # ë¹ˆ ì¤„ë¡œ êµ¬ë¶„
        
        return "\n".join(combined)
    
    def _extract_bullet_points(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆë › í¬ì¸íŠ¸ ì¶”ì¶œ ë° ì •ì œ"""
        lines = text.split('\n')
        bullet_points = []
        
        for line in lines:
            line = line.strip()
            # ë¶ˆë › í¬ì¸íŠ¸ ì‹ë³„ (â€¢, -, *, ìˆ«ì ë“±)
            if re.match(r'^[â€¢\-\*]\s+|^\d+[\.\)]\s+', line):
                # ë¶ˆë › ë§ˆì»¤ ì •ê·œí™”
                cleaned = re.sub(r'^[â€¢\-\*\d\.\)]+\s*', 'â€¢ ', line)
                if cleaned.strip() and len(cleaned) > 3:  # ì˜ë¯¸ìˆëŠ” ë‚´ìš©ë§Œ
                    bullet_points.append(cleaned)
        
        # ë¶ˆë › í¬ì¸íŠ¸ê°€ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ëª‡ ë¬¸ì¥ì„ ë¶ˆë ›ìœ¼ë¡œ ë³€í™˜
        if not bullet_points:
            sentences = re.split(r'(?<=[.!?])\s+', text)
            for i, sentence in enumerate(sentences[:self.max_summary_points]):
                if sentence.strip() and len(sentence) > 10:
                    bullet_points.append(f"â€¢ {sentence.strip()}")
        
        return bullet_points[:self.max_summary_points]
    
    def _generate_bullet_points_for_context(self, context: Dict[str, Any]) -> List[str]:
        """ë‹¨ì¼ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆë › í¬ì¸íŠ¸ ìƒì„± (ì ì‘ì  ì „ëµ ì‚¬ìš©)"""
        content = context.get("content", "")
        title = context.get("title", "")
        
        if not content.strip():
            return []
        
        try:
            # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë”°ë¥¸ ì „ëµ ê²°ì •
            if len(content) < self.short_context_threshold:
                # ì§§ì€ ì»¨í…ìŠ¤íŠ¸: ë‹¨ì¼ ìš”ì²­
                prompt = f"""
Create 2-3 key bullet points from the following content:

TITLE: {title}
CONTENT: {content}

Format as:
â€¢ [Point 1]
â€¢ [Point 2]  
â€¢ [Point 3]

Focus on the most important factual information.
"""
                
                summary = self.llm_manager.generate_response(
                    system_prompt="You are a helpful assistant that extracts key points from text.",
                    user_prompt=prompt,
                    llm_model="gpt-4",
                    max_tokens=400
                )
                
                return self._extract_bullet_points(summary)
            else:
                # ê¸´ ì»¨í…ìŠ¤íŠ¸: ê³„ì¸µì  ì²˜ë¦¬
                chunks = self._split_into_chunks(content)
                all_points = []
                
                for chunk in chunks[:3]:  # ìµœëŒ€ 3ê°œ ì²­í¬ë§Œ ì²˜ë¦¬
                    prompt = f"""
Create 1-2 key bullet points from this text chunk:

{chunk}

Format as bullet points (â€¢). Focus on the most important information.
"""
                    
                    summary = self.llm_manager.generate_response(
                        system_prompt="You are extracting key points from a text chunk.",
                        user_prompt=prompt,
                        llm_model="gpt-4",
                        max_tokens=200
                    )
                    
                    points = self._extract_bullet_points(summary)
                    all_points.extend(points)
                
                # ì¤‘ë³µ ì œê±°í•˜ê³  ìµœëŒ€ 3ê°œë§Œ ë°˜í™˜
                unique_points = []
                seen = set()
                for point in all_points:
                    point_key = point.lower().strip("â€¢ ").strip()
                    if point_key not in seen and len(unique_points) < 3:
                        unique_points.append(point)
                        seen.add(point_key)
                
                return unique_points
                
        except Exception as e:
            logger.error(f"Error generating bullet points for context {title}: {str(e)}")
            
            # í´ë°±: ì²« ë²ˆì§¸ ë¬¸ì¥ë“¤ì„ ë¶ˆë ›ìœ¼ë¡œ ë³€í™˜
            sentences = re.split(r'(?<=[.!?])\s+', content)
            bullet_points = []
            for sentence in sentences[:3]:
                if sentence.strip() and len(sentence) > 20:
                    bullet_points.append(f"â€¢ {sentence.strip()}")
            
            return bullet_points
    
    def get_context_stats(self) -> Dict[str, Any]:
        """ì»¨í…ìŠ¤íŠ¸ í†µê³„ ì •ë³´ ë°˜í™˜ (ë””ë²„ê¹…ìš©)"""
        combined_length = len(self._combine_active_contexts()) if self.active_contexts else 0
        
        return {
            "total_contexts": len(self.user_contexts),
            "active_contexts": len(self.active_contexts),
            "combined_context_length": combined_length,
            "summarization_strategy": "single" if combined_length < self.short_context_threshold else "hierarchical",
            "short_context_threshold": self.short_context_threshold,
            "chunk_size": self.chunk_size,
            "cached_summaries": len(self.summaries),
            "cached_bullet_points": len(self.context_bullet_points),
            "max_summary_points": self.max_summary_points,
            "context_details": [
                {
                    "id": ctx_id,
                    "title": ctx.get("title", "Unknown"),
                    "type": ctx.get("type", "unknown"),
                    "length": len(ctx.get("content", "")),
                    "active": ctx_id in self.active_contexts
                }
                for ctx_id, ctx in self.user_contexts.items()
            ]
        }

    def get_objective_summary(self, topic: str = "", context_type: str = None) -> str:
        """
        ê°ê´€ì ì¸ ì»¨í…ìŠ¤íŠ¸ ìš”ì•½ ë°˜í™˜ (ìƒˆë¡œìš´ ê°„í¸ ë©”ì„œë“œ)
        
        Args:
            topic: í† ë¡  ì£¼ì œ
            context_type: ì»¨í…ìŠ¤íŠ¸ íƒ€ì…
        
        Returns:
            ê°ê´€ì  ìš”ì•½ í…ìŠ¤íŠ¸
        """
        if not topic:
            # í† ë¡  ì£¼ì œê°€ ì—†ìœ¼ë©´ ì¼ë°˜ì ì¸ ë¶ˆë › í¬ì¸íŠ¸ ë°˜í™˜
            bullet_points = self.get_context_bullet_points()
            return "\n".join(bullet_points)
        
        # ê°ê´€ì  ìš”ì•½ ìƒì„±
        result = self.generate_summary(topic, context_type=context_type)
        return result.get("summary", "")
    
    def _truncate_context_intelligently(self, context: str) -> str:
        """
        ì»¨í…ìŠ¤íŠ¸ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ìë¥´ê¸°
        - ì•ë¶€ë¶„ (ë„ì…ë¶€) ìœ ì§€
        - ì¤‘ê°„ ë¶€ë¶„ ì••ì¶•
        - ëë¶€ë¶„ (ê²°ë¡ ) ìœ ì§€
        """
        target_length = self.max_processable_length
        
        if len(context) <= target_length:
            return context
        
        # ì•ë¶€ë¶„ 30%, ëë¶€ë¶„ 30%, ì¤‘ê°„ 40% ë¹„ìœ¨ë¡œ ë¶„ë°°
        front_size = int(target_length * 0.3)
        back_size = int(target_length * 0.3)
        middle_size = target_length - front_size - back_size
        
        # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ìë¥´ê¸° ì‹œë„
        paragraphs = context.split('\n\n')
        
        # ì•ë¶€ë¶„ í™•ë³´
        front_text = ""
        for para in paragraphs:
            if len(front_text) + len(para) < front_size:
                front_text += para + "\n\n"
            else:
                break
        
        # ëë¶€ë¶„ í™•ë³´
        back_text = ""
        for para in reversed(paragraphs):
            if len(back_text) + len(para) < back_size:
                back_text = para + "\n\n" + back_text
            else:
                break
        
        # ì¤‘ê°„ ë¶€ë¶„ ìƒ˜í”Œë§
        remaining_paras = paragraphs[len(front_text.split('\n\n')):len(paragraphs)-len(back_text.split('\n\n'))]
        middle_text = ""
        
        if remaining_paras and middle_size > 0:
            # ì¤‘ê°„ ë¬¸ë‹¨ë“¤ì„ ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§
            step = max(1, len(remaining_paras) // (middle_size // 200))  # ëŒ€ëµ 200ìë‹¹ 1ë¬¸ë‹¨
            sampled_paras = remaining_paras[::step]
            
            for para in sampled_paras:
                if len(middle_text) + len(para) < middle_size:
                    middle_text += para + "\n\n"
                else:
                    break
        
        truncated = front_text + middle_text + back_text
        
        # ì—¬ì „íˆ ë„ˆë¬´ ê¸¸ë©´ ë‹¨ìˆœ ìë¥´ê¸°
        if len(truncated) > target_length:
            truncated = truncated[:target_length]
        
        logger.info(f"Context truncated: {len(context):,} â†’ {len(truncated):,} chars")
        return truncated.strip()
    
    def _select_most_important_chunks(self, chunks: List[str], max_chunks: int) -> List[str]:
        """
        ê°€ì¥ ì¤‘ìš”í•œ ì²­í¬ë“¤ ì„ íƒ
        - ì•ë¶€ë¶„ (ë„ì…ë¶€)
        - ì¤‘ê°„ë¶€ë¶„ (ê· ë“± ìƒ˜í”Œë§)  
        - ëë¶€ë¶„ (ê²°ë¡ )
        """
        if len(chunks) <= max_chunks:
            return chunks
        
        # ì•ë¶€ë¶„ 3ê°œ, ëë¶€ë¶„ 3ê°œ, ë‚˜ë¨¸ì§€ëŠ” ì¤‘ê°„ì—ì„œ ê· ë“± ìƒ˜í”Œë§
        front_count = min(3, max_chunks // 3)
        back_count = min(3, max_chunks // 3)
        middle_count = max_chunks - front_count - back_count
        
        selected_chunks = []
        
        # ì•ë¶€ë¶„ ì¶”ê°€
        selected_chunks.extend(chunks[:front_count])
        
        # ì¤‘ê°„ë¶€ë¶„ ê· ë“± ìƒ˜í”Œë§
        if middle_count > 0 and len(chunks) > front_count + back_count:
            middle_chunks = chunks[front_count:len(chunks)-back_count]
            if middle_chunks:
                step = max(1, len(middle_chunks) // middle_count)
                sampled_middle = middle_chunks[::step][:middle_count]
                selected_chunks.extend(sampled_middle)
        
        # ëë¶€ë¶„ ì¶”ê°€
        if back_count > 0:
            selected_chunks.extend(chunks[-back_count:])
        
        logger.info(f"Selected {len(selected_chunks)} most important chunks from {len(chunks)} total chunks")
        return selected_chunks 