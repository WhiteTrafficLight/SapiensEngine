"""
Fast Debate Opening Service

ê¸°ì¡´ 55ì´ˆ â†’ 3ì´ˆë¡œ ë‹¨ì¶•ì‹œí‚¤ëŠ” í•µì‹¬ ì„œë¹„ìŠ¤
ê¸°ì¡´ chat.pyì™€ í˜¸í™˜ë˜ëŠ” ì¸í„°í˜ì´ìŠ¤ ì œê³µ
"""

import asyncio
import time
import logging
from typing import Dict, Any, Optional
import redis
import json
import hashlib

from ..models.debate_models import (
    FastDebateRequest, FastDebateResponse, DebatePackage,
    ModeratorStyle, ContextType, PerformanceMetrics
)
from .openai_service import OpenAIDebateService

logger = logging.getLogger(__name__)

class FastDebateOpeningService:
    """ğŸš€ ì´ˆê³ ì† í† ë¡  ì˜¤í”„ë‹ ìƒì„± ì„œë¹„ìŠ¤"""
    
    def __init__(self, use_cache: bool = True, use_fine_tuned: bool = False):
        self.openai_service = OpenAIDebateService(use_fine_tuned=use_fine_tuned)
        self.use_cache = use_cache
        
        # Redis ìºì‹œ (ì„ íƒì )
        try:
            if use_cache:
                self.redis_client = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)
                self.cache_ttl = 24 * 60 * 60  # 24ì‹œê°„
            else:
                self.redis_client = None
        except Exception as e:
            logger.warning(f"Redis connection failed, caching disabled: {e}")
            self.redis_client = None
    
    async def create_fast_debate_room(self, 
                                    room_id: str,
                                    title: str, 
                                    context: str = "",
                                    pro_npcs: list = None,
                                    con_npcs: list = None,
                                    user_ids: list = None,
                                    user_side: str = "neutral",
                                    moderator_style: str = "0") -> FastDebateResponse:
        """ğŸ”¥ ê¸°ì¡´ chat.py í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤ - 3ì´ˆ ëª©í‘œ"""
        
        total_start = time.time()
        
        # ìš”ì²­ ê°ì²´ ìƒì„±
        request = FastDebateRequest(
            room_id=room_id,
            title=title,
            context=context,
            context_type=self._detect_context_type(context),
            pro_npcs=pro_npcs or [],
            con_npcs=con_npcs or [],
            user_ids=user_ids or [],
            user_side=user_side,
            moderator_style=ModeratorStyle(moderator_style)
        )
        
        # 1ë‹¨ê³„: ìºì‹œ í™•ì¸ (0.1ì´ˆ)
        cache_start = time.time()
        cached_package = await self._get_from_cache(request)
        cache_time = time.time() - cache_start
        
        if cached_package:
            logger.info(f"ğŸ¯ Cache HIT! Returning cached result in {cache_time:.3f}s")
            return FastDebateResponse(
                room_id=room_id,
                debate_package=cached_package,
                performance_metrics={
                    "total_time": time.time() - total_start,
                    "cache_hit": True,
                    "cache_time": cache_time
                },
                cache_hit=True
            )
        
        # 2ë‹¨ê³„: ë¹ ë¥¸ ìƒì„± (2-3ì´ˆ)
        api_start = time.time()
        debate_package = await self.openai_service.generate_complete_debate_package(request)
        api_time = time.time() - api_start
        
        # 3ë‹¨ê³„: ìºì‹œì— ì €ì¥ (ë°±ê·¸ë¼ìš´ë“œ)
        if self.redis_client:
            asyncio.create_task(self._save_to_cache(request, debate_package))
        
        total_time = time.time() - total_start
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        performance_metrics = PerformanceMetrics(
            total_time=total_time,
            api_call_time=api_time,
            cache_check_time=cache_time,
            processing_time=total_time - api_time - cache_time,
            system_version="v2_fast"
        )
        
        logger.info(f"âœ… Fast debate created in {total_time:.2f}s (API: {api_time:.2f}s)")
        
        return FastDebateResponse(
            room_id=room_id,
            debate_package=debate_package,
            performance_metrics=performance_metrics.dict(),
            cache_hit=False
        )
    
    async def _get_from_cache(self, request: FastDebateRequest) -> Optional[DebatePackage]:
        """ìºì‹œì—ì„œ í† ë¡  íŒ¨í‚¤ì§€ ì¡°íšŒ"""
        
        if not self.redis_client:
            return None
        
        try:
            cache_key = self._generate_cache_key(request)
            cached_data = self.redis_client.get(cache_key)
            
            if cached_data:
                package_data = json.loads(cached_data)
                return DebatePackage(**package_data)
                
        except Exception as e:
            logger.warning(f"Cache retrieval failed: {e}")
        
        return None
    
    async def _save_to_cache(self, request: FastDebateRequest, package: DebatePackage):
        """ìºì‹œì— í† ë¡  íŒ¨í‚¤ì§€ ì €ì¥ (ë°±ê·¸ë¼ìš´ë“œ)"""
        
        if not self.redis_client:
            return
        
        try:
            cache_key = self._generate_cache_key(request)
            package_json = package.json()
            
            self.redis_client.setex(
                cache_key, 
                self.cache_ttl, 
                package_json
            )
            
            logger.info(f"ğŸ’¾ Cached debate package: {cache_key}")
            
        except Exception as e:
            logger.warning(f"Cache save failed: {e}")
    
    def _generate_cache_key(self, request: FastDebateRequest) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        
        # ì¤‘ìš”í•œ í•„ë“œë“¤ë¡œ í•´ì‹œ ìƒì„±
        key_data = {
            "title": request.title,
            "context": request.context[:100] if request.context else "",  # ì»¨í…ìŠ¤íŠ¸ëŠ” ì²˜ìŒ 100ìë§Œ
            "pro_npcs": sorted(request.pro_npcs),
            "con_npcs": sorted(request.con_npcs),
            "moderator_style": request.moderator_style.value
        }
        
        key_string = json.dumps(key_data, sort_keys=True)
        hash_key = hashlib.md5(key_string.encode()).hexdigest()
        
        return f"debate_package:{hash_key}"
    
    def _detect_context_type(self, context: str) -> ContextType:
        """ì»¨í…ìŠ¤íŠ¸ íƒ€ì… ìë™ ê°ì§€"""
        
        if not context.strip():
            return ContextType.EMPTY
        
        context_lower = context.lower().strip()
        
        if context_lower.startswith(('http://', 'https://', 'www.')):
            return ContextType.URL
        elif context_lower.endswith('.pdf') or 'pdf' in context_lower:
            return ContextType.PDF
        else:
            return ContextType.TEXT
    
    async def get_compatible_opening_data(self, request: FastDebateRequest) -> Dict[str, Any]:
        """ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ í˜¸í™˜ë˜ëŠ” ì˜¤í”„ë‹ ë°ì´í„° ë°˜í™˜"""
        
        response = await self.create_fast_debate_room(
            room_id=request.room_id,
            title=request.title,
            context=request.context,
            pro_npcs=request.pro_npcs,
            con_npcs=request.con_npcs,
            user_ids=request.user_ids,
            user_side=request.user_side,
            moderator_style=request.moderator_style.value
        )
        
        package = response.debate_package
        
        # ê¸°ì¡´ ì‹œìŠ¤í…œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        return {
            "stance_statements": {
                "pro": package.stance_statements.pro,
                "con": package.stance_statements.con
            },
            "opening_message": package.opening_message,
            "context_summary": package.context_summary.dict() if package.context_summary else None,
            "performance": {
                "generation_time": package.generation_time,
                "system_version": package.system_version,
                "cache_hit": response.cache_hit
            }
        }

    async def warm_popular_cache(self, popular_combinations: list):
        """ì¸ê¸° ì¡°í•©ë“¤ì„ ë¯¸ë¦¬ ìºì‹œì— ì €ì¥"""
        
        logger.info(f"ğŸ”¥ Warming cache for {len(popular_combinations)} popular combinations")
        
        tasks = []
        for combo in popular_combinations:
            request = FastDebateRequest(**combo)
            task = self.openai_service.generate_complete_debate_package(request)
            tasks.append((request, task))
        
        # ë³‘ë ¬ ì²˜ë¦¬
        for request, task in tasks:
            try:
                package = await task
                await self._save_to_cache(request, package)
            except Exception as e:
                logger.error(f"Failed to warm cache for {request.title}: {e}")
        
        logger.info("âœ… Cache warming completed")

    def get_performance_summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ ë°˜í™˜"""
        
        # ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” ë©”íŠ¸ë¦­ì„ ì¶”ì í•˜ê³  ì €ì¥í•´ì•¼ í•¨
        return {
            "system_version": "v2_fast",
            "target_time": "3 seconds",
            "cache_enabled": self.redis_client is not None,
            "fine_tuned_model": self.openai_service.use_fine_tuned
        }


# ê¸°ì¡´ chat.pyì™€ì˜ í†µí•©ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜ë“¤
async def create_fast_debate_compatible(room_id: str, 
                                      title: str,
                                      context: str = "",
                                      pro_npcs: list = None,
                                      con_npcs: list = None,
                                      user_ids: list = None,
                                      user_side: str = "neutral", 
                                      moderator_style: str = "0") -> Dict[str, Any]:
    """ğŸ”¥ ê¸°ì¡´ chat.pyì—ì„œ ì§ì ‘ í˜¸ì¶œí•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜"""
    
    service = FastDebateOpeningService(use_cache=True, use_fine_tuned=False)
    
    response = await service.create_fast_debate_room(
        room_id=room_id,
        title=title, 
        context=context,
        pro_npcs=pro_npcs or [],
        con_npcs=con_npcs or [],
        user_ids=user_ids or [],
        user_side=user_side,
        moderator_style=moderator_style
    )
    
    # ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
    package = response.debate_package
    
    return {
        "status": "success",
        "room_id": room_id,
        "stance_statements": {
            "pro": package.stance_statements.pro,
            "con": package.stance_statements.con
        },
        "opening_message": package.opening_message,
        "context_summary": package.context_summary.dict() if package.context_summary else None,
        "philosopher_profiles": [p.dict() for p in package.philosopher_profiles],
        "performance": response.performance_metrics,
        "system_version": "v2_fast",
        "cache_hit": response.cache_hit
    }

async def test_vs_original_system(test_cases: list) -> Dict[str, Any]:
    """ê¸°ì¡´ ì‹œìŠ¤í…œ vs ìƒˆ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¹„êµ"""
    
    service = FastDebateOpeningService()
    
    # ìƒˆ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
    new_system_results = []
    total_start = time.time()
    
    for case in test_cases:
        start = time.time()
        try:
            result = await service.create_fast_debate_room(**case)
            duration = time.time() - start
            new_system_results.append({
                "topic": case["title"],
                "time": duration,
                "success": True,
                "cache_hit": result.cache_hit
            })
        except Exception as e:
            new_system_results.append({
                "topic": case["title"], 
                "time": time.time() - start,
                "success": False,
                "error": str(e)
            })
    
    total_new_time = time.time() - total_start
    
    return {
        "new_system": {
            "total_time": total_new_time,
            "average_time": total_new_time / len(test_cases),
            "results": new_system_results
        },
        "estimated_original_time": len(test_cases) * 55,  # ê¸°ì¡´ ì‹œìŠ¤í…œ ì¶”ì •ì¹˜
        "improvement": f"{((len(test_cases) * 55 - total_new_time) / (len(test_cases) * 55) * 100):.1f}%"
    } 