"""
Fine-tuning Model Training Script

ìƒì„±ëœ í•™ìŠµ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ OpenAI ëª¨ë¸ íŒŒì¸íŠœë‹
ğŸ”¥ ì—¬ê¸°ì„œ íŒŒì¸íŠœë‹ ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ë°°í¬í•©ë‹ˆë‹¤
"""

import os
import time
import json
from pathlib import Path
from typing import Dict, Any, Optional
import openai
from openai import OpenAI

class DebateModeratorTrainer:
    """í† ë¡  ëª¨ë”ë ˆì´í„° íŒŒì¸íŠœë‹ íŠ¸ë ˆì´ë„ˆ"""
    
    def __init__(self, api_key: Optional[str] = None):
        self.client = OpenAI(api_key=api_key)
        self.training_dir = Path("src/new/fine_tuning")
        
        # íŒŒì¸íŠœë‹ ì„¤ì •
        self.model_config = {
            "model": "gpt-4o-mini-2024-07-18",  # íŒŒì¸íŠœë‹ ê°€ëŠ¥í•œ ëª¨ë¸
            "training_file": None,
            "validation_file": None,
            "hyperparameters": {
                "n_epochs": 3,  # ì ì€ ì—í¬í¬ë¡œ ì‹œì‘ (ê³¼ì í•© ë°©ì§€)
                "batch_size": 1,
                "learning_rate_multiplier": 1.0
            },
            "suffix": "debate-moderator-v1"
        }
    
    def upload_training_files(self) -> tuple[str, str]:
        """ğŸ”¥ í•™ìŠµ/ê²€ì¦ íŒŒì¼ì„ OpenAIì— ì—…ë¡œë“œ"""
        
        train_file = self.training_dir / "train_data.jsonl"
        val_file = self.training_dir / "validation_data.jsonl"
        
        if not train_file.exists() or not val_file.exists():
            raise FileNotFoundError(
                "Training data not found. Run prepare_training_data.py first."
            )
        
        print("ğŸ“¤ Uploading training files to OpenAI...")
        
        # í•™ìŠµ íŒŒì¼ ì—…ë¡œë“œ
        print(f"ğŸ“š Uploading training file: {train_file}")
        with open(train_file, "rb") as f:
            training_file = self.client.files.create(
                file=f,
                purpose="fine-tune"
            )
        
        # ê²€ì¦ íŒŒì¼ ì—…ë¡œë“œ
        print(f"ğŸ” Uploading validation file: {val_file}")
        with open(val_file, "rb") as f:
            validation_file = self.client.files.create(
                file=f,
                purpose="fine-tune"
            )
        
        print(f"âœ… Training file uploaded: {training_file.id}")
        print(f"âœ… Validation file uploaded: {validation_file.id}")
        
        return training_file.id, validation_file.id
    
    def start_fine_tuning(self, training_file_id: str, validation_file_id: str) -> str:
        """ğŸš€ íŒŒì¸íŠœë‹ ì‘ì—… ì‹œì‘"""
        
        print("ğŸš€ Starting fine-tuning job...")
        
        fine_tuning_job = self.client.fine_tuning.jobs.create(
            training_file=training_file_id,
            validation_file=validation_file_id,
            model=self.model_config["model"],
            hyperparameters=self.model_config["hyperparameters"],
            suffix=self.model_config["suffix"]
        )
        
        job_id = fine_tuning_job.id
        print(f"âœ… Fine-tuning job started: {job_id}")
        print(f"ğŸ“Š Model: {self.model_config['model']}")
        print(f"âš™ï¸ Hyperparameters: {self.model_config['hyperparameters']}")
        
        return job_id
    
    def monitor_training(self, job_id: str) -> Dict[str, Any]:
        """ğŸ“Š í›ˆë ¨ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§"""
        
        print(f"ğŸ“Š Monitoring training job: {job_id}")
        print("â³ This may take 10-30 minutes depending on data size...")
        
        while True:
            job = self.client.fine_tuning.jobs.retrieve(job_id)
            status = job.status
            
            print(f"ğŸ“ˆ Status: {status}")
            
            if status == "succeeded":
                print("ğŸ‰ Training completed successfully!")
                model_id = job.fine_tuned_model
                print(f"ğŸ¤– Fine-tuned model ID: {model_id}")
                
                # í›ˆë ¨ ê²°ê³¼ ì €ì¥
                self._save_training_results(job_id, model_id, job)
                
                return {
                    "status": "succeeded",
                    "model_id": model_id,
                    "job_id": job_id,
                    "training_file": job.training_file,
                    "validation_file": job.validation_file
                }
            
            elif status == "failed":
                print("âŒ Training failed!")
                error_msg = getattr(job, 'error', 'Unknown error')
                print(f"Error: {error_msg}")
                
                return {
                    "status": "failed",
                    "error": error_msg,
                    "job_id": job_id
                }
            
            elif status in ["validating_files", "queued", "running"]:
                # ì§„í–‰ ì¤‘
                if hasattr(job, 'trained_tokens') and job.trained_tokens:
                    print(f"ğŸ”„ Trained tokens: {job.trained_tokens}")
                
                time.sleep(30)  # 30ì´ˆë§ˆë‹¤ í™•ì¸
            
            else:
                print(f"âš ï¸ Unknown status: {status}")
                time.sleep(30)
    
    def _save_training_results(self, job_id: str, model_id: str, job_info) -> None:
        """í›ˆë ¨ ê²°ê³¼ ì €ì¥"""
        
        results = {
            "job_id": job_id,
            "model_id": model_id,
            "status": job_info.status,
            "created_at": job_info.created_at,
            "finished_at": getattr(job_info, 'finished_at', None),
            "training_file": job_info.training_file,
            "validation_file": getattr(job_info, 'validation_file', None),
            "hyperparameters": getattr(job_info, 'hyperparameters', None),
            "result_files": getattr(job_info, 'result_files', []),
            "trained_tokens": getattr(job_info, 'trained_tokens', None)
        }
        
        results_file = self.training_dir / "training_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ Training results saved to: {results_file}")
    
    def test_fine_tuned_model(self, model_id: str, test_cases: list) -> Dict[str, Any]:
        """ğŸ§ª íŒŒì¸íŠœë‹ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        
        print(f"ğŸ§ª Testing fine-tuned model: {model_id}")
        
        test_results = []
        
        for i, test_case in enumerate(test_cases):
            print(f"ğŸ”¬ Test case {i+1}: {test_case['title'][:50]}...")
            
            start_time = time.time()
            
            try:
                response = self.client.chat.completions.create(
                    model=model_id,
                    messages=[
                        {
                            "role": "system", 
                            "content": "Generate complete debate package as Jamie the Host (casual, friendly, young-style)."
                        },
                        {
                            "role": "user",
                            "content": f"""Create a complete debate package for this topic:

TOPIC: {test_case['title']}

PARTICIPANTS:
- PRO side: {', '.join(test_case.get('pro_npcs', []))}
- CON side: {', '.join(test_case.get('con_npcs', []))}

Use the create_complete_debate_package function to provide structured output."""
                        }
                    ],
                    max_tokens=2000,
                    temperature=0.7
                )
                
                generation_time = time.time() - start_time
                
                test_results.append({
                    "test_case": i + 1,
                    "title": test_case['title'],
                    "generation_time": generation_time,
                    "success": True,
                    "response_length": len(response.choices[0].message.content) if response.choices[0].message.content else 0,
                    "tokens_used": response.usage.total_tokens if hasattr(response, 'usage') else 0
                })
                
            except Exception as e:
                test_results.append({
                    "test_case": i + 1,
                    "title": test_case['title'],
                    "generation_time": time.time() - start_time,
                    "success": False,
                    "error": str(e)
                })
        
        # ê²°ê³¼ ìš”ì•½
        successful_tests = [r for r in test_results if r["success"]]
        success_rate = len(successful_tests) / len(test_results)
        avg_time = sum(r["generation_time"] for r in successful_tests) / len(successful_tests) if successful_tests else 0
        
        summary = {
            "model_id": model_id,
            "total_tests": len(test_results),
            "success_rate": success_rate,
            "average_generation_time": avg_time,
            "test_results": test_results
        }
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥
        test_results_file = self.training_dir / "test_results.json"
        with open(test_results_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š Test Results Summary:")
        print(f"   Success Rate: {success_rate:.1%}")
        print(f"   Average Time: {avg_time:.2f}s")
        print(f"   Results saved to: {test_results_file}")
        
        return summary
    
    def list_fine_tuned_models(self) -> list:
        """ğŸ“‹ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
        
        print("ğŸ“‹ Listing fine-tuned models...")
        
        models = self.client.fine_tuning.jobs.list(limit=10)
        
        for job in models.data:
            print(f"ğŸ¤– Job ID: {job.id}")
            print(f"   Status: {job.status}")
            print(f"   Model: {getattr(job, 'fine_tuned_model', 'N/A')}")
            print(f"   Created: {job.created_at}")
            print()
        
        return models.data
    
    def run_full_training_pipeline(self) -> Dict[str, Any]:
        """ğŸš€ ì „ì²´ í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        
        print("ğŸ¯ Starting full training pipeline...")
        
        try:
            # 1. íŒŒì¼ ì—…ë¡œë“œ
            training_file_id, validation_file_id = self.upload_training_files()
            
            # 2. íŒŒì¸íŠœë‹ ì‹œì‘
            job_id = self.start_fine_tuning(training_file_id, validation_file_id)
            
            # 3. í›ˆë ¨ ëª¨ë‹ˆí„°ë§
            results = self.monitor_training(job_id)
            
            if results["status"] == "succeeded":
                print("\nğŸ‰ Training pipeline completed successfully!")
                print(f"ğŸ¤– Your fine-tuned model: {results['model_id']}")
                print("\nğŸ“‹ Next steps:")
                print("1. Update the model ID in openai_service.py")
                print("2. Test the model in Jupyter notebook")
                print("3. Deploy to production")
                
                return results
            else:
                print(f"\nâŒ Training failed: {results}")
                return results
                
        except Exception as e:
            print(f"âŒ Pipeline error: {str(e)}")
            return {"status": "error", "error": str(e)}

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸš€ Debate Moderator Fine-tuning Script")
    print("=" * 50)
    
    # OpenAI API í‚¤ í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸ OPENAI_API_KEY environment variable not set!")
        print("Please set your OpenAI API key and try again.")
        return
    
    trainer = DebateModeratorTrainer()
    
    # ë©”ë‰´ ì„ íƒ
    print("\nğŸ“‹ Available options:")
    print("1. Run full training pipeline")
    print("2. List existing fine-tuned models")
    print("3. Test specific model")
    
    choice = input("\nEnter your choice (1-3): ").strip()
    
    if choice == "1":
        # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        results = trainer.run_full_training_pipeline()
        
    elif choice == "2":
        # ê¸°ì¡´ ëª¨ë¸ ëª©ë¡
        trainer.list_fine_tuned_models()
        
    elif choice == "3":
        # íŠ¹ì • ëª¨ë¸ í…ŒìŠ¤íŠ¸
        model_id = input("Enter model ID to test: ").strip()
        if model_id:
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
            test_cases = [
                {
                    "title": "Will AI threaten humanity or liberate us?",
                    "pro_npcs": ["nietzsche", "sartre"],
                    "con_npcs": ["kant", "confucius"]
                }
            ]
            trainer.test_fine_tuned_model(model_id, test_cases)
        
    else:
        print("âŒ Invalid choice")

if __name__ == "__main__":
    main() 