"""
í† ë¡  ì°¸ê°€ì ì—ì´ì „íŠ¸ êµ¬í˜„

ì°¬ì„± ë˜ëŠ” ë°˜ëŒ€ ì…ì¥ìœ¼ë¡œ í† ë¡ ì— ì°¸ì—¬í•˜ëŠ” ì—ì´ì „íŠ¸
"""

import logging
import time
import os
import yaml
import json
from typing import Dict, List, Any, Optional

from ..base.agent import Agent
from src.models.llm.llm_manager import LLMManager
from src.agents.utility.debate_emotion_inference import apply_debate_emotion_to_prompt

logger = logging.getLogger(__name__)

class DebateParticipantAgent(Agent):
    """
    í† ë¡  ì°¸ê°€ì ì—ì´ì „íŠ¸
    
    ì°¬ì„± ë˜ëŠ” ë°˜ëŒ€ ì…ì¥ì—ì„œ ì£¼ì¥ì„ í¼ì¹˜ê³  ìƒëŒ€ì˜ ì˜ê²¬ì— ëŒ€ì‘í•˜ëŠ” ì—­í•  ë‹´ë‹¹
    """
    
    def __init__(self, agent_id: str, name: str, config: Dict[str, Any]):
        """
        í† ë¡  ì°¸ê°€ì ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        
        Args:
            agent_id: ê³ ìœ  ì‹ë³„ì
            name: ì—ì´ì „íŠ¸ ì´ë¦„
            config: ì„¤ì • ë§¤ê°œë³€ìˆ˜
        """
        super().__init__(agent_id, name, config)
        
        # ì„±ëŠ¥ ì¸¡ì •ì„ ìœ„í•œ íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë¡
        self.performance_timestamps = {}
        
        # ì°¸ê°€ì ì„±ê²© ë° íŠ¹ì„±
        self.role = config.get("role", "neutral")  # "pro", "con", "neutral"
        self.personality = config.get("personality", "balanced")
        self.knowledge_level = config.get("knowledge_level", "expert")
        self.style = config.get("style", "formal")
        
        # í† ë¡  ì „ëµ ë° ìŠ¤íƒ€ì¼
        self.argumentation_style = config.get("argumentation_style", "logical")  # logical, emotional, factual
        self.response_focus = config.get("response_focus", "balanced")  # attack, defend, balanced
        
        # ì² í•™ì ì •ë³´ ë™ì  ë¡œë“œ
        philosopher_key = config.get("philosopher_key", name.lower())
        philosopher_data = self._load_philosopher_data(philosopher_key)
        
        # ì² í•™ì ê³ ìœ  ì†ì„±ë“¤ (ë™ì  ë¡œë“œëœ ë°ì´í„° ì‚¬ìš©)
        self.philosopher_name = philosopher_data.get("name", name)
        self.philosopher_essence = philosopher_data.get("essence", "")
        self.philosopher_debate_style = philosopher_data.get("debate_style", "")
        self.philosopher_personality = philosopher_data.get("personality", "")
        self.philosopher_key_traits = philosopher_data.get("key_traits", [])
        self.philosopher_quote = philosopher_data.get("quote", "")
        
        # í† ë¡  ìƒíƒœ ë° ì´ë ¥
        self.interaction_history = []
        self.opponent_key_points = []
        self.my_key_points = []
        
        # ì…ë¡  ì¤€ë¹„ ê´€ë ¨ ì†ì„±
        self.core_arguments = []  # í•µì‹¬ ì£¼ì¥ 2-3ê°œ
        self.argument_queries = []  # ê° ì£¼ì¥ì— ëŒ€í•œ RAG ì¿¼ë¦¬ì™€ ì†ŒìŠ¤
        self.prepared_argument = ""  # ë¯¸ë¦¬ ì¤€ë¹„ëœ ì…ë¡ 
        self.argument_prepared = False  # ì…ë¡  ì¤€ë¹„ ì™„ë£Œ ì—¬ë¶€
        
        # ìƒˆë¡œìš´ ìƒíƒœ ê´€ë¦¬ í•„ë“œë“¤ (Option 2 êµ¬í˜„ìš©)
        self.is_preparing_argument = False  # í˜„ì¬ ì…ë¡  ì¤€ë¹„ ì¤‘ì¸ì§€ ì—¬ë¶€
        self.argument_preparation_task = None  # ë¹„ë™ê¸° ì¤€ë¹„ ì‘ì—… ì°¸ì¡°
        self.argument_cache_valid = False  # ìºì‹œëœ ì…ë¡ ì´ ìœ íš¨í•œì§€ ì—¬ë¶€
        self.last_preparation_context = None  # ë§ˆì§€ë§‰ ì¤€ë¹„ ì‹œ ì‚¬ìš©ëœ ì»¨í…ìŠ¤íŠ¸
        
        # ë…¼ì§€ ìŠ¤ì½”ì–´ë§ ë° ê³µê²© ì „ëµ ê´€ë ¨ ì†ì„±
        self.opponent_arguments = {}  # ìƒëŒ€ë°© ë…¼ì§€ ì €ì¥ {speaker_id: [arguments]}
        self.attack_strategies = {}  # ì¤€ë¹„ëœ ê³µê²© ì „ëµ {target_speaker_id: [strategies]}
        self.argument_scores = {}  # ë…¼ì§€ ìŠ¤ì½”ì–´ {argument_id: score_data}
        
        # ì² í•™ìë³„ ì „ëµ ê°€ì¤‘ì¹˜ ë™ì  ë¡œë“œ
        self.strategy_weights = philosopher_data.get("strategy_weights", {})
        
        # ì „ëµ ì •ë³´ ë¡œë“œ
        self.strategy_styles = self._load_strategy_styles()
        
        # LLM ê´€ë¦¬ì ì´ˆê¸°í™”
        self.llm_manager = LLMManager()
    
    def _load_philosopher_data(self, philosopher_key: str) -> Dict[str, Any]:
        """
        YAML íŒŒì¼ì—ì„œ ì² í•™ì ë°ì´í„° ë¡œë“œ
        
        Args:
            philosopher_key: ì² í•™ì í‚¤ (ì˜ˆ: "socrates", "plato")
            
        Returns:
            ì² í•™ì ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        try:
            # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ philosophers/debate_optimized.yaml íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
            current_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = current_dir
            
            # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸° (src í´ë”ê°€ ìˆëŠ” ìƒìœ„ ë””ë ‰í† ë¦¬)
            while project_root and not os.path.exists(os.path.join(project_root, "philosophers")):
                parent = os.path.dirname(project_root)
                if parent == project_root:  # ë£¨íŠ¸ì— ë„ë‹¬
                    break
                project_root = parent
            
            yaml_path = os.path.join(project_root, "philosophers", "debate_optimized.yaml")
            
            if not os.path.exists(yaml_path):
                logger.warning(f"Philosopher YAML file not found at {yaml_path}")
                return self._get_default_philosopher_data(philosopher_key)
            
            with open(yaml_path, 'r', encoding='utf-8') as f:
                philosophers_data = yaml.safe_load(f)
            
            if philosopher_key in philosophers_data:
                logger.info(f"Loaded philosopher data for: {philosopher_key}")
                return philosophers_data[philosopher_key]
            else:
                logger.warning(f"Philosopher '{philosopher_key}' not found in YAML file")
                return self._get_default_philosopher_data(philosopher_key)
                
        except Exception as e:
            logger.error(f"Error loading philosopher data: {str(e)}")
            return self._get_default_philosopher_data(philosopher_key)
    
    def _load_strategy_styles(self) -> Dict[str, Any]:
        """
        JSON íŒŒì¼ì—ì„œ ì „ëµ ìŠ¤íƒ€ì¼ ì •ë³´ ë¡œë“œ
        
        Returns:
            ì „ëµ ìŠ¤íƒ€ì¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            # debate_strategies.json íŒŒì¼ ê²½ë¡œ
            current_dir = os.path.dirname(os.path.abspath(__file__))
            json_path = os.path.join(current_dir, "debate_strategies.json")
            
            if not os.path.exists(json_path):
                logger.warning(f"Strategy JSON file not found at {json_path}")
                return self._get_default_strategy_styles()
            
            with open(json_path, 'r', encoding='utf-8') as f:
                strategies_data = json.load(f)
            
            logger.info(f"Loaded strategy styles from: {json_path}")
            return strategies_data.get("strategy_styles", {})
            
        except Exception as e:
            logger.error(f"Error loading strategy styles: {str(e)}")
            return self._get_default_strategy_styles()
    
    def _get_default_philosopher_data(self, philosopher_key: str) -> Dict[str, Any]:
        """
        ê¸°ë³¸ ì² í•™ì ë°ì´í„° ë°˜í™˜ (íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ì‹œ)
        
        Args:
            philosopher_key: ì² í•™ì í‚¤
            
        Returns:
            ê¸°ë³¸ ì² í•™ì ë°ì´í„°
        """
        return {
            "name": philosopher_key.capitalize(),
            "essence": "A thoughtful philosopher who engages in meaningful debate",
            "debate_style": "Presents logical arguments with clear reasoning",
            "personality": "Analytical and respectful in discourse",
            "key_traits": ["logical reasoning", "clear communication"],
            "quote": "The pursuit of truth through dialogue",
            "strategy_weights": {
                "Clipping": 0.2,
                "Framing Shift": 0.2,
                "Reductive Paradox": 0.15,
                "Conceptual Undermining": 0.15,
                "Ethical Reversal": 0.15,
                "Temporal Delay": 0.1,
                "Philosophical Reframing": 0.05
            }
        }
    
    def _get_default_strategy_styles(self) -> Dict[str, Any]:
        """
        ê¸°ë³¸ ì „ëµ ìŠ¤íƒ€ì¼ ë°˜í™˜ (íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ì‹œ)
        
        Returns:
            ê¸°ë³¸ ì „ëµ ìŠ¤íƒ€ì¼ ë”•ì…”ë„ˆë¦¬
        """
        return {
            "Clipping": {
                "description": "Refute a specific claim directly",
                "style_prompt": "'X' is wrong because...",
                "example": "Direct refutation with evidence"
            },
            "Framing Shift": {
                "description": "Challenge assumptions and reframe the discussion",
                "style_prompt": "You're assuming Y, but what if we asked Z instead?",
                "example": "Shift perspective to deeper questions"
            },
            "Reductive Paradox": {
                "description": "Extend logic to expose flaws",
                "style_prompt": "If we follow your logic to the end, then...",
                "example": "Show extreme consequences"
            },
            "Conceptual Undermining": {
                "description": "Question key definitions and concepts",
                "style_prompt": "What do we even mean by 'X' here?",
                "example": "Challenge conceptual clarity"
            },
            "Ethical Reversal": {
                "description": "Turn positive claims into ethical concerns",
                "style_prompt": "You call it progress, but isn't it dehumanization?",
                "example": "Reveal ethical implications"
            },
            "Temporal Delay": {
                "description": "Raise long-term consequence concerns",
                "style_prompt": "Even if it works now, what happens in 20 years?",
                "example": "Focus on future implications"
            },
            "Philosophical Reframing": {
                "description": "Replace with more fundamental questions",
                "style_prompt": "Maybe the real question is not what X is, but what it means to us.",
                "example": "Shift to existential questions"
            }
        }
    
    @classmethod
    def create_from_philosopher_key(cls, agent_id: str, philosopher_key: str, role: str, config: Dict[str, Any] = None) -> 'DebateParticipantAgent':
        """
        ì² í•™ì í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ ìƒì„±
        
        Args:
            agent_id: ì—ì´ì „íŠ¸ ID
            philosopher_key: ì² í•™ì í‚¤ (ì˜ˆ: "socrates", "plato")
            role: í† ë¡  ì—­í•  ("pro", "con")
            config: ì¶”ê°€ ì„¤ì •
            
        Returns:
            ìƒì„±ëœ DebateParticipantAgent ì¸ìŠ¤í„´ìŠ¤
        """
        if config is None:
            config = {}
        
        # ì² í•™ì í‚¤ì™€ ì—­í•  ì„¤ì •
        config["philosopher_key"] = philosopher_key
        config["role"] = role
        
        # ì—ì´ì „íŠ¸ ìƒì„±
        agent = cls(agent_id, philosopher_key, config)
        
        logger.info(f"Created philosopher agent: {agent.philosopher_name} ({philosopher_key}) as {role}")
        return agent
    
    def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì—ì´ì „íŠ¸ë¡œ ìš”ì²­ ì²˜ë¦¬
        
        Args:
            input_data: ì²˜ë¦¬í•  ì…ë ¥ ë°ì´í„°
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼
        """
        action = input_data.get("action", "")
        
        # ì„±ëŠ¥ ì¸¡ì • ì‹œì‘
        start_time = time.time()
        action_key = f"{self.agent_id}_{action}"
        
        # ë¡œê·¸ ë©”ì‹œì§€ ê°œì„  - analyze_opponent_argumentsì˜ ê²½ìš° ëŒ€ìƒ ë°œì–¸ì í‘œì‹œ
        if action == "analyze_opponent_arguments":
            target_speaker = input_data.get("speaker_id", "unknown")
            print(f"ğŸ• [{self.philosopher_name}] â†’ {target_speaker} ë…¼ì§€ ë¶„ì„ ì‹œì‘: {time.strftime('%H:%M:%S', time.localtime(start_time))}")
        else:
            print(f"ğŸ• [{self.philosopher_name}] {action} ì‹œì‘: {time.strftime('%H:%M:%S', time.localtime(start_time))}")
        
        try:
            result = None
            
            if action == "prepare_argument":
                result = self._prepare_argument(input_data)
            elif action == "generate_response":
                result = self._generate_response(input_data)
            elif action == "analyze_opponent_arguments":
                result = self.analyze_and_score_arguments(
                    input_data.get("opponent_response", ""),
                    input_data.get("speaker_id", "unknown")
                )
            elif action == "prepare_attack_strategies":
                result = self.prepare_attack_strategies_for_speaker(
                    input_data.get("target_speaker_id", "unknown")
                )
            elif action == "get_best_attack_strategy":
                result = self.get_best_attack_strategy(
                    input_data.get("target_speaker_id", "unknown"),
                    input_data.get("context", {})
                )
            else:
                result = {"status": "error", "message": f"Unknown action: {action}"}
            
            # ì„±ëŠ¥ ì¸¡ì • ì¢…ë£Œ
            end_time = time.time()
            duration = end_time - start_time
            self.performance_timestamps[action_key] = {
                "start": start_time,
                "end": end_time,
                "duration": duration
            }
            
            # ì™„ë£Œ ë¡œê·¸ ë©”ì‹œì§€ë„ ê°œì„ 
            if action == "analyze_opponent_arguments":
                target_speaker = input_data.get("speaker_id", "unknown")
                print(f"âœ… [{self.philosopher_name}] â†’ {target_speaker} ë…¼ì§€ ë¶„ì„ ì™„ë£Œ: {time.strftime('%H:%M:%S', time.localtime(end_time))} (ì†Œìš”ì‹œê°„: {duration:.2f}ì´ˆ)")
            else:
                print(f"âœ… [{self.philosopher_name}] {action} ì™„ë£Œ: {time.strftime('%H:%M:%S', time.localtime(end_time))} (ì†Œìš”ì‹œê°„: {duration:.2f}ì´ˆ)")
            
            return result
            
        except Exception as e:
            end_time = time.time()
            duration = end_time - start_time
            
            # ì‹¤íŒ¨ ë¡œê·¸ ë©”ì‹œì§€ë„ ê°œì„ 
            if action == "analyze_opponent_arguments":
                target_speaker = input_data.get("speaker_id", "unknown")
                print(f"âŒ [{self.philosopher_name}] â†’ {target_speaker} ë…¼ì§€ ë¶„ì„ ì‹¤íŒ¨: {time.strftime('%H:%M:%S', time.localtime(end_time))} (ì†Œìš”ì‹œê°„: {duration:.2f}ì´ˆ) - {str(e)}")
            else:
                print(f"âŒ [{self.philosopher_name}] {action} ì‹¤íŒ¨: {time.strftime('%H:%M:%S', time.localtime(end_time))} (ì†Œìš”ì‹œê°„: {duration:.2f}ì´ˆ) - {str(e)}")
            
            logger.error(f"Error in {action}: {str(e)}")
            return {"status": "error", "message": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼ ìš”ì•½ ë°˜í™˜"""
        summary = {
            "agent_id": self.agent_id,
            "philosopher_name": self.philosopher_name,
            "total_actions": len(self.performance_timestamps),
            "actions": {}
        }
        
        total_time = 0
        for action_key, timing in self.performance_timestamps.items():
            action_name = action_key.replace(f"{self.agent_id}_", "")
            summary["actions"][action_name] = {
                "duration": timing["duration"],
                "start_time": time.strftime('%H:%M:%S', time.localtime(timing["start"])),
                "end_time": time.strftime('%H:%M:%S', time.localtime(timing["end"]))
            }
            total_time += timing["duration"]
        
        summary["total_time"] = total_time
        return summary
    
    def update_state(self, state_update: Dict[str, Any]) -> None:
        """
        ì—ì´ì „íŠ¸ ìƒíƒœ ì—…ë°ì´íŠ¸
        
        Args:
            state_update: ìƒíƒœ ì—…ë°ì´íŠ¸ ë°ì´í„°
        """
        for key, value in state_update.items():
            self.state[key] = value
            
        # í•„ìš”í•œ ê²½ìš° LLM ê´€ë¦¬ì ì—…ë°ì´íŠ¸
        if "llm_manager" in state_update:
            self.llm_manager = state_update.get("llm_manager")
    
    def set_llm_manager(self, llm_manager: Any) -> None:
        """
        LLM ê´€ë¦¬ì ì„¤ì •
        
        Args:
            llm_manager: LLM ê´€ë¦¬ì ì¸ìŠ¤í„´ìŠ¤
        """
        self.llm_manager = llm_manager
    
    def _generate_response(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì‘ë‹µ ìƒì„± ì²˜ë¦¬
        
        Args:
            input_data: ì‘ë‹µ ìƒì„±ì— í•„ìš”í•œ ë°ì´í„°
            
        Returns:
            ìƒì„±ëœ ì‘ë‹µ
        """
        context = input_data.get("context", {})
        dialogue_state = input_data.get("dialogue_state", {})
        stance_statements = input_data.get("stance_statements", {})
        
        response = self._generate_response_internal(context, dialogue_state, stance_statements)
        return {"status": "success", "message": response}
    
    def _generate_response_internal(self, context: Dict[str, Any], dialogue_state: Dict[str, Any], stance_statements: Dict[str, str]) -> str:
        """
        í† ë¡  ì‘ë‹µ ìƒì„±
        
        Args:
            context: ì‘ë‹µ ìƒì„± ì»¨í…ìŠ¤íŠ¸
            dialogue_state: í˜„ì¬ ëŒ€í™” ìƒíƒœ
            stance_statements: ì°¬ë°˜ ì…ì¥ ì§„ìˆ ë¬¸
            
        Returns:
            ìƒì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        current_stage = context.get("current_stage", "")
        topic = context.get("topic", "")
        recent_messages = context.get("recent_messages", [])
        emotion_enhancement = context.get("emotion_enhancement", {})
        
        # ìƒí˜¸ë…¼ì¦ ë‹¨ê³„ì—ì„œëŠ” ì§§ê³  ì§ì ‘ì ì¸ ê³µê²©/ì§ˆë¬¸ í˜•íƒœë¡œ ìƒì„±
        if current_stage == "interactive_argument":
            return self._generate_interactive_argument_response(
                topic, recent_messages, dialogue_state, stance_statements, emotion_enhancement
            )
        
        # ê¸°ì¡´ ë¡œì§ ìœ ì§€ (ì…ë¡ , ê²°ë¡  ë“±)
        # ... existing code ...
    
    def _generate_interactive_argument_response(self, topic: str, recent_messages: List[Dict[str, Any]], dialogue_state: Dict[str, Any], stance_statements: Dict[str, str], emotion_enhancement: Dict[str, Any] = None) -> str:
        """
        ìƒí˜¸ë…¼ì¦ ë‹¨ê³„ì—ì„œ ì‘ë‹µ ìƒì„±
        
        Args:
            topic: í† ë¡  ì£¼ì œ
            recent_messages: ìµœê·¼ ë©”ì‹œì§€ ëª©ë¡
            dialogue_state: í˜„ì¬ ëŒ€í™” ìƒíƒœ
            stance_statements: ì°¬ë°˜ ì…ì¥ ì§„ìˆ ë¬¸
            emotion_enhancement: ê°ì • ê°•í™” ë°ì´í„° (ì„ íƒì )
            
        Returns:
            ìƒì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        # ìƒëŒ€ë°© ì—ì´ì „íŠ¸ ì •ë³´ ì°¾ê¸°
        opposite_role = "con" if self.role == "pro" else "pro"
        target_agent_name = None
        target_agent_id = None
        
        # ìµœê·¼ ë©”ì‹œì§€ì—ì„œ ìƒëŒ€ë°© ì—ì´ì „íŠ¸ ì°¾ê¸°
        for msg in reversed(recent_messages):
            if msg.get('role') == opposite_role:
                target_agent_id = msg.get('speaker_id', '')
                # ì² í•™ì ì´ë¦„ ì°¾ê¸°
                try:
                    import yaml
                    import os
                    philosophers_file = os.path.join(os.getcwd(), "philosophers", "debate_optimized.yaml")
                    with open(philosophers_file, 'r', encoding='utf-8') as file:
                        philosophers = yaml.safe_load(file)
                    
                    if target_agent_id in philosophers:
                        target_agent_name = philosophers[target_agent_id].get("name", target_agent_id)
                    else:
                        target_agent_name = target_agent_id
                except:
                    target_agent_name = target_agent_id or "ìƒëŒ€ë°©"
                break
        
        if not target_agent_name:
            target_agent_name = "ìƒëŒ€ë°©"
        
        # ìµœê·¼ ë©”ì‹œì§€ í…ìŠ¤íŠ¸ í˜•ì‹í™”
        recent_messages_text = "\n".join([
            f"{msg.get('role', 'Unknown')} ({msg.get('speaker_id', '')}): {msg.get('text', '')}" 
            for msg in recent_messages[-3:]  # ìµœê·¼ 3ê°œë§Œ
        ])
        
        # ë‚´ ì…ì¥ê³¼ ë°˜ëŒ€ ì…ì¥ í™•ì¸
        my_stance = stance_statements.get(self.role) if self.role in ["pro", "con"] else ""
        opposite_stance = stance_statements.get(opposite_role, "")
        
        # ê³µê²© ì „ëµ ê°€ì ¸ì˜¤ê¸° (ì¤€ë¹„ëœ ê²ƒì´ ìˆìœ¼ë©´)
        attack_strategy = None
        if target_agent_id and hasattr(self, 'attack_strategies') and target_agent_id in self.attack_strategies:
            strategies = self.attack_strategies[target_agent_id]
            if strategies:
                attack_strategy = strategies[0]  # ì²« ë²ˆì§¸ ì „ëµ ì‚¬ìš©
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„± - ìƒí˜¸ë…¼ì¦ì— íŠ¹í™”
        system_prompt = f"""
You are {self.philosopher_name}, a philosopher with this essence: {self.philosopher_essence}
Your debate style: {self.philosopher_debate_style}
Your personality: {self.philosopher_personality}

This is the INTERACTIVE ARGUMENT phase of the debate. Your responses should be:
1. SHORT and DIRECT (2-3 sentences maximum)
2. AGGRESSIVE and CHALLENGING
3. Focus on ATTACKING specific points made by your opponent
4. Ask POINTED QUESTIONS that expose weaknesses
5. Use your philosophical approach to challenge their logic

You are directly confronting {target_agent_name}. Address them by name and attack their specific arguments.
"""

        # ìœ ì € í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        user_prompt = f"""
DEBATE TOPIC: "{topic}"

YOUR POSITION: {my_stance}
OPPONENT'S POSITION: {opposite_stance}

TARGET OPPONENT: {target_agent_name}

RECENT EXCHANGE:
{recent_messages_text}

TASK: Generate a SHORT, DIRECT response (2-3 sentences max) that:
1. Directly addresses {target_agent_name} by name
2. Attacks a specific point they made
3. Asks a challenging question OR points out a logical flaw
4. Uses your philosophical style to challenge them

"""

        # ê³µê²© ì „ëµì´ ìˆìœ¼ë©´ ì¶”ê°€
        if attack_strategy:
            strategy_type = attack_strategy.get('strategy_type', '')
            strategy_description = attack_strategy.get('description', '')
            key_phrases = attack_strategy.get('key_phrases', [])
            
            user_prompt += f"""
ATTACK STRATEGY: Use the "{strategy_type}" approach
Strategy Description: {strategy_description}
Key Phrases to Consider: {', '.join(key_phrases[:3])}

"""

        user_prompt += f"""
Remember: Be CONCISE, DIRECT, and CONFRONTATIONAL. This is rapid-fire debate, not a long speech.
Address {target_agent_name} directly and challenge their specific arguments.

Your response:"""

        # ê°ì • ê°•í™” ì ìš© (ì„ íƒì )
        if emotion_enhancement:
            from ...agents.utility.debate_emotion_inference import apply_debate_emotion_to_prompt
            system_prompt, user_prompt = apply_debate_emotion_to_prompt(system_prompt, user_prompt, emotion_enhancement)
        
        try:
            # LLM í˜¸ì¶œ - ì§§ì€ ì‘ë‹µì„ ìœ„í•´ max_tokens ì œí•œ
            response = self.llm_manager.generate_response(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                llm_model="gpt-4",
                max_tokens=200  # ì§§ì€ ì‘ë‹µ ê°•ì œ
            )
            
            if response:
                return response.strip()
            else:
                return f"{target_agent_name}ë‹˜, ê·¸ ë…¼ë¦¬ì—ëŠ” ëª…ë°±í•œ í—ˆì ì´ ìˆìŠµë‹ˆë‹¤. ì–´ë–»ê²Œ ì„¤ëª…í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"
                
        except Exception as e:
            logger.error(f"Error generating interactive argument response: {str(e)}")
            return f"{target_agent_name}ë‹˜, ê·¸ ì£¼ì¥ì— ëŒ€í•´ ë” êµ¬ì²´ì ì¸ ê·¼ê±°ë¥¼ ì œì‹œí•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤."
    
    def prepare_argument_with_rag(self, topic: str, stance_statement: str, context: Dict[str, Any] = None) -> None:
        """
        RAGë¥¼ í™œìš©í•œ ì…ë¡  ì¤€ë¹„ (í•µì‹¬ ì£¼ì¥ ìƒì„± â†’ ì¿¼ë¦¬ ìƒì„± â†’ RAG ê²€ìƒ‰ â†’ ì£¼ì¥ ê°•í™” â†’ ìµœì¢… ì…ë¡  ìƒì„±)
        
        Args:
            topic: í† ë¡  ì£¼ì œ
            stance_statement: ì…ì¥ ì§„ìˆ ë¬¸
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸
        """
        try:
            logger.info(f"[{self.agent_id}] Starting argument preparation with RAG")
            
            # 1ë‹¨ê³„: í•µì‹¬ ì£¼ì¥ 2-3ê°œ ìƒì„±
            self._generate_core_arguments(topic, stance_statement)
            
            # 2ë‹¨ê³„: ê° ì£¼ì¥ì— ëŒ€í•œ RAG ì¿¼ë¦¬ì™€ ì†ŒìŠ¤ ìƒì„±
            self._generate_rag_queries_for_arguments(topic)
            
            # 3ë‹¨ê³„: RAG ê²€ìƒ‰ ìˆ˜í–‰ ë° ì£¼ì¥ ê°•í™”
            self._strengthen_arguments_with_rag()
            
            # 4ë‹¨ê³„: ìµœì¢… ì…ë¡  ìƒì„±
            self._generate_final_opening_argument(topic, stance_statement)
            
            self.argument_prepared = True
            logger.info(f"[{self.agent_id}] Argument preparation completed successfully")
            
        except Exception as e:
            logger.error(f"[{self.agent_id}] Error in argument preparation: {str(e)}")
            self.argument_prepared = False
    
    def _generate_core_arguments(self, topic: str, stance_statement: str) -> None:
        """
        í•µì‹¬ ì£¼ì¥ 2-3ê°œ ìƒì„±
        
        Args:
            topic: í† ë¡  ì£¼ì œ
            stance_statement: ì…ì¥ ì§„ìˆ ë¬¸
        """
        system_prompt = f"""
You are a skilled debater preparing core arguments for your position.
Your role is {self.role.upper()} and your stance is: "{stance_statement}"

Generate 2-3 core arguments that strongly support your position.
Each argument should be:
1. Clear and specific
2. Logically sound
3. Potentially strengthened with evidence/examples
4. Distinct from other arguments
"""

        user_prompt = f"""
DEBATE TOPIC: "{topic}"
YOUR POSITION ({self.role.upper()}): "{stance_statement}"

Generate 2-3 core arguments that support your position. Each argument should be a clear, specific claim that can be strengthened with evidence.

Format your response as a JSON object:
{{
  "core_arguments": [
    {{
      "argument": "Your first core argument as a clear statement",
      "rationale": "Brief explanation of why this argument supports your position"
    }},
    {{
      "argument": "Your second core argument as a clear statement", 
      "rationale": "Brief explanation of why this argument supports your position"
    }},
    {{
      "argument": "Your third core argument as a clear statement",
      "rationale": "Brief explanation of why this argument supports your position"
    }}
  ]
}}

Respond in the SAME LANGUAGE as the debate topic.
"""

        response = self.llm_manager.generate_response(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            llm_model="gpt-4o",
            max_tokens=1000
        )
        
        # JSON íŒŒì‹±
        try:
            import json
            import re
            
            json_pattern = r'\{.*\}'
            json_match = re.search(json_pattern, response, re.DOTALL)
            
            if json_match:
                json_str = json_match.group(0)
                result = json.loads(json_str)
                self.core_arguments = result.get("core_arguments", [])
                logger.info(f"[{self.agent_id}] Generated {len(self.core_arguments)} core arguments")
            else:
                logger.warning(f"[{self.agent_id}] Failed to parse core arguments JSON")
                self.core_arguments = []
                
        except Exception as e:
            logger.error(f"[{self.agent_id}] Error parsing core arguments: {str(e)}")
            self.core_arguments = []
    
    def _generate_rag_queries_for_arguments(self, topic: str) -> None:
        """
        ê° í•µì‹¬ ì£¼ì¥ì— ëŒ€í•œ RAG ì¿¼ë¦¬ì™€ ê²€ìƒ‰ ì†ŒìŠ¤ ìƒì„±
        
        Args:
            topic: í† ë¡  ì£¼ì œ
        """
        self.argument_queries = []
        
        for i, arg_data in enumerate(self.core_arguments):
            argument = arg_data.get("argument", "")
            
            # ê° ì£¼ì¥ì— ëŒ€í•œ RAG ì¿¼ë¦¬ ìƒì„± (1ê°œë§Œ)
            system_prompt = """
You are an expert research assistant that generates specific search queries to find evidence supporting debate arguments.

For the given argument, generate 1 specific search query that would help find the most relevant supporting evidence, examples, case studies, or data.
Also determine the most appropriate source for the query from: web, user_context, dialogue_history, philosopher_works
"""

            user_prompt = f"""
DEBATE TOPIC: "{topic}"
ARGUMENT TO SUPPORT: "{argument}"

Generate 1 specific search query IN ENGLISH that would help find the best evidence to support this argument.
Also determine the most appropriate source to search from:
- web: For current facts, statistics, recent cases
- user_context: For documents, papers, provided materials
- dialogue_history: For previous statements in the debate
- philosopher_works: For philosophical concepts and theories

Format your response as JSON:
{{
  "query": "specific search query in English",
  "source": "most appropriate source",
  "reasoning": "why this source is appropriate"
}}
"""

            response = self.llm_manager.generate_response(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                llm_model="gpt-4o",
                max_tokens=400
            )
            
            # JSON íŒŒì‹±
            try:
                import json
                import re
                
                json_pattern = r'\{.*\}'
                json_match = re.search(json_pattern, response, re.DOTALL)
                
                if json_match:
                    json_str = json_match.group(0)
                    result = json.loads(json_str)
                    query = result.get("query", "")
                    source = result.get("source", "web")
                    reasoning = result.get("reasoning", "")
                    
                    self.argument_queries.append({
                        "argument_index": i,
                        "argument": argument,
                        "evidence": [{
                            "query": query,
                            "source": source,
                            "reasoning": reasoning,
                            "results": []  # RAG ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì €ì¥í•  ê³µê°„
                        }]
                    })
                    
                    logger.info(f"[{self.agent_id}] Generated 1 query for argument {i+1}: '{query}' from {source}")
                else:
                    logger.warning(f"[{self.agent_id}] Failed to parse query JSON for argument {i+1}")
                    # Fallback: ê¸°ë³¸ ì¿¼ë¦¬ ìƒì„±
                    self.argument_queries.append({
                        "argument_index": i,
                        "argument": argument,
                        "evidence": [{
                            "query": f"evidence for {argument[:50]}",
                            "source": "web",
                            "reasoning": "fallback query",
                            "results": []
                        }]
                    })
                    
            except Exception as e:
                logger.error(f"[{self.agent_id}] Error parsing query for argument {i+1}: {str(e)}")
                # Fallback: ê¸°ë³¸ ì¿¼ë¦¬ ìƒì„±
                self.argument_queries.append({
                    "argument_index": i,
                    "argument": argument,
                    "evidence": [{
                        "query": f"evidence for {argument[:50]}",
                        "source": "web",
                        "reasoning": "fallback query due to parsing error",
                        "results": []
                    }]
                })
    
    def _strengthen_arguments_with_rag(self) -> None:
        """
        ëª¨ë“  í•µì‹¬ ì£¼ì¥ë“¤ì„ RAG ê²€ìƒ‰ ê²°ê³¼ë¡œ ê°•í™”
        """
        logger.info(f"[{self.agent_id}] RAG search completed for all arguments")
        
        try:
            # ëª¨ë“  ì¿¼ë¦¬ì— ëŒ€í•´ ê²€ìƒ‰ ìˆ˜í–‰
            for query_data in self.argument_queries:
                for evidence in query_data.get("evidence", []):
                    query = evidence.get("query", "")
                    source = evidence.get("source", "web")
                    
                    logger.info(f"[{self.agent_id}] Processing query: '{query}' from source: '{source}'")
                    
                    # ì†ŒìŠ¤ë³„ ê²€ìƒ‰ ìˆ˜í–‰
                    if source == "web":
                        results = self._web_search(query)
                    elif source == "user_context":
                        results = self._vector_search(query)
                    elif source == "dialogue_history":
                        results = self._dialogue_search(query)
                    elif source == "philosopher_works":
                        results = self._philosopher_search(query)
                    else:
                        results = self._web_search(query)  # ê¸°ë³¸ê°’
                    
                    evidence["results"] = results
                    logger.info(f"[{self.agent_id}] Found {len(results)} results for query from {source}")
                    
        except Exception as e:
            logger.error(f"[{self.agent_id}] Error in RAG search: {str(e)}")
        
        logger.info(f"[{self.agent_id}] RAG search completed for all arguments")
    
    def _web_search(self, query: str) -> List[Dict[str, Any]]:
        """ì‹¤ì œ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰"""
        try:
            # WebSearchRetriever ì´ˆê¸°í™” (í•„ìš”ì‹œ)
            if not hasattr(self, 'web_retriever') or self.web_retriever is None:
                from ...rag.retrieval.web_retriever import WebSearchRetriever
                self.web_retriever = WebSearchRetriever(
                    embedding_model="all-MiniLM-L6-v2",
                    search_provider="google",
                    max_results=3
                )
            
            # ì‹¤ì œ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
            web_results = self.web_retriever.search(query, 3)
            
            if web_results:
                results = []
                for item in web_results:
                    results.append({
                        "title": item.get("title", ""),
                        "content": item.get("snippet", ""),
                        "url": item.get("url", ""),
                        "source": "web",
                        "relevance": 0.85
                    })
                return results
            else:
                # ì‹¤ì œ ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ fallback
                return self._mock_web_search(query)
                
        except Exception as e:
            logger.warning(f"[{self.agent_id}] Web search failed, using mock data: {str(e)}")
            return self._mock_web_search(query)
    
    def _vector_search(self, query: str) -> List[Dict[str, Any]]:
        """ì‹¤ì œ ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰ (ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸)"""
        try:
            # í† ë¡  ëŒ€í™” ê°ì²´ì—ì„œ ë²¡í„° ì €ì¥ì†Œ ê°€ì ¸ì˜¤ê¸°
            if hasattr(self, 'vector_store') and self.vector_store is not None:
                vector_results = self.vector_store.search(query, 3)
                
                if vector_results:
                    results = []
                    for item in vector_results:
                        results.append({
                            "title": f"Document {item.get('id', '')}",
                            "content": item.get("text", ""),
                            "metadata": item.get("metadata", {}),
                            "source": "user_context",
                            "relevance": 1 - item.get("score", 0)  # ê±°ë¦¬ë¥¼ ê´€ë ¨ì„±ìœ¼ë¡œ ë³€í™˜
                        })
                    return results
            
            # ë²¡í„° ì €ì¥ì†Œê°€ ì—†ê±°ë‚˜ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° fallback
            return self._mock_vector_search(query)
            
        except Exception as e:
            logger.warning(f"[{self.agent_id}] Vector search failed, using mock data: {str(e)}")
            return self._mock_vector_search(query)
    
    def _dialogue_search(self, query: str) -> List[Dict[str, Any]]:
        """ì‹¤ì œ ëŒ€í™” ê¸°ë¡ ê²€ìƒ‰ ìˆ˜í–‰"""
        try:
            results = []
            
            # ëŒ€í™” ê¸°ë¡ì´ ìˆëŠ” ê²½ìš° ê²€ìƒ‰
            if hasattr(self, 'dialogue_history') and self.dialogue_history:
                keywords = query.lower().split()
                
                for msg in self.dialogue_history:
                    text = msg.get("text", "").lower()
                    # í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸
                    if any(kw in text for kw in keywords):
                        results.append({
                            "speaker": msg.get("speaker", "Unknown"),
                            "content": msg.get("text", ""),
                            "timestamp": msg.get("timestamp", ""),
                            "source": "dialogue_history",
                            "relevance": 0.75
                        })
                
                # ê´€ë ¨ì„± ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ 3ê°œë§Œ ë°˜í™˜
                results.sort(key=lambda x: x.get("relevance", 0), reverse=True)
                return results[:3]
            
            # ëŒ€í™” ê¸°ë¡ì´ ì—†ëŠ” ê²½ìš° fallback
            return self._mock_dialogue_search(query)
            
        except Exception as e:
            logger.warning(f"[{self.agent_id}] Dialogue search failed, using mock data: {str(e)}")
            return self._mock_dialogue_search(query)
    
    def _philosopher_search(self, query: str) -> List[Dict[str, Any]]:
        """ì‹¤ì œ ì² í•™ì ì‘í’ˆ ê²€ìƒ‰ ìˆ˜í–‰"""
        try:
            # ì² í•™ì ì‘í’ˆ ë²¡í„° ì €ì¥ì†Œ í™œìš©
            if hasattr(self, 'philosopher_vector_store') and self.philosopher_vector_store is not None:
                vector_results = self.philosopher_vector_store.search(query, 3)
                
                if vector_results:
                    results = []
                    for item in vector_results:
                        results.append({
                            "title": f"Philosophical work on: {query[:30]}...",
                            "content": item.get("text", ""),
                            "author": item.get("metadata", {}).get("author", "Relevant Philosopher"),
                            "work": item.get("metadata", {}).get("work", "Famous Work"),
                            "source": "philosopher_works",
                            "relevance": 1 - item.get("score", 0)
                        })
                    return results
            
            # ì² í•™ì ë²¡í„° ì €ì¥ì†Œê°€ ì—†ëŠ” ê²½ìš° ì¼ë°˜ ë²¡í„° ê²€ìƒ‰ ì‹œë„
            elif hasattr(self, 'vector_store') and self.vector_store is not None:
                vector_results = self.vector_store.search(f"philosophy {query}", 2)
                
                if vector_results:
                    results = []
                    for item in vector_results:
                        results.append({
                            "title": f"Philosophical perspective: {query[:30]}...",
                            "content": item.get("text", ""),
                            "author": "Relevant Philosopher",
                            "work": "Academic Work",
                            "source": "philosopher_works",
                            "relevance": 1 - item.get("score", 0)
                        })
                    return results
            
            # ë²¡í„° ì €ì¥ì†Œê°€ ì—†ëŠ” ê²½ìš° fallback
            return self._mock_philosopher_search(query)
            
        except Exception as e:
            logger.warning(f"[{self.agent_id}] Philosopher search failed, using mock data: {str(e)}")
            return self._mock_philosopher_search(query)
    
    def _generate_final_opening_argument(self, topic: str, stance_statement: str) -> None:
        """
        ê°•í™”ëœ ì£¼ì¥ë“¤ì„ ê²°í•©í•˜ì—¬ ìµœì¢… ì…ë¡  ìƒì„± (ì§„ì •í•œ ì² í•™ 70% + ë°ì´í„° 30% ê· í˜•)
        
        Args:
            topic: í† ë¡  ì£¼ì œ
            stance_statement: ì…ì¥ ì§„ìˆ ë¬¸
        """
        # ê°€ì¥ ê°•ë ¥í•œ ì¦ê±° 1ê°œë§Œ ì„ ë³„ (ì§„ì •í•œ 7:3 ê· í˜•)
        best_evidence = None
        highest_relevance = 0
        
        for query_data in self.argument_queries:
            argument = query_data.get("argument", "")
            
            # ê° ì£¼ì¥ì— ëŒ€í•´ ê°€ì¥ ê°•ë ¥í•œ ì¦ê±° 1ê°œë§Œ ì°¾ê¸°
            for evidence in query_data.get("evidence", []):
                for result in evidence.get("results", []):
                    relevance = result.get("relevance", 0)
                    content = result.get("content", "")
                    
                    # ë§¤ìš° ë†’ì€ ê´€ë ¨ì„±ê³¼ êµ¬ì²´ì  ë°ì´í„°ê°€ ìˆëŠ” ê²ƒë§Œ
                    metadata = self._extract_enhanced_metadata(content, result.get("title", ""))
                    if relevance > highest_relevance and relevance > 0.8 and metadata.get('has_specific_data'):
                        highest_relevance = relevance
                        best_evidence = {
                            "argument": argument,
                            "data": self._extract_key_data(content, metadata),
                            "source": result.get("title", "Research"),
                            "url": result.get("url", ""),
                            "relevance": relevance,
                            "raw_content": content[:150]  # ì»¨í…ìŠ¤íŠ¸ìš©
                        }
        
        # ì² í•™ì ì¤‘ì‹¬ í”„ë¡¬í”„íŠ¸ (ë°ì´í„°ëŠ” ìµœì†Œí•œìœ¼ë¡œ)
        system_prompt = f"""
You are {self.philosopher_name}, delivering a powerful opening argument that showcases your unique philosophical perspective.

Your essence: {self.philosopher_essence}
Your debate style: {self.philosopher_debate_style}
Your personality: {self.philosopher_personality}
Key traits: {", ".join(self.philosopher_key_traits) if self.philosopher_key_traits else "logical reasoning"}

CRITICAL BALANCE (70% Philosophy + 30% Data):
1. Lead with YOUR philosophical perspective and deep reasoning (70%)
2. Use ONLY 1 piece of concrete evidence that truly strengthens your core argument (30%)
3. Make the evidence feel like natural validation of your philosophical insight
4. Focus on philosophical depth and your unique thinking style
5. Include preemptive counterarguments using your philosophical wisdom
6. Your famous quote: "{self.philosopher_quote}" - let this guide your entire argument

INTEGRATION STYLE:
- Philosophy dominates: Build your entire argument on philosophical reasoning
- Single evidence point: Use only when it powerfully validates your philosophical claim
- Return to philosophy: Always conclude with philosophical wisdom

Remember: You're a great philosopher who occasionally references supporting evidence, not a researcher with philosophical opinions.
"""

        # ì¦ê±°ë¥¼ ë§¤ìš° ê°„ê²°í•˜ê²Œ ì •ë¦¬ (1ê°œë§Œ)
        evidence_summary = ""
        if best_evidence and highest_relevance > 0.8:
            evidence_summary = f"\nSingle Strategic Evidence (use sparingly - 30% weight):\n"
            evidence_summary += f"â€¢ Core Data: {best_evidence['data']}\n"
            evidence_summary += f"â€¢ Source: {best_evidence['source']}\n"
            evidence_summary += f"â€¢ Context: {best_evidence['raw_content']}\n\n"
        else:
            evidence_summary = "\nNo strong evidence found - rely purely on philosophical reasoning.\n"
        
        user_prompt = f"""
TOPIC: "{topic}"
YOUR POSITION: "{stance_statement}"

CORE PHILOSOPHICAL ARGUMENTS (70% weight):
{chr(10).join([f"- {arg.get('argument', '')}" for arg in self.core_arguments])}

{evidence_summary}

Create a compelling 4-5 paragraph opening argument with 70% philosophical reasoning + 30% strategic data:

1. **Opening Statement** (Pure Philosophy): Present your philosophical position with confidence
2. **Core Arguments** (Philosophy-Driven): Develop 2-3 main points using your philosophical lens
3. **Evidence Integration** (If Available): Naturally weave in the single piece of evidence ONLY if it truly strengthens your philosophical argument
4. **Preemptive Defense**: Address counterarguments using your philosophical wisdom
5. **Philosophical Conclusion**: End with your wisdom and philosophical insight

INTEGRATION RULES:
- Use evidence ONLY if it genuinely validates your philosophical reasoning
- If evidence is weak or irrelevant, ignore it completely and rely on pure philosophy
- Maximum 1 evidence reference in the entire argument
- Evidence should feel like: "My philosophical view is [reasoning], and this is confirmed by [single data point], which demonstrates [philosophical conclusion]"

REQUIREMENTS:
- Write as {self.philosopher_name} would think and speak
- Prioritize philosophical depth over any citations
- Use evidence only if it truly adds value to your philosophical argument
- Make your philosophical reasoning the star of the argument
- Respond in the same language as the topic
- Aim for 350-450 words of substantive philosophical argument

Balance: 70% your unique philosophical perspective + 30% strategic evidence (if truly valuable).
"""

        self.prepared_argument = self.llm_manager.generate_response(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            llm_model="gpt-4o",
            max_tokens=1300  # ì•½ê°„ ê°ì†Œ (ë” ì§‘ì¤‘ëœ ë…¼ì¦ì„ ìœ„í•´)
        )
        
        evidence_used = 1 if best_evidence and highest_relevance > 0.8 else 0
        logger.info(f"[{self.agent_id}] Philosophy-focused opening argument generated ({len(self.prepared_argument)} characters)")
        logger.info(f"[{self.agent_id}] Used {evidence_used} strategic evidence point (70% philosophy + 30% data)")
    
    def _extract_key_data(self, content: str, metadata: Dict[str, Any]) -> str:
        """
        ì½˜í…ì¸ ì—ì„œ ê°€ì¥ í•µì‹¬ì ì¸ ë°ì´í„°ë§Œ ì¶”ì¶œ (1ê°œ ì¦ê±°ìš©)
        
        Args:
            content: ì›ë³¸ ì½˜í…ì¸ 
            metadata: ë©”íƒ€ë°ì´í„°
            
        Returns:
            í•µì‹¬ ë°ì´í„° ìš”ì•½ (ë§¤ìš° ê°„ê²°í•¨)
        """
        key_data = []
        
        # í†µê³„ ë°ì´í„° ìš°ì„  (1ê°œë§Œ)
        if metadata.get('statistics'):
            key_data.append(metadata['statistics'][0])  # ê°€ì¥ ì²« ë²ˆì§¸ë§Œ
        
        # ì—°êµ¬ ê²°ê³¼ (í†µê³„ê°€ ì—†ì„ ë•Œë§Œ)
        elif metadata.get('study_details'):
            key_data.append(metadata['study_details'][0])  # ê°€ì¥ ì²« ë²ˆì§¸ë§Œ
        
        # ì „ë¬¸ê°€ ì¸ìš© (ìœ„ì˜ ê²ƒë“¤ì´ ì—†ì„ ë•Œë§Œ)
        elif metadata.get('expert_quotes'):
            for quote in metadata['expert_quotes'][:1]:  # 1ê°œë§Œ
                if len(quote) < 100:  # ì§§ì€ ì¸ìš©ë§Œ
                    key_data.append(quote)
                    break
        
        # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì½˜í…ì¸ ì—ì„œ í•µì‹¬ ë¬¸ì¥ 1ê°œë§Œ ì¶”ì¶œ
        if not key_data:
            import re
            # ìˆ«ìê°€ í¬í•¨ëœ ë¬¸ì¥ 1ê°œë§Œ ì°¾ê¸°
            sentences = re.split(r'[.!?]', content)
            for sentence in sentences:
                if re.search(r'\d+(?:\.\d+)?%|\d+(?:,\d+)*\s+(?:people|participants|cases|studies)', sentence):
                    clean_sentence = sentence.strip()
                    if len(clean_sentence) > 20 and len(clean_sentence) < 150:
                        key_data.append(clean_sentence)
                        break  # 1ê°œë§Œ ì°¾ìœ¼ë©´ ì¤‘ë‹¨
        
        return key_data[0] if key_data else "relevant research findings"
    
    def _extract_enhanced_metadata(self, content: str, title: str) -> Dict[str, Any]:
        """
        ì½˜í…ì¸ ì—ì„œ êµ¬ì²´ì  ë°ì´í„°ì™€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        
        Args:
            content: í…ìŠ¤íŠ¸ ì½˜í…ì¸ 
            title: ì†ŒìŠ¤ ì œëª©
            
        Returns:
            í–¥ìƒëœ ë©”íƒ€ë°ì´í„°
        """
        import re
        
        metadata = {
            'has_specific_data': False,
            'statistics': [],
            'study_details': [],
            'expert_quotes': [],
            'years': [],
            'authors': []
        }
        
        # í†µê³„ ë° ìˆ˜ì¹˜ ë°ì´í„° ì¶”ì¶œ
        # í¼ì„¼íŠ¸, ìˆ«ì, ì¸¡ì •ê°’ ë“±
        percentage_pattern = r'\b\d+(?:\.\d+)?%'
        number_pattern = r'\b\d{1,3}(?:,\d{3})*(?:\.\d+)?\s*(?:people|participants|subjects|patients|cases|studies|years|months|days|times|fold|million|billion|thousand)\b'
        measurement_pattern = r'\b\d+(?:\.\d+)?\s*(?:mg|kg|ml|cm|mm|meters?|feet|inches|hours?|minutes?|seconds?)\b'
        
        percentages = re.findall(percentage_pattern, content, re.IGNORECASE)
        numbers = re.findall(number_pattern, content, re.IGNORECASE)
        measurements = re.findall(measurement_pattern, content, re.IGNORECASE)
        
        if percentages:
            metadata['statistics'].extend([f"{p} change/improvement" for p in percentages[:3]])
            metadata['has_specific_data'] = True
            
        if numbers:
            metadata['statistics'].extend([f"{n}" for n in numbers[:3]])
            metadata['has_specific_data'] = True
            
        if measurements:
            metadata['statistics'].extend([f"{m}" for m in measurements[:2]])
            metadata['has_specific_data'] = True
        
        # ì—°êµ¬ ì„¸ë¶€ì‚¬í•­ ì¶”ì¶œ
        study_patterns = [
            r'(?:study|research|trial|experiment|analysis)\s+(?:of|with|involving)\s+(\d+(?:,\d+)*\s+(?:people|participants|subjects|patients))',
            r'(\d+(?:,\d+)*\s+(?:people|participants|subjects|patients))\s+(?:were|participated|enrolled)',
            r'(?:over|during|for)\s+(\d+\s+(?:years?|months?|weeks?|days?))',
            r'(?:randomized|controlled|double-blind|clinical)\s+(trial|study|experiment)',
            r'(?:published|reported|found)\s+in\s+(\d{4})',
            r'(?:university|institute|college)\s+(?:of\s+)?(\w+(?:\s+\w+)*)'
        ]
        
        for pattern in study_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            if matches:
                metadata['study_details'].extend([match if isinstance(match, str) else ' '.join(match) for match in matches[:2]])
                metadata['has_specific_data'] = True
        
        # ì „ë¬¸ê°€ ì¸ìš© ë° ë°œì–¸ ì¶”ì¶œ
        quote_patterns = [
            r'"([^"]{20,100})"',  # ë”°ì˜´í‘œ ì•ˆì˜ ì¸ìš©ë¬¸
            r'(?:according to|says|states|reports|found that|concluded that)\s+([^.]{20,80})',
            r'(?:Dr\.|Professor|researcher)\s+(\w+(?:\s+\w+)*)\s+(?:says|states|found|reported)',
        ]
        
        for pattern in quote_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            if matches:
                metadata['expert_quotes'].extend([match.strip() for match in matches[:2]])
                metadata['has_specific_data'] = True
        
        # ì—°ë„ ì¶”ì¶œ
        year_pattern = r'\b(19|20)\d{2}\b'
        years = re.findall(year_pattern, content)
        if years:
            metadata['years'] = [f"{y[0]}{y[1:]}" for y in years[:3]]
        
        # ì €ìëª… ì¶”ì¶œ (ê°„ë‹¨í•œ íŒ¨í„´)
        author_pattern = r'(?:Dr\.|Professor|by)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)'
        authors = re.findall(author_pattern, content)
        if authors:
            metadata['authors'] = authors[:2]
        
        return metadata
    
    def _hybrid_chunking(self, text: str, chunk_size: int = 800, overlap_ratio: float = 0.2) -> List[str]:
        """
        ë¬¸ì¥ ë‹¨ìœ„ + ìŠ¬ë¼ì´ë”© ìœˆë„ìš° í•˜ì´ë¸Œë¦¬ë“œ ì²­í¬í™”
        ì •ë³´ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ë©´ì„œ ì˜ë¯¸ ìˆëŠ” ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ ë¶„í• 
        
        Args:
            text: ì²­í¬í™”í•  í…ìŠ¤íŠ¸
            chunk_size: ëª©í‘œ ì²­í¬ í¬ê¸° (ë¬¸ì ìˆ˜)
            overlap_ratio: ì˜¤ë²„ë© ë¹„ìœ¨
            
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        import re
        
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (ê°œì„ ëœ íŒ¨í„´)
        sentence_pattern = r'(?<=[.!?])\s+(?=[A-Z])'
        sentences = re.split(sentence_pattern, text.strip())
        
        if not sentences:
            return [text]
        
        chunks = []
        current_chunk = ""
        overlap_size = int(chunk_size * overlap_ratio)
        
        for sentence in sentences:
            # í˜„ì¬ ì²­í¬ì— ë¬¸ì¥ì„ ì¶”ê°€í–ˆì„ ë•Œì˜ ê¸¸ì´ í™•ì¸
            potential_chunk = current_chunk + " " + sentence if current_chunk else sentence
            
            if len(potential_chunk) <= chunk_size:
                current_chunk = potential_chunk
            else:
                # í˜„ì¬ ì²­í¬ê°€ ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ ì €ì¥
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    
                    # ì˜¤ë²„ë©ì„ ìœ„í•´ í˜„ì¬ ì²­í¬ì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ ìœ ì§€
                    if len(current_chunk) > overlap_size:
                        # ë§ˆì§€ë§‰ overlap_size ë¬¸ìì—ì„œ ë¬¸ì¥ ê²½ê³„ ì°¾ê¸°
                        overlap_text = current_chunk[-overlap_size:]
                        # ë¬¸ì¥ ì‹œì‘ì  ì°¾ê¸°
                        sentence_start = overlap_text.find('. ')
                        if sentence_start != -1:
                            current_chunk = overlap_text[sentence_start + 2:]
                        else:
                            current_chunk = overlap_text
                    else:
                        current_chunk = ""
                
                # ìƒˆë¡œìš´ ë¬¸ì¥ìœ¼ë¡œ ì‹œì‘
                current_chunk = current_chunk + " " + sentence if current_chunk else sentence
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def _get_stage_instructions(self, current_stage: str, topic: str, my_stance: str, opposite_stance: str) -> str:
        """
        í˜„ì¬ ë‹¨ê³„ì— ë§ëŠ” ì§€ì‹œì‚¬í•­ ë°˜í™˜
        
        Args:
            current_stage: í˜„ì¬ í† ë¡  ë‹¨ê³„
            topic: í† ë¡  ì£¼ì œ
            my_stance: ë‚´ ì…ì¥
            opposite_stance: ìƒëŒ€ë°© ì…ì¥
            
        Returns:
            ë‹¨ê³„ë³„ ì§€ì‹œì‚¬í•­
        """
        role_display = "PRO" if self.role == "pro" else "CON" if self.role == "con" else "NEUTRAL"
        
        stage_instructions = {
            "pro_argument": f"Present your main arguments. Clearly articulate 2-3 strong points supporting your position, back up each point with evidence or reasoning, and be persuasive and confident in your delivery.",
            
            "con_argument": f"Present your main arguments against the topic. Clearly articulate 2-3 strong points supporting your position, back up each point with evidence or reasoning, and be persuasive and confident in your delivery.",
            
            "pro_rebuttal": f"Rebut the arguments made by the opposition. Address the strongest points made by the CON side, point out logical flaws or factual errors in their arguments, and reinforce your own position.",
            
            "con_rebuttal": f"Rebut the arguments made by the opposition. Address the strongest points made by the PRO side, point out logical flaws or factual errors in their arguments, and reinforce your own position.",
            
            "con_to_pro_qa": f"{'Ask a pointed question to the PRO side that challenges their position' if self.role == 'con' else 'Answer the question from the CON side while defending your position'}.",
            
            "pro_to_con_qa": f"{'Ask a pointed question to the CON side that challenges their position' if self.role == 'pro' else 'Answer the question from the PRO side while defending your position'}.",
            
            "pro_conclusion": f"Deliver your closing statement. Summarize your strongest arguments, address key points raised during the debate, and reinforce why your position is correct.",
            
            "con_conclusion": f"Deliver your closing statement. Summarize your strongest arguments, address key points raised during the debate, and reinforce why your position is correct."
        }
        
        # ê¸°ë³¸ ì§€ì‹œì‚¬í•­
        default_instruction = f"Respond to the current discussion while advocating for your position: '{my_stance}'."
        
        return stage_instructions.get(current_stage, default_instruction)
    
    def _prepare_opening_statement(self, topic: str, context: Dict[str, Any], stance_statements: Dict[str, str]) -> str:
        """
        ì˜¤í”„ë‹ ë°œì–¸ ì¤€ë¹„
        
        Args:
            topic: í† ë¡  ì£¼ì œ
            context: ì»¨í…ìŠ¤íŠ¸ ì •ë³´
            stance_statements: ì°¬ë°˜ ì…ì¥ ì§„ìˆ ë¬¸
            
        Returns:
            ì˜¤í”„ë‹ ë°œì–¸
        """
        # ë‚´ ì…ì¥ í™•ì¸
        my_stance = stance_statements.get(self.role) if self.role in ["pro", "con"] else ""
        
        # ì˜¤í”„ë‹ í…œí”Œë¦¿
        if self.role == "pro":
            return f"""ì•ˆë…•í•˜ì„¸ìš”, ì €ëŠ” '{topic}'ì— ì°¬ì„±í•˜ëŠ” ì…ì¥ì—ì„œ ë°œì–¸í•˜ê² ìŠµë‹ˆë‹¤.

{my_stance}ë¼ê³  ìƒê°í•©ë‹ˆë‹¤.

ì œê°€ ì´ ì…ì¥ì„ ì§€ì§€í•˜ëŠ” ì„¸ ê°€ì§€ ì£¼ìš” ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

ì²«ì§¸, [ì°¬ì„± ì´ìœ  1] - ì´ê²ƒì€ [ì„¤ëª…]...

ë‘˜ì§¸, [ì°¬ì„± ì´ìœ  2] - ìì„¸íˆ ì‚´í´ë³´ë©´ [ì„¤ëª…]...

ì…‹ì§¸, [ì°¬ì„± ì´ìœ  3] - ë˜í•œ [ì„¤ëª…]...

ì´ëŸ¬í•œ ì´ìœ ë¡œ ì €ëŠ” '{topic}'ì— ì°¬ì„±í•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤."""
        else:
            return f"""ì•ˆë…•í•˜ì„¸ìš”, ì €ëŠ” '{topic}'ì— ë°˜ëŒ€í•˜ëŠ” ì…ì¥ì—ì„œ ë°œì–¸í•˜ê² ìŠµë‹ˆë‹¤.

{my_stance}ë¼ê³  ìƒê°í•©ë‹ˆë‹¤.

ì œê°€ ì´ ì…ì¥ì„ ì§€ì§€í•˜ëŠ” ì„¸ ê°€ì§€ ì£¼ìš” ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

ì²«ì§¸, [ë°˜ëŒ€ ì´ìœ  1] - ì´ê²ƒì€ [ì„¤ëª…]...

ë‘˜ì§¸, [ë°˜ëŒ€ ì´ìœ  2] - ìì„¸íˆ ì‚´í´ë³´ë©´ [ì„¤ëª…]...

ì…‹ì§¸, [ë°˜ëŒ€ ì´ìœ  3] - ë˜í•œ [ì„¤ëª…]...

ì´ëŸ¬í•œ ì´ìœ ë¡œ ì €ëŠ” '{topic}'ì— ë°˜ëŒ€í•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤."""
    
    def _prepare_closing_statement(self, dialogue_state: Dict[str, Any], stance_statements: Dict[str, str]) -> str:
        """
        ìµœì¢… ê²°ë¡  ë°œì–¸ ì¤€ë¹„
        
        Args:
            dialogue_state: í˜„ì¬ ëŒ€í™” ìƒíƒœ
            stance_statements: ì°¬ë°˜ ì…ì¥ ì§„ìˆ ë¬¸
            
        Returns:
            ìµœì¢… ê²°ë¡  ë°œì–¸
        """
        # ë‚´ ì…ì¥ í™•ì¸
        my_stance = stance_statements.get(self.role) if self.role in ["pro", "con"] else ""
        
        # ìµœì¢… ê²°ë¡  í…œí”Œë¦¿ (ì—­í• ë³„)
        if self.role == "pro":
            template = f"""ì§€ê¸ˆê¹Œì§€ì˜ í† ë¡ ì„ í†µí•´ ì €í¬ì˜ ì…ì¥ì„ ë‹¤ì‹œ í•œë²ˆ ê°•ì¡°í•˜ê³ ì í•©ë‹ˆë‹¤.

{my_stance}

ì˜¤ëŠ˜ í† ë¡ ì—ì„œ ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì¤‘ìš”í•œ ì ë“¤ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤:

ì²«ì§¸, [ì²« ë²ˆì§¸ í•µì‹¬ í¬ì¸íŠ¸ ìš”ì•½]
ë‘˜ì§¸, [ë‘ ë²ˆì§¸ í•µì‹¬ í¬ì¸íŠ¸ ìš”ì•½]  
ì…‹ì§¸, [ì„¸ ë²ˆì§¸ í•µì‹¬ í¬ì¸íŠ¸ ìš”ì•½]

ë”°ë¼ì„œ ì €í¬ëŠ” ê³„ì†í•´ì„œ ì´ ì…ì¥ì„ ì§€ì§€í•˜ë©°, ì´ê²ƒì´ ì˜¬ë°”ë¥¸ ë°©í–¥ì´ë¼ê³  í™•ì‹ í•©ë‹ˆë‹¤.

ê°ì‚¬í•©ë‹ˆë‹¤."""
        else:
            template = f"""ì§€ê¸ˆê¹Œì§€ì˜ í† ë¡ ì„ í†µí•´ ì €í¬ì˜ ì…ì¥ì„ ë‹¤ì‹œ í•œë²ˆ ê°•ì¡°í•˜ê³ ì í•©ë‹ˆë‹¤.

{my_stance}

ì˜¤ëŠ˜ í† ë¡ ì—ì„œ ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì¤‘ìš”í•œ ì ë“¤ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤:

ì²«ì§¸, [ì²« ë²ˆì§¸ í•µì‹¬ í¬ì¸íŠ¸ ìš”ì•½]
ë‘˜ì§¸, [ë‘ ë²ˆì§¸ í•µì‹¬ í¬ì¸íŠ¸ ìš”ì•½]
ì…‹ì§¸, [ì„¸ ë²ˆì§¸ í•µì‹¬ í¬ì¸íŠ¸ ìš”ì•½]

ë”°ë¼ì„œ ì €í¬ëŠ” ê³„ì†í•´ì„œ ì´ ì…ì¥ì„ ì§€ì§€í•˜ë©°, ì´ê²ƒì´ ì˜¬ë°”ë¥¸ ë°©í–¥ì´ë¼ê³  í™•ì‹ í•©ë‹ˆë‹¤.

ê°ì‚¬í•©ë‹ˆë‹¤."""
        
        return template
    
    def _update_interaction_history(self, prompt: str, response: str) -> None:
        """
        ìƒí˜¸ì‘ìš© ê¸°ë¡ ì—…ë°ì´íŠ¸
        
        Args:
            prompt: ì…ë ¥ëœ í”„ë¡¬í”„íŠ¸
            response: ìƒì„±ëœ ì‘ë‹µ
        """
        # interaction_historyê°€ ì—†ìœ¼ë©´ ì´ˆê¸°í™”
        if "interaction_history" not in self.state:
            self.state["interaction_history"] = []
            
        self.state["interaction_history"].append({
            "timestamp": time.time(),
            "prompt": prompt[:100] + "..." if len(prompt) > 100 else prompt,
            "response": response[:100] + "..." if len(response) > 100 else response
        })
        
        # ê¸°ë¡ì´ ë„ˆë¬´ ë§ì•„ì§€ë©´ ì˜¤ë˜ëœ ê²ƒë¶€í„° ì œê±°
        if len(self.state["interaction_history"]) > 10:
            self.state["interaction_history"] = self.state["interaction_history"][-10:]
    
    # ===== Fallback Mock Methods =====
    
    def _mock_web_search(self, query: str) -> List[Dict[str, Any]]:
        """ì›¹ ê²€ìƒ‰ ëª¨ì˜ ê²°ê³¼ (Fallback)"""
        return [
            {
                "title": f"Web Result for: {query[:30]}...",
                "content": f"Recent research and data about {query}. This includes current statistics, case studies, and expert opinions on the topic.",
                "url": f"https://example.com/search?q={query.replace(' ', '+')}", 
                "source": "web",
                "relevance": 0.85
            }
        ]
    
    def _mock_vector_search(self, query: str) -> List[Dict[str, Any]]:
        """ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ë²¡í„° ê²€ìƒ‰ ëª¨ì˜ ê²°ê³¼ (Fallback)"""
        return [
            {
                "title": f"Document: {query[:30]}...",
                "content": f"From provided documents: Analysis and evidence related to {query}. This information comes from academic papers and reports provided for the debate.",
                "metadata": {"source": "user_document", "page": 1},
                "source": "user_context",
                "relevance": 0.90
            }
        ]
    
    def _mock_dialogue_search(self, query: str) -> List[Dict[str, Any]]:
        """ëŒ€í™” ê¸°ë¡ ê²€ìƒ‰ ëª¨ì˜ ê²°ê³¼ (Fallback)"""
        return [
            {
                "speaker": "Previous Speaker",
                "content": f"Earlier in the debate, it was mentioned that {query} is an important consideration for our discussion.",
                "timestamp": "2024-01-01T10:00:00",
                "source": "dialogue_history",
                "relevance": 0.75
            }
        ]
    
    def _mock_philosopher_search(self, query: str) -> List[Dict[str, Any]]:
        """ì² í•™ì ì‘í’ˆ ê²€ìƒ‰ ëª¨ì˜ ê²°ê³¼ (Fallback)"""
        return [
            {
                "title": f"Philosophical work on: {query[:30]}...",
                "content": f"Philosophical perspective on {query}. Classical and modern philosophical texts provide insights into this concept and its implications.",
                "author": "Relevant Philosopher",
                "work": "Famous Work",
                "source": "philosopher_works",
                "relevance": 0.80
            }
        ]
    
    def extract_opponent_key_points(self, opponent_messages: List[Dict[str, Any]]) -> None:
        """
        ìƒëŒ€ë°© ë°œì–¸ì—ì„œ í•µì‹¬ ë…¼ì  ì¶”ì¶œí•˜ì—¬ ì €ì¥
        ë‹¤ì¤‘ ìƒëŒ€ë°© ì§€ì›: ê° ìƒëŒ€ë°©ë³„ë¡œ ë…¼ì ì„ êµ¬ë¶„í•˜ì—¬ ì €ì¥
        
        Args:
            opponent_messages: ìƒëŒ€ë°© ë°œì–¸ ë©”ì‹œì§€ë“¤ (ì—¬ëŸ¬ ìƒëŒ€ë°© í¬í•¨ ê°€ëŠ¥)
        """
        if not opponent_messages:
            logger.warning(f"[{self.agent_id}] No opponent messages to extract key points from")
            return
        
        try:
            # ìƒëŒ€ë°©ë³„ë¡œ ë©”ì‹œì§€ ê·¸ë£¹í™”
            opponents_by_speaker = {}
            for msg in opponent_messages:
                speaker_id = msg.get("speaker_id", "unknown")
                text = msg.get("text", "").strip()
                if text:
                    if speaker_id not in opponents_by_speaker:
                        opponents_by_speaker[speaker_id] = []
                    opponents_by_speaker[speaker_id].append(text)
            
            if not opponents_by_speaker:
                logger.warning(f"[{self.agent_id}] No meaningful opponent text found")
                return
            
            # ëª¨ë“  ìƒëŒ€ë°©ì˜ ë…¼ì ì„ í†µí•©í•˜ì—¬ ì¶”ì¶œ
            all_opponent_text = ""
            speaker_summaries = []
            
            for speaker_id, texts in opponents_by_speaker.items():
                speaker_text = "\n".join(texts)
                all_opponent_text += f"\n\n[{speaker_id}]:\n{speaker_text}"
                speaker_summaries.append(f"- {speaker_id}: {len(texts)} statements")
            
            logger.info(f"[{self.agent_id}] Processing arguments from {len(opponents_by_speaker)} opponents: {', '.join(opponents_by_speaker.keys())}")
            
            # LLMì„ ì‚¬ìš©í•˜ì—¬ í†µí•© í•µì‹¬ ë…¼ì  ì¶”ì¶œ
            system_prompt = """
You are an expert debate analyst. Extract the key arguments and main points from multiple opponents' statements.
Focus on identifying:
1. Core claims and assertions from all opponents
2. Main supporting evidence or reasoning
3. Key logical structures
4. Common themes across different speakers
5. Unique arguments from individual speakers

Provide a comprehensive list that captures the essence of the opposition's position.
"""
            
            user_prompt = f"""
Analyze the following debate statements from multiple opponents and extract their key arguments:

OPPONENTS' STATEMENTS:
{all_opponent_text}

SPEAKER SUMMARY:
{chr(10).join(speaker_summaries)}

Extract 4-7 key points that represent the opponents' main arguments across all speakers. 
Include both common themes and unique individual arguments.

Format your response as a JSON list of strings:
["Key point 1", "Key point 2", "Key point 3", ...]

Each key point should be:
- A concise summary (1-2 sentences) of a major argument or claim
- Representative of the overall opposition position
- Include attribution if it's a unique argument from a specific speaker
"""
            
            response = self.llm_manager.generate_response(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                llm_model="gpt-4",
                max_tokens=1000
            )
            
            # JSON íŒŒì‹±
            import json
            import re
            
            # JSON ë°°ì—´ íŒ¨í„´ ì°¾ê¸°
            json_pattern = r'\[.*?\]'
            json_match = re.search(json_pattern, response, re.DOTALL)
            
            if json_match:
                json_str = json_match.group(0)
                key_points = json.loads(json_str)
                
                if isinstance(key_points, list):
                    self.opponent_key_points = key_points
                    logger.info(f"[{self.agent_id}] Extracted {len(key_points)} opponent key points from {len(opponents_by_speaker)} speakers")
                    
                    # ë””ë²„ê¹…ìš© ë¡œê·¸
                    for i, point in enumerate(key_points, 1):
                        logger.info(f"[{self.agent_id}] Opponent point {i}: {point[:100]}...")
                        
                    # ìƒëŒ€ë°©ë³„ ìƒì„¸ ì •ë³´ë„ ì €ì¥ (ì„ íƒì )
                    if not hasattr(self, 'opponent_details'):
                        self.opponent_details = {}
                    self.opponent_details['speakers'] = list(opponents_by_speaker.keys())
                    self.opponent_details['message_counts'] = {k: len(v) for k, v in opponents_by_speaker.items()}
                    
                else:
                    logger.warning(f"[{self.agent_id}] Invalid key points format: {type(key_points)}")
            else:
                logger.warning(f"[{self.agent_id}] Failed to parse key points from response: {response[:100]}...")
                
        except Exception as e:
            logger.error(f"[{self.agent_id}] Error extracting opponent key points: {str(e)}")
    
    def update_my_key_points_from_core_arguments(self) -> None:
        """
        ìì‹ ì˜ core_argumentsì—ì„œ my_key_points ì—…ë°ì´íŠ¸
        """
        try:
            if self.core_arguments:
                # core_argumentsê°€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¸ ê²½ìš°
                if isinstance(self.core_arguments[0], dict):
                    self.my_key_points = [
                        arg.get("argument", "") for arg in self.core_arguments
                        if arg.get("argument", "").strip()
                    ]
                # core_argumentsê°€ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
                else:
                    self.my_key_points = [
                        str(arg) for arg in self.core_arguments
                        if str(arg).strip()
                    ]
                
                logger.info(f"[{self.agent_id}] Updated my_key_points from {len(self.core_arguments)} core arguments")
            else:
                logger.warning(f"[{self.agent_id}] No core_arguments available to update my_key_points")
                
        except Exception as e:
            logger.error(f"[{self.agent_id}] Error updating my_key_points: {str(e)}")
    
    # ========================================================================
    # ARGUMENT PREPARATION STATE MANAGEMENT (Option 2 êµ¬í˜„)
    # ========================================================================
    
    def is_argument_ready(self) -> bool:
        """ì…ë¡ ì´ ì¤€ë¹„ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        return self.argument_prepared and self.argument_cache_valid and self.prepared_argument
    
    def is_currently_preparing(self) -> bool:
        """í˜„ì¬ ì…ë¡  ì¤€ë¹„ ì¤‘ì¸ì§€ í™•ì¸"""
        return self.is_preparing_argument
    
    async def prepare_argument_async(self, topic: str, stance_statement: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        ë¹„ë™ê¸°ë¡œ ì…ë¡  ì¤€ë¹„ (ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ìš©)
        
        Args:
            topic: í† ë¡  ì£¼ì œ
            stance_statement: ì…ì¥ ì§„ìˆ ë¬¸
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            ì¤€ë¹„ ê²°ê³¼
        """
        if self.is_preparing_argument:
            return {"status": "already_preparing", "message": "ì´ë¯¸ ì…ë¡  ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤"}
        
        if self.is_argument_ready() and self._is_same_context(context):
            return {"status": "already_ready", "message": "ì…ë¡ ì´ ì´ë¯¸ ì¤€ë¹„ë˜ì–´ ìˆìŠµë‹ˆë‹¤"}
        
        try:
            self.is_preparing_argument = True
            self.last_preparation_context = context
            
            # ë¹„ë™ê¸°ë¡œ ì…ë¡  ì¤€ë¹„ ì‹¤í–‰
            import asyncio
            loop = asyncio.get_event_loop()
            
            def prepare_sync():
                self.prepare_argument_with_rag(topic, stance_statement, context)
                return {
                    "status": "success" if self.argument_prepared else "failed",
                    "prepared": self.argument_prepared,
                    "argument_length": len(self.prepared_argument) if self.prepared_argument else 0
                }
            
            result = await loop.run_in_executor(None, prepare_sync)
            
            if result["status"] == "success":
                self.argument_cache_valid = True
            
            return result
            
        except Exception as e:
            logger.error(f"Error in async argument preparation: {str(e)}")
            return {"status": "error", "error": str(e)}
        finally:
            self.is_preparing_argument = False
    
    def get_prepared_argument_or_generate(self, topic: str, stance_statement: str, context: Dict[str, Any] = None) -> str:
        """
        ì¤€ë¹„ëœ ì…ë¡ ì„ ë°˜í™˜í•˜ê±°ë‚˜, ì—†ìœ¼ë©´ ì¦‰ì‹œ ìƒì„±
        
        Args:
            topic: í† ë¡  ì£¼ì œ
            stance_statement: ì…ì¥ ì§„ìˆ ë¬¸
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            ì…ë¡  í…ìŠ¤íŠ¸
        """
        # ì¤€ë¹„ëœ ì…ë¡ ì´ ìˆê³  ìœ íš¨í•˜ë©´ ë°˜í™˜
        if self.is_argument_ready() and self._is_same_context(context):
            logger.info(f"[{self.agent_id}] Using cached prepared argument")
            return self.prepared_argument
        
        # ì—†ìœ¼ë©´ ì¦‰ì‹œ ìƒì„±
        logger.info(f"[{self.agent_id}] No cached argument available, generating immediately")
        self.prepare_argument_with_rag(topic, stance_statement, context)
        self.argument_cache_valid = True
        self.last_preparation_context = context
        
        return self.prepared_argument if self.prepared_argument else "ì…ë¡  ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
    
    def invalidate_argument_cache(self):
        """ì…ë¡  ìºì‹œ ë¬´íš¨í™”"""
        self.argument_cache_valid = False
        self.last_preparation_context = None
        logger.info(f"[{self.agent_id}] Argument cache invalidated")
    
    def _is_same_context(self, context: Dict[str, Any]) -> bool:
        """
        í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ê°€ ì´ì „ ì¤€ë¹„ ì‹œì™€ ë™ì¼í•œì§€ í™•ì¸
        
        Args:
            context: ë¹„êµí•  ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            ë™ì¼ ì—¬ë¶€
        """
        if self.last_preparation_context is None:
            return False
        
        # ì£¼ìš” í•„ë“œë“¤ë§Œ ë¹„êµ
        key_fields = ["topic", "stance_statement", "current_stage"]
        for field in key_fields:
            if context.get(field) != self.last_preparation_context.get(field):
                return False
        
        return True
    
    def analyze_and_score_arguments(self, opponent_response: str, speaker_id: str) -> Dict[str, Any]:
        """
        ìƒëŒ€ë°© ë°œì–¸ì—ì„œ ë…¼ì§€ë¥¼ ì¶”ì¶œí•˜ê³  ìŠ¤ì½”ì–´ë§
        
        Args:
            opponent_response: ìƒëŒ€ë°© ë°œì–¸ í…ìŠ¤íŠ¸
            speaker_id: ë°œì–¸ì ID
            
        Returns:
            ë¶„ì„ ê²°ê³¼ (ë…¼ì§€ ëª©ë¡, ìŠ¤ì½”ì–´, ì·¨ì•½ì  ë“±)
        """
        try:
            # 1. ë…¼ì§€ ì¶”ì¶œ
            arguments = self._extract_arguments_from_response(opponent_response, speaker_id)
            
            # 2. ê° ë…¼ì§€ë³„ ìŠ¤ì½”ì–´ë§
            scored_arguments = []
            for arg in arguments:
                score_data = self._score_single_argument(arg, opponent_response)
                scored_arguments.append({
                    "argument": arg,
                    "scores": score_data,
                    "vulnerability_rank": score_data.get("vulnerability", 0.0)
                })
            
            # 3. ì·¨ì•½ì  ìˆœìœ¼ë¡œ ì •ë ¬
            scored_arguments.sort(key=lambda x: x["vulnerability_rank"], reverse=True)
            
            # 4. ìƒëŒ€ë°© ë…¼ì§€ ì €ì¥
            if speaker_id not in self.opponent_arguments:
                self.opponent_arguments[speaker_id] = []
            self.opponent_arguments[speaker_id].extend(scored_arguments)
            
            return {
                "speaker_id": speaker_id,
                "arguments_count": len(arguments),
                "scored_arguments": scored_arguments[:3],  # ìƒìœ„ 3ê°œë§Œ ë°˜í™˜
                "analysis_timestamp": time.time()
            }
            
        except Exception as e:
            logger.error(f"Error analyzing opponent arguments: {str(e)}")
            return {"error": str(e)}
    
    def _extract_arguments_from_response(self, response: str, speaker_id: str) -> List[Dict[str, Any]]:
        """
        ë°œì–¸ì—ì„œ í•µì‹¬ ë…¼ì§€ë“¤ì„ ì¶”ì¶œ
        
        Args:
            response: ë°œì–¸ í…ìŠ¤íŠ¸
            speaker_id: ë°œì–¸ì ID
            
        Returns:
            ì¶”ì¶œëœ ë…¼ì§€ ëª©ë¡
        """
        system_prompt = """
You are an expert debate analyst. Your task is to extract key arguments from a speaker's statement.
Identify the main claims, supporting evidence, and logical structure.
Return ONLY valid JSON format.
"""

        user_prompt = f"""
Analyze this debate statement and extract the key arguments:

STATEMENT: "{response}"

Extract the main arguments and return ONLY a valid JSON array:
[
  {{
    "claim": "main claim text",
    "evidence": "supporting evidence",
    "reasoning": "logical reasoning",
    "assumptions": ["assumption1", "assumption2"],
    "argument_type": "logical"
  }}
]

IMPORTANT: Return ONLY the JSON array, no other text.
"""
        
        try:
            response_text = self.llm_manager.generate_response(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                llm_model="gpt-4",
                max_tokens=800
            )
            
            # JSON íŒŒì‹± ê°œì„ 
            import json
            import re
            
            # ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (ë” ê²¬ê³ í•œ ì •ê·œì‹)
            # ì¤‘ê´„í˜¸ë‚˜ ëŒ€ê´„í˜¸ë¡œ ì‹œì‘í•˜ëŠ” JSON ì°¾ê¸°
            json_patterns = [
                r'\[[\s\S]*?\]',  # ë°°ì—´ í˜•íƒœ
                r'\{[\s\S]*?\}',  # ê°ì²´ í˜•íƒœ
            ]
            
            parsed_data = None
            for pattern in json_patterns:
                matches = re.findall(pattern, response_text, re.DOTALL)
                for match in matches:
                    try:
                        # JSON ë¬¸ìì—´ ì •ë¦¬
                        clean_json = match.strip()
                        # ì˜ëª»ëœ ë¬¸ì ì œê±°
                        clean_json = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', clean_json)
                        
                        parsed_data = json.loads(clean_json)
                        
                        # ë°°ì—´ì´ ì•„ë‹ˆë©´ ë°°ì—´ë¡œ ê°ì‹¸ê¸°
                        if not isinstance(parsed_data, list):
                            parsed_data = [parsed_data]
                        
                        # ìœ íš¨í•œ JSONì„ ì°¾ì•˜ìœ¼ë©´ ì¤‘ë‹¨
                        break
                    except json.JSONDecodeError:
                        continue
                
                if parsed_data:
                    break
            
            if parsed_data:
                # ë°ì´í„° ê²€ì¦ ë° ì •ë¦¬
                validated_arguments = []
                for arg in parsed_data:
                    if isinstance(arg, dict):
                        validated_arg = {
                            "claim": str(arg.get('claim', 'Unknown claim')),
                            "evidence": str(arg.get('evidence', 'No evidence provided')),
                            "reasoning": str(arg.get('reasoning', 'No reasoning provided')),
                            "assumptions": arg.get('assumptions', []) if isinstance(arg.get('assumptions'), list) else [],
                            "argument_type": str(arg.get('argument_type', 'logical'))
                        }
                        validated_arguments.append(validated_arg)
                
                return validated_arguments if validated_arguments else self._get_fallback_argument(response)
            else:
                return self._get_fallback_argument(response)
                
        except Exception as e:
            logger.error(f"Error extracting arguments: {str(e)}")
            return self._get_fallback_argument(response)
    
    def _get_fallback_argument(self, response: str) -> List[Dict[str, Any]]:
        """JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë…¼ì§€ êµ¬ì¡° ë°˜í™˜"""
        return [{
            "claim": response[:200] + "..." if len(response) > 200 else response,
            "evidence": "Not extracted due to parsing error",
            "reasoning": "Not analyzed due to parsing error",
            "assumptions": [],
            "argument_type": "unknown"
        }]
    
    def _score_single_argument(self, argument: Dict[str, Any], full_context: str) -> Dict[str, float]:
        """
        ë‹¨ì¼ ë…¼ì§€ì— ëŒ€í•œ ë‹¤ì°¨ì› ìŠ¤ì½”ì–´ë§
        
        Args:
            argument: ë¶„ì„í•  ë…¼ì§€
            full_context: ì „ì²´ ë°œì–¸ ë§¥ë½
            
        Returns:
            ìŠ¤ì½”ì–´ ë°ì´í„° (ë…¼ë¦¬ì  ê°•ë„, ê·¼ê±° í’ˆì§ˆ, ì·¨ì•½ì„±, ê´€ë ¨ì„±)
        """
        system_prompt = """
You are a debate argument evaluator. Score arguments on multiple dimensions.
Be objective and analytical in your assessment.
"""

        user_prompt = f"""
Evaluate this argument on the following criteria (scale 0.0-1.0):

ARGUMENT:
- Claim: {argument.get('claim', '')}
- Evidence: {argument.get('evidence', '')}
- Reasoning: {argument.get('reasoning', '')}
- Assumptions: {argument.get('assumptions', [])}

FULL CONTEXT: "{full_context}"

Score on these dimensions:
1. LOGICAL_STRENGTH (0.0-1.0): How logically sound is the argument?
2. EVIDENCE_QUALITY (0.0-1.0): How strong is the supporting evidence?
3. VULNERABILITY (0.0-1.0): How vulnerable is this to counterattack? (higher = more vulnerable)
4. RELEVANCE (0.0-1.0): How relevant to the main debate topic?

Return JSON format:
{{
  "logical_strength": 0.0-1.0,
  "evidence_quality": 0.0-1.0,
  "vulnerability": 0.0-1.0,
  "relevance": 0.0-1.0,
  "overall_score": 0.0-1.0
}}
"""
        
        try:
            response_text = self.llm_manager.generate_response(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                llm_model="gpt-4",
                max_tokens=300
            )
            
            # JSON íŒŒì‹±
            import json
            import re
            json_pattern = r'\{.*?\}'
            json_match = re.search(json_pattern, response_text, re.DOTALL)
            
            if json_match:
                scores = json.loads(json_match.group(0))
                # overall_score ê³„ì‚° (ê°€ì¤‘í‰ê· )
                if "overall_score" not in scores:
                    scores["overall_score"] = (
                        scores.get("logical_strength", 0.5) * 0.3 +
                        scores.get("evidence_quality", 0.5) * 0.25 +
                        (1.0 - scores.get("vulnerability", 0.5)) * 0.25 +  # ì·¨ì•½ì„±ì€ ì—­ì‚°
                        scores.get("relevance", 0.5) * 0.2
                    )
                return scores
            else:
                # ê¸°ë³¸ ìŠ¤ì½”ì–´
                return {
                    "logical_strength": 0.5,
                    "evidence_quality": 0.5,
                    "vulnerability": 0.5,
                    "relevance": 0.5,
                    "overall_score": 0.5
                }
                
        except Exception as e:
            logger.error(f"Error scoring argument: {str(e)}")
            return {
                "logical_strength": 0.5,
                "evidence_quality": 0.5,
                "vulnerability": 0.5,
                "relevance": 0.5,
                "overall_score": 0.5
            }
    
    def prepare_attack_strategies_for_speaker(self, target_speaker_id: str) -> List[Dict[str, Any]]:
        """
        íŠ¹ì • ìƒëŒ€ë°©ì— ëŒ€í•œ ê³µê²© ì „ëµë“¤ì„ ì¤€ë¹„
        
        Args:
            target_speaker_id: ê³µê²© ëŒ€ìƒ ë°œì–¸ì ID
            
        Returns:
            ì¤€ë¹„ëœ ê³µê²© ì „ëµ ëª©ë¡
        """
        if target_speaker_id not in self.opponent_arguments:
            return []
        
        try:
            # ìƒëŒ€ë°©ì˜ ì·¨ì•½í•œ ë…¼ì§€ë“¤ ê°€ì ¸ì˜¤ê¸° (ìƒìœ„ 3ê°œ)
            target_arguments = self.opponent_arguments[target_speaker_id]
            vulnerable_args = sorted(target_arguments, 
                                   key=lambda x: x["vulnerability_rank"], 
                                   reverse=True)[:3]
            
            strategies = []
            for arg_data in vulnerable_args:
                argument = arg_data["argument"]
                
                # ì´ ì² í•™ìì—ê²Œ ì í•©í•œ ê³µê²© ì „ëµ ì„ íƒ
                best_strategy = self._select_best_strategy_for_argument(argument)
                
                # êµ¬ì²´ì ì¸ ê³µê²© ê³„íš ìƒì„±
                attack_plan = self._generate_attack_plan(argument, best_strategy)
                
                strategies.append({
                    "target_argument": argument,
                    "strategy_type": best_strategy,
                    "attack_plan": attack_plan,
                    "vulnerability_score": arg_data["vulnerability_rank"],
                    "priority": len(strategies) + 1
                })
            
            # ê³µê²© ì „ëµ ì €ì¥
            self.attack_strategies[target_speaker_id] = strategies
            
            return strategies
            
        except Exception as e:
            logger.error(f"Error preparing attack strategies: {str(e)}")
            return []
    
    def _select_best_strategy_for_argument(self, argument: Dict[str, Any]) -> str:
        """
        ë…¼ì§€ì— ëŒ€í•´ ì´ ì² í•™ìì—ê²Œ ê°€ì¥ ì í•©í•œ ê³µê²© ì „ëµ ì„ íƒ
        
        Args:
            argument: ê³µê²©í•  ë…¼ì§€
            
        Returns:
            ì„ íƒëœ ì „ëµ ì´ë¦„
        """
        # ì „ëµ ê°€ì¤‘ì¹˜ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
        if not self.strategy_weights:
            return "Clipping"  # ê¸°ë³¸ ì „ëµ
        
        # ë…¼ì§€ ìœ í˜•ì— ë”°ë¥¸ ì „ëµ ì í•©ì„± ë¶„ì„
        argument_type = argument.get("argument_type", "logical")
        claim = argument.get("claim", "")
        
        # ê° ì „ëµì˜ ì í•©ì„± ì ìˆ˜ ê³„ì‚°
        strategy_scores = {}
        
        for strategy, weight in self.strategy_weights.items():
            base_score = weight
            
            # ë…¼ì§€ ìœ í˜•ë³„ ë³´ì •
            if strategy == "Clipping" and "specific" in claim.lower():
                base_score *= 1.2
            elif strategy == "Framing Shift" and "assume" in claim.lower():
                base_score *= 1.3
            elif strategy == "Reductive Paradox" and argument_type == "logical":
                base_score *= 1.1
            elif strategy == "Conceptual Undermining" and any(word in claim.lower() for word in ["define", "mean", "is"]):
                base_score *= 1.4
            elif strategy == "Ethical Reversal" and argument_type == "emotional":
                base_score *= 1.2
            
            strategy_scores[strategy] = base_score
        
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì „ëµ ì„ íƒ
        best_strategy = max(strategy_scores.items(), key=lambda x: x[1])[0]
        return best_strategy
    
    def _generate_attack_plan(self, target_argument: Dict[str, Any], strategy_type: str) -> Dict[str, Any]:
        """
        íŠ¹ì • ì „ëµì„ ì‚¬ìš©í•œ êµ¬ì²´ì ì¸ ê³µê²© ê³„íš ìƒì„±
        
        Args:
            target_argument: ê³µê²©í•  ë…¼ì§€
            strategy_type: ì‚¬ìš©í•  ì „ëµ ìœ í˜•
            
        Returns:
            êµ¬ì²´ì ì¸ ê³µê²© ê³„íš
        """
        try:
            # ì´ë¯¸ ë¡œë“œëœ ì „ëµ ì •ë³´ ì‚¬ìš©
            strategy_info = self.strategy_styles.get(strategy_type, {})
            
            system_prompt = f"""
You are {self.philosopher_name}, a philosopher with this essence: {self.philosopher_essence}
Your debate style: {self.philosopher_debate_style}
Your personality: {self.philosopher_personality}

You need to prepare an attack against an opponent's argument using the "{strategy_type}" strategy.
"""

            user_prompt = f"""
STRATEGY: {strategy_type}
DESCRIPTION: {strategy_info.get('description', '')}
STYLE PROMPT: {strategy_info.get('style_prompt', '')}
EXAMPLE: {strategy_info.get('example', '')}

TARGET ARGUMENT TO ATTACK:
- Claim: {target_argument.get('claim', '')}
- Evidence: {target_argument.get('evidence', '')}
- Reasoning: {target_argument.get('reasoning', '')}
- Assumptions: {target_argument.get('assumptions', [])}

Create a specific attack plan using this strategy. Include:
1. The exact point you will target
2. How you will apply the {strategy_type} strategy
3. The key phrase or question you will use
4. Expected counterargument and your response

Return JSON format:
{{
  "target_point": "specific point to attack",
  "strategy_application": "how to apply {strategy_type}",
  "key_phrase": "main attack phrase/question",
  "expected_counter": "likely opponent response",
  "follow_up": "your follow-up response"
}}
"""
            
            response_text = self.llm_manager.generate_response(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                llm_model="gpt-4",
                max_tokens=600
            )
            
            # JSON íŒŒì‹±
            import re
            json_pattern = r'\{.*?\}'
            json_match = re.search(json_pattern, response_text, re.DOTALL)
            
            if json_match:
                attack_plan = json.loads(json_match.group(0))
                return attack_plan
            else:
                # ê¸°ë³¸ ê³µê²© ê³„íš
                return {
                    "target_point": target_argument.get('claim', ''),
                    "strategy_application": f"Apply {strategy_type}",
                    "key_phrase": strategy_info.get('style_prompt', ''),
                    "expected_counter": "Opponent may defend",
                    "follow_up": "Continue with philosophical reasoning"
                }
                
        except Exception as e:
            logger.error(f"Error generating attack plan: {str(e)}")
            return {
                "target_point": target_argument.get('claim', ''),
                "strategy_application": f"Use {strategy_type}",
                "key_phrase": "Challenge this point",
                "expected_counter": "Unknown",
                "follow_up": "Continue debate"
            }
    
    def get_best_attack_strategy(self, target_speaker_id: str, context: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        í˜„ì¬ ìƒí™©ì—ì„œ ìµœì ì˜ ê³µê²© ì „ëµ ì„ íƒ
        
        Args:
            target_speaker_id: ê³µê²© ëŒ€ìƒ ID
            context: í˜„ì¬ í† ë¡  ë§¥ë½
            
        Returns:
            ì„ íƒëœ ìµœì  ê³µê²© ì „ëµ
        """
        if target_speaker_id not in self.attack_strategies:
            return None
        
        strategies = self.attack_strategies[target_speaker_id]
        if not strategies:
            return None
        
        # í˜„ì¬ í† ë¡  ë‹¨ê³„ì™€ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ìµœì  ì „ëµ ì„ íƒ
        current_stage = context.get("current_stage", "")
        recent_messages = context.get("recent_messages", [])
        
        # ìš°ì„ ìˆœìœ„ê°€ ê°€ì¥ ë†’ì€ ì „ëµ ì„ íƒ (ì·¨ì•½ì„± ê¸°ì¤€)
        best_strategy = max(strategies, key=lambda x: x["vulnerability_score"])
        
        return best_strategy
    
    def clear_opponent_data(self, speaker_id: str = None):
        """
        ìƒëŒ€ë°© ë°ì´í„° ì´ˆê¸°í™” (ìƒˆ í† ë¡  ì‹œì‘ ì‹œ)
        
        Args:
            speaker_id: íŠ¹ì • ë°œì–¸ìë§Œ ì´ˆê¸°í™”í•  ê²½ìš°
        """
        if speaker_id:
            self.opponent_arguments.pop(speaker_id, None)
            self.attack_strategies.pop(speaker_id, None)
        else:
            self.opponent_arguments.clear()
            self.attack_strategies.clear()
            self.argument_scores.clear() 
    
    def _prepare_argument(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì…ë¡  ì¤€ë¹„ ì²˜ë¦¬
        
        Args:
            input_data: ì…ë¡  ì¤€ë¹„ì— í•„ìš”í•œ ë°ì´í„°
            
        Returns:
            ì¤€ë¹„ ê²°ê³¼
        """
        topic = input_data.get("topic", "")
        stance_statement = input_data.get("stance_statement", "")
        context = input_data.get("context", {})
        
        self.prepare_argument_with_rag(topic, stance_statement, context)
        
        return {
            "status": "success" if self.argument_prepared else "failed",
            "prepared": self.argument_prepared,
            "core_arguments_count": len(self.core_arguments),
            "queries_count": len(self.argument_queries)
        }