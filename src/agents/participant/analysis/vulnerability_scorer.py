"""
Vulnerability scoring functionality for debate participant agent.
Handles detailed vulnerability analysis and personalized scoring.
"""

import json
import re
from typing import Dict, List, Any
import logging

logger = logging.getLogger(__name__)


class VulnerabilityScorer:
    """Handles vulnerability analysis and scoring of arguments."""
    
    def __init__(self, llm_manager, agent_id: str, philosopher_name: str, philosopher_data: Dict[str, Any]):
        """
        Initialize the VulnerabilityScorer.
        
        Args:
            llm_manager: LLM manager for generating responses
            agent_id: Agent identifier
            philosopher_name: Name of the philosopher
            philosopher_data: Philosopher-specific data including vulnerability sensitivity
        """
        self.llm_manager = llm_manager
        self.agent_id = agent_id
        self.philosopher_name = philosopher_name
        self.philosopher_data = philosopher_data
    
    def score_single_argument(self, argument: Dict[str, Any], full_context: str) -> Dict[str, float]:
        """
        단일 논지에 대한 다차원 스코어링 (개선된 취약성 분석 포함)
        
        Args:
            argument: 분석할 논지
            full_context: 전체 발언 맥락
            
        Returns:
            스코어 데이터 (논리적 강도, 근거 품질, 세부 취약성, 관련성, 최종 취약성)
        """
        # 1. 세부 취약성 분석 (LLM 사용)
        detailed_vulnerabilities = self.analyze_detailed_vulnerabilities(argument, full_context)
        
        # 2. 철학자별 민감도 적용한 최종 취약성 점수 계산
        final_vulnerability = self.calculate_personalized_vulnerability(detailed_vulnerabilities)
        
        # 3. 기존 스코어링 (논리적 강도, 근거 품질, 관련성)
        basic_scores = self.get_basic_argument_scores(argument, full_context)
        
        # 4. 통합 결과 반환
        result = {
            **basic_scores,
            **detailed_vulnerabilities,
            "final_vulnerability": final_vulnerability,
            "overall_score": (
                basic_scores.get("logical_strength", 0.5) * 0.3 +
                basic_scores.get("evidence_quality", 0.5) * 0.25 +
                (1.0 - final_vulnerability) * 0.25 +  # 개선된 취약성 사용
                basic_scores.get("relevance", 0.5) * 0.2
            )
        }
        
        return result
    
    def analyze_detailed_vulnerabilities(self, argument: Dict[str, Any], full_context: str) -> Dict[str, float]:
        """
        논지의 세부적인 취약성 분석
        
        Args:
            argument: 분석할 논지
            full_context: 전체 발언 맥락
            
        Returns:
            세부 취약성 점수들
        """
        print(f"   🔍 [{self.philosopher_name}] 세부 취약성 분석 시작:")
        print(f"      - 대상 논지: {argument.get('claim', '')[:100]}...")
        
        system_prompt = """
You are an expert argument analyzer. Evaluate the vulnerabilities of debate arguments on specific dimensions.
Be precise and objective in your assessment.
"""

        user_prompt = f"""
Analyze this argument for specific vulnerabilities (scale 0.0-1.0, where higher = more vulnerable):

ARGUMENT:
- Claim: {argument.get('claim', '')}
- Evidence: {argument.get('evidence', '')}
- Reasoning: {argument.get('reasoning', '')}
- Assumptions: {argument.get('assumptions', [])}

FULL CONTEXT: "{full_context}"

Evaluate these specific vulnerability dimensions:
1. CONCEPTUAL_CLARITY (0.0-1.0): How unclear or ambiguous are the key concepts?
2. LOGICAL_LEAP (0.0-1.0): How big are the logical gaps in reasoning?
3. OVERGENERALIZATION (0.0-1.0): How much does it generalize beyond evidence?
4. EMOTIONAL_APPEAL (0.0-1.0): How much does it rely on emotion over logic?
5. LACK_OF_CONCRETE_EVIDENCE (0.0-1.0): How lacking is specific, concrete evidence?

Return JSON format:
{{
  "conceptual_clarity": 0.0-1.0,
  "logical_leap": 0.0-1.0,
  "overgeneralization": 0.0-1.0,
  "emotional_appeal": 0.0-1.0,
  "lack_of_concrete_evidence": 0.0-1.0
}}
"""
        
        try:
            print(f"      🤖 LLM 분석 요청 중...")
            response_text = self.llm_manager.generate_response(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                llm_model="gpt-4o",
                max_tokens=1200
            )
            
            print(f"      📝 LLM 응답: {response_text[:200]}...")
            
            # JSON 파싱
            json_pattern = r'\{.*?\}'
            json_match = re.search(json_pattern, response_text, re.DOTALL)
            
            if json_match:
                vulnerabilities = json.loads(json_match.group(0))
                # 키 이름 정규화
                result = {
                    "conceptual_clarity": vulnerabilities.get("conceptual_clarity", 0.5),
                    "logical_leap": vulnerabilities.get("logical_leap", 0.5),
                    "overgeneralization": vulnerabilities.get("overgeneralization", 0.5),
                    "emotional_appeal": vulnerabilities.get("emotional_appeal", 0.5),
                    "lack_of_concrete_evidence": vulnerabilities.get("lack_of_concrete_evidence", 0.5)
                }
                
                print(f"      ✅ 세부 취약성 분석 완료:")
                for vuln_type, score in result.items():
                    print(f"         • {vuln_type}: {score:.3f}")
                
                return result
            else:
                # 기본값
                print(f"      ❌ JSON 파싱 실패 - 기본값 사용")
                return {
                    "conceptual_clarity": 0.5,
                    "logical_leap": 0.5,
                    "overgeneralization": 0.5,
                    "emotional_appeal": 0.5,
                    "lack_of_concrete_evidence": 0.5
                }
                
        except Exception as e:
            logger.error(f"Error analyzing detailed vulnerabilities: {str(e)}")
            print(f"      ❌ 분석 오류: {str(e)} - 기본값 사용")
            return {
                "conceptual_clarity": 0.5,
                "logical_leap": 0.5,
                "overgeneralization": 0.5,
                "emotional_appeal": 0.5,
                "lack_of_concrete_evidence": 0.5
            }
    
    def calculate_personalized_vulnerability(self, detailed_vulnerabilities: Dict[str, float]) -> float:
        """
        철학자별 민감도를 적용한 개인화된 취약성 점수 계산
        
        Args:
            detailed_vulnerabilities: 세부 취약성 점수들
            
        Returns:
            최종 개인화된 취약성 점수 (0.0-1.0)
        """
        try:
            vulnerability_sensitivity = self.philosopher_data.get("vulnerability_sensitivity", {})
            
            print(f"   🧮 [{self.philosopher_name}] 취약성 점수 계산 시작:")
            
            if not vulnerability_sensitivity:
                # 민감도 데이터가 없으면 평균값 반환
                avg_score = sum(detailed_vulnerabilities.values()) / len(detailed_vulnerabilities)
                print(f"      ❌ 민감도 데이터 없음 - 평균값 사용: {avg_score:.3f}")
                return avg_score
            
            print(f"      ✅ 민감도 데이터 로드 완료")
            print(f"      📊 세부 취약성:")
            for vuln_type, score in detailed_vulnerabilities.items():
                print(f"         • {vuln_type}: {score:.3f}")
            
            print(f"      🎯 철학자 민감도:")
            for vuln_type, sensitivity in vulnerability_sensitivity.items():
                print(f"         • {vuln_type}: {sensitivity:.3f}")
            
            # 벡터 내적 계산: 취약성 * 민감도
            total_score = 0.0
            total_weight = 0.0
            
            print(f"      🔢 계산 과정:")
            for vuln_type, vuln_score in detailed_vulnerabilities.items():
                sensitivity = vulnerability_sensitivity.get(vuln_type, 0.5)
                weighted_score = vuln_score * sensitivity
                total_score += weighted_score
                total_weight += sensitivity
                print(f"         • {vuln_type}: {vuln_score:.3f} × {sensitivity:.3f} = {weighted_score:.3f}")
            
            print(f"      📈 합계:")
            print(f"         • 가중합: {total_score:.3f}")
            print(f"         • 가중치합: {total_weight:.3f}")
            
            # 가중평균 계산
            if total_weight > 0:
                final_score = total_score / total_weight
                print(f"         • 최종점수: {total_score:.3f} ÷ {total_weight:.3f} = {final_score:.3f}")
            else:
                final_score = sum(detailed_vulnerabilities.values()) / len(detailed_vulnerabilities)
                print(f"         • 가중치합이 0 - 평균값 사용: {final_score:.3f}")
            
            # 0.0-1.0 범위로 클리핑
            clipped_score = max(0.0, min(1.0, final_score))
            if clipped_score != final_score:
                print(f"         • 클리핑: {final_score:.3f} → {clipped_score:.3f}")
            
            print(f"      🎯 [{self.philosopher_name}] 최종 개인화된 취약성: {clipped_score:.3f}")
            print()
            
            return clipped_score
            
        except Exception as e:
            logger.error(f"Error calculating personalized vulnerability: {str(e)}")
            # 오류 시 기본값 반환
            if detailed_vulnerabilities and len(detailed_vulnerabilities) > 0:
                try:
                    avg_score = sum(detailed_vulnerabilities.values()) / len(detailed_vulnerabilities)
                    return max(0.0, min(1.0, avg_score))
                except:
                    return 0.5  # 완전히 실패한 경우 중간값 반환
            else:
                return 0.5  # 빈 데이터인 경우 중간값 반환
    
    def get_basic_argument_scores(self, argument: Dict[str, Any], full_context: str) -> Dict[str, float]:
        """
        기본 논지 스코어링 (논리적 강도, 근거 품질, 관련성)
        
        Args:
            argument: 분석할 논지
            full_context: 전체 발언 맥락
            
        Returns:
            기본 스코어들
        """
        system_prompt = """
You are a debate argument evaluator. Score arguments on basic dimensions.
Be objective and analytical in your assessment.
"""

        user_prompt = f"""
Evaluate this argument on the following basic criteria (scale 0.0-1.0):

ARGUMENT:
- Claim: {argument.get('claim', '')}
- Evidence: {argument.get('evidence', '')}
- Reasoning: {argument.get('reasoning', '')}
- Assumptions: {argument.get('assumptions', [])}

FULL CONTEXT: "{full_context}"

Score on these dimensions:
1. LOGICAL_STRENGTH (0.0-1.0): How logically sound is the argument?
2. EVIDENCE_QUALITY (0.0-1.0): How strong is the supporting evidence?
3. RELEVANCE (0.0-1.0): How relevant to the main debate topic?

Return JSON format:
{{
  "logical_strength": 0.0-1.0,
  "evidence_quality": 0.0-1.0,
  "relevance": 0.0-1.0
}}
"""
        
        try:
            response_text = self.llm_manager.generate_response(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                llm_model="gpt-4o",
                max_tokens=300
            )
            
            # JSON 파싱
            json_pattern = r'\{.*?\}'
            json_match = re.search(json_pattern, response_text, re.DOTALL)
            
            if json_match:
                scores = json.loads(json_match.group(0))
                return {
                    "logical_strength": scores.get("logical_strength", 0.5),
                    "evidence_quality": scores.get("evidence_quality", 0.5),
                    "relevance": scores.get("relevance", 0.5)
                }
            else:
                # 기본 스코어
                return {
                    "logical_strength": 0.5,
                    "evidence_quality": 0.5,
                    "relevance": 0.5
                }
                
        except Exception as e:
            logger.error(f"Error getting basic argument scores: {str(e)}")
            return {
                "logical_strength": 0.5,
                "evidence_quality": 0.5,
                "relevance": 0.5
            } 