"""
Vulnerability scoring functionality for debate participant agent.
Handles detailed vulnerability analysis and personalized scoring.
"""

import json
import re
from typing import Dict, List, Any
import logging

logger = logging.getLogger(__name__)


class VulnerabilityScorer:
    """Handles vulnerability analysis and scoring of arguments."""
    
    def __init__(self, llm_manager, agent_id: str, philosopher_name: str, philosopher_data: Dict[str, Any]):
        """
        Initialize the VulnerabilityScorer.
        
        Args:
            llm_manager: LLM manager for generating responses
            agent_id: Agent identifier
            philosopher_name: Name of the philosopher
            philosopher_data: Philosopher-specific data including vulnerability sensitivity
        """
        self.llm_manager = llm_manager
        self.agent_id = agent_id
        self.philosopher_name = philosopher_name
        self.philosopher_data = philosopher_data
    
    def score_single_argument(self, argument: Dict[str, Any], full_context: str) -> Dict[str, float]:
        """
        Îã®Ïùº ÎÖºÏßÄÏóê ÎåÄÌïú Îã§Ï∞®Ïõê Ïä§ÏΩîÏñ¥ÎßÅ (Í∞úÏÑ†Îêú Ï∑®ÏïΩÏÑ± Î∂ÑÏÑù Ìè¨Ìï®)
        
        Args:
            argument: Î∂ÑÏÑùÌï† ÎÖºÏßÄ
            full_context: Ï†ÑÏ≤¥ Î∞úÏñ∏ Îß•ÎùΩ
            
        Returns:
            Ïä§ÏΩîÏñ¥ Îç∞Ïù¥ÌÑ∞ (ÎÖºÎ¶¨Ï†Å Í∞ïÎèÑ, Í∑ºÍ±∞ ÌíàÏßà, ÏÑ∏Î∂Ä Ï∑®ÏïΩÏÑ±, Í¥ÄÎ†®ÏÑ±, ÏµúÏ¢Ö Ï∑®ÏïΩÏÑ±)
        """
        # 1. ÏÑ∏Î∂Ä Ï∑®ÏïΩÏÑ± Î∂ÑÏÑù (LLM ÏÇ¨Ïö©)
        detailed_vulnerabilities = self.analyze_detailed_vulnerabilities(argument, full_context)
        
        # 2. Ï≤†ÌïôÏûêÎ≥Ñ ÎØºÍ∞êÎèÑ Ï†ÅÏö©Ìïú ÏµúÏ¢Ö Ï∑®ÏïΩÏÑ± Ï†êÏàò Í≥ÑÏÇ∞
        final_vulnerability = self.calculate_personalized_vulnerability(detailed_vulnerabilities)
        
        # 3. Í∏∞Ï°¥ Ïä§ÏΩîÏñ¥ÎßÅ (ÎÖºÎ¶¨Ï†Å Í∞ïÎèÑ, Í∑ºÍ±∞ ÌíàÏßà, Í¥ÄÎ†®ÏÑ±)
        basic_scores = self.get_basic_argument_scores(argument, full_context)
        
        # 4. ÌÜµÌï© Í≤∞Í≥º Î∞òÌôò
        result = {
            **basic_scores,
            **detailed_vulnerabilities,
            "final_vulnerability": final_vulnerability,
            "overall_score": (
                basic_scores.get("logical_strength", 0.5) * 0.3 +
                basic_scores.get("evidence_quality", 0.5) * 0.25 +
                (1.0 - final_vulnerability) * 0.25 +  # Í∞úÏÑ†Îêú Ï∑®ÏïΩÏÑ± ÏÇ¨Ïö©
                basic_scores.get("relevance", 0.5) * 0.2
            )
        }
        
        return result
    
    def analyze_detailed_vulnerabilities(self, argument: Dict[str, Any], full_context: str) -> Dict[str, float]:
        """
        ÎÖºÏßÄÏùò ÏÑ∏Î∂ÄÏ†ÅÏù∏ Ï∑®ÏïΩÏÑ± Î∂ÑÏÑù
        
        Args:
            argument: Î∂ÑÏÑùÌï† ÎÖºÏßÄ
            full_context: Ï†ÑÏ≤¥ Î∞úÏñ∏ Îß•ÎùΩ
            
        Returns:
            ÏÑ∏Î∂Ä Ï∑®ÏïΩÏÑ± Ï†êÏàòÎì§
        """
        print(f"   üîç [{self.philosopher_name}] ÏÑ∏Î∂Ä Ï∑®ÏïΩÏÑ± Î∂ÑÏÑù ÏãúÏûë:")
        print(f"      - ÎåÄÏÉÅ ÎÖºÏßÄ: {argument.get('claim', '')[:100]}...")
        
        system_prompt = """
You are an expert argument analyzer. Evaluate the vulnerabilities of debate arguments on specific dimensions.
Be precise and objective in your assessment.
"""

        user_prompt = f"""
Analyze this argument for specific vulnerabilities (scale 0.0-1.0, where higher = more vulnerable):

ARGUMENT:
- Claim: {argument.get('claim', '')}
- Evidence: {argument.get('evidence', '')}
- Reasoning: {argument.get('reasoning', '')}
- Assumptions: {argument.get('assumptions', [])}

FULL CONTEXT: "{full_context}"

Evaluate these specific vulnerability dimensions:
1. CONCEPTUAL_CLARITY (0.0-1.0): How unclear or ambiguous are the key concepts?
2. LOGICAL_LEAP (0.0-1.0): How big are the logical gaps in reasoning?
3. OVERGENERALIZATION (0.0-1.0): How much does it generalize beyond evidence?
4. EMOTIONAL_APPEAL (0.0-1.0): How much does it rely on emotion over logic?
5. LACK_OF_CONCRETE_EVIDENCE (0.0-1.0): How lacking is specific, concrete evidence?

Return JSON format:
{{
  "conceptual_clarity": 0.0-1.0,
  "logical_leap": 0.0-1.0,
  "overgeneralization": 0.0-1.0,
  "emotional_appeal": 0.0-1.0,
  "lack_of_concrete_evidence": 0.0-1.0
}}
"""
        
        try:
            print(f"      ü§ñ LLM Î∂ÑÏÑù ÏöîÏ≤≠ Ï§ë...")
            response_text = self.llm_manager.generate_response(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                llm_model="gpt-4o",
                max_tokens=1200
            )
            
            print(f"      üìù LLM ÏùëÎãµ: {response_text[:200]}...")
            
            # JSON ÌååÏã±
            json_pattern = r'\{.*?\}'
            json_match = re.search(json_pattern, response_text, re.DOTALL)
            
            if json_match:
                vulnerabilities = json.loads(json_match.group(0))
                # ÌÇ§ Ïù¥Î¶Ñ Ï†ïÍ∑úÌôî
                result = {
                    "conceptual_clarity": vulnerabilities.get("conceptual_clarity", 0.5),
                    "logical_leap": vulnerabilities.get("logical_leap", 0.5),
                    "overgeneralization": vulnerabilities.get("overgeneralization", 0.5),
                    "emotional_appeal": vulnerabilities.get("emotional_appeal", 0.5),
                    "lack_of_concrete_evidence": vulnerabilities.get("lack_of_concrete_evidence", 0.5)
                }
                
                print(f"      ‚úÖ ÏÑ∏Î∂Ä Ï∑®ÏïΩÏÑ± Î∂ÑÏÑù ÏôÑÎ£å:")
                for vuln_type, score in result.items():
                    print(f"         ‚Ä¢ {vuln_type}: {score:.3f}")
                
                return result
            else:
                # Í∏∞Î≥∏Í∞í
                print(f"      ‚ùå JSON ÌååÏã± Ïã§Ìå® - Í∏∞Î≥∏Í∞í ÏÇ¨Ïö©")
                return {
                    "conceptual_clarity": 0.5,
                    "logical_leap": 0.5,
                    "overgeneralization": 0.5,
                    "emotional_appeal": 0.5,
                    "lack_of_concrete_evidence": 0.5
                }
                
        except Exception as e:
            logger.error(f"Error analyzing detailed vulnerabilities: {str(e)}")
            print(f"      ‚ùå Î∂ÑÏÑù Ïò§Î•ò: {str(e)} - Í∏∞Î≥∏Í∞í ÏÇ¨Ïö©")
            return {
                "conceptual_clarity": 0.5,
                "logical_leap": 0.5,
                "overgeneralization": 0.5,
                "emotional_appeal": 0.5,
                "lack_of_concrete_evidence": 0.5
            }
    
    def calculate_personalized_vulnerability(self, detailed_vulnerabilities: Dict[str, float]) -> float:
        """
        Ï≤†ÌïôÏûêÎ≥Ñ ÎØºÍ∞êÎèÑÎ•º Ï†ÅÏö©Ìïú Í∞úÏù∏ÌôîÎêú Ï∑®ÏïΩÏÑ± Ï†êÏàò Í≥ÑÏÇ∞
        
        Args:
            detailed_vulnerabilities: ÏÑ∏Î∂Ä Ï∑®ÏïΩÏÑ± Ï†êÏàòÎì§
            
        Returns:
            ÏµúÏ¢Ö Í∞úÏù∏ÌôîÎêú Ï∑®ÏïΩÏÑ± Ï†êÏàò (0.0-1.0)
        """
        try:
            vulnerability_sensitivity = self.philosopher_data.get("vulnerability_sensitivity", {})
            
            print(f"   üßÆ [{self.philosopher_name}] Ï∑®ÏïΩÏÑ± Ï†êÏàò Í≥ÑÏÇ∞ ÏãúÏûë:")
            
            if not vulnerability_sensitivity:
                # ÎØºÍ∞êÎèÑ Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏúºÎ©¥ ÌèâÍ∑†Í∞í Î∞òÌôò
                avg_score = sum(detailed_vulnerabilities.values()) / len(detailed_vulnerabilities)
                print(f"      ‚ùå ÎØºÍ∞êÎèÑ Îç∞Ïù¥ÌÑ∞ ÏóÜÏùå - ÌèâÍ∑†Í∞í ÏÇ¨Ïö©: {avg_score:.3f}")
                return avg_score
            
            print(f"      ‚úÖ ÎØºÍ∞êÎèÑ Îç∞Ïù¥ÌÑ∞ Î°úÎìú ÏôÑÎ£å")
            print(f"      üìä ÏÑ∏Î∂Ä Ï∑®ÏïΩÏÑ±:")
            for vuln_type, score in detailed_vulnerabilities.items():
                print(f"         ‚Ä¢ {vuln_type}: {score:.3f}")
            
            print(f"      üéØ Ï≤†ÌïôÏûê ÎØºÍ∞êÎèÑ:")
            for vuln_type, sensitivity in vulnerability_sensitivity.items():
                print(f"         ‚Ä¢ {vuln_type}: {sensitivity:.3f}")
            
            # Î≤°ÌÑ∞ ÎÇ¥Ï†Å Í≥ÑÏÇ∞: Ï∑®ÏïΩÏÑ± * ÎØºÍ∞êÎèÑ
            total_score = 0.0
            total_weight = 0.0
            
            print(f"      üî¢ Í≥ÑÏÇ∞ Í≥ºÏ†ï:")
            for vuln_type, vuln_score in detailed_vulnerabilities.items():
                sensitivity = vulnerability_sensitivity.get(vuln_type, 0.5)
                weighted_score = vuln_score * sensitivity
                total_score += weighted_score
                total_weight += sensitivity
                print(f"         ‚Ä¢ {vuln_type}: {vuln_score:.3f} √ó {sensitivity:.3f} = {weighted_score:.3f}")
            
            print(f"      üìà Ìï©Í≥Ñ:")
            print(f"         ‚Ä¢ Í∞ÄÏ§ëÌï©: {total_score:.3f}")
            print(f"         ‚Ä¢ Í∞ÄÏ§ëÏπòÌï©: {total_weight:.3f}")
            
            # Í∞ÄÏ§ëÌèâÍ∑† Í≥ÑÏÇ∞
            if total_weight > 0:
                final_score = total_score / total_weight
                print(f"         ‚Ä¢ ÏµúÏ¢ÖÏ†êÏàò: {total_score:.3f} √∑ {total_weight:.3f} = {final_score:.3f}")
            else:
                final_score = sum(detailed_vulnerabilities.values()) / len(detailed_vulnerabilities)
                print(f"         ‚Ä¢ Í∞ÄÏ§ëÏπòÌï©Ïù¥ 0 - ÌèâÍ∑†Í∞í ÏÇ¨Ïö©: {final_score:.3f}")
            
            # 0.0-1.0 Î≤îÏúÑÎ°ú ÌÅ¥Î¶¨Ìïë
            clipped_score = max(0.0, min(1.0, final_score))
            if clipped_score != final_score:
                print(f"         ‚Ä¢ ÌÅ¥Î¶¨Ìïë: {final_score:.3f} ‚Üí {clipped_score:.3f}")
            
            print(f"      üéØ [{self.philosopher_name}] ÏµúÏ¢Ö Í∞úÏù∏ÌôîÎêú Ï∑®ÏïΩÏÑ±: {clipped_score:.3f}")
            print()
            
            return clipped_score
            
        except Exception as e:
            logger.error(f"Error calculating personalized vulnerability: {str(e)}")
            # Ïò§Î•ò Ïãú Í∏∞Î≥∏Í∞í Î∞òÌôò
            if detailed_vulnerabilities and len(detailed_vulnerabilities) > 0:
                try:
                    avg_score = sum(detailed_vulnerabilities.values()) / len(detailed_vulnerabilities)
                    return max(0.0, min(1.0, avg_score))
                except:
                    return 0.5  # ÏôÑÏ†ÑÌûà Ïã§Ìå®Ìïú Í≤ΩÏö∞ Ï§ëÍ∞ÑÍ∞í Î∞òÌôò
            else:
                return 0.5  # Îπà Îç∞Ïù¥ÌÑ∞Ïù∏ Í≤ΩÏö∞ Ï§ëÍ∞ÑÍ∞í Î∞òÌôò
    
    def get_basic_argument_scores(self, argument: Dict[str, Any], full_context: str) -> Dict[str, float]:
        """
        Í∏∞Î≥∏ ÎÖºÏßÄ Ïä§ÏΩîÏñ¥ÎßÅ (ÎÖºÎ¶¨Ï†Å Í∞ïÎèÑ, Í∑ºÍ±∞ ÌíàÏßà, Í¥ÄÎ†®ÏÑ±)
        
        Args:
            argument: Î∂ÑÏÑùÌï† ÎÖºÏßÄ
            full_context: Ï†ÑÏ≤¥ Î∞úÏñ∏ Îß•ÎùΩ
            
        Returns:
            Í∏∞Î≥∏ Ïä§ÏΩîÏñ¥Îì§
        """
        system_prompt = """
You are a debate argument evaluator. Score arguments on basic dimensions.
Be objective and analytical in your assessment.
"""

        user_prompt = f"""
Evaluate this argument on the following basic criteria (scale 0.0-1.0):

ARGUMENT:
- Claim: {argument.get('claim', '')}
- Evidence: {argument.get('evidence', '')}
- Reasoning: {argument.get('reasoning', '')}
- Assumptions: {argument.get('assumptions', [])}

FULL CONTEXT: "{full_context}"

Score on these dimensions:
1. LOGICAL_STRENGTH (0.0-1.0): How logically sound is the argument?
2. EVIDENCE_QUALITY (0.0-1.0): How strong is the supporting evidence?
3. RELEVANCE (0.0-1.0): How relevant to the main debate topic?

Return JSON format:
{{
  "logical_strength": 0.0-1.0,
  "evidence_quality": 0.0-1.0,
  "relevance": 0.0-1.0
}}
"""
        
        try:
            response_text = self.llm_manager.generate_response(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                llm_model="gpt-4o",
                max_tokens=300
            )
            
            # JSON ÌååÏã±
            json_pattern = r'\{.*?\}'
            json_match = re.search(json_pattern, response_text, re.DOTALL)
            
            if json_match:
                scores = json.loads(json_match.group(0))
                return {
                    "logical_strength": scores.get("logical_strength", 0.5),
                    "evidence_quality": scores.get("evidence_quality", 0.5),
                    "relevance": scores.get("relevance", 0.5)
                }
            else:
                # Í∏∞Î≥∏ Ïä§ÏΩîÏñ¥
                return {
                    "logical_strength": 0.5,
                    "evidence_quality": 0.5,
                    "relevance": 0.5
                }
                
        except Exception as e:
            logger.error(f"Error getting basic argument scores: {str(e)}")
            return {
                "logical_strength": 0.5,
                "evidence_quality": 0.5,
                "relevance": 0.5
            } 