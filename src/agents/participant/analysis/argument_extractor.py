"""
Argument extraction functionality for debate participant agent.
Handles extracting arguments from opponent responses and user input.
"""

import json
import re
import time
from typing import Dict, List, Any, Optional
import logging

logger = logging.getLogger(__name__)


class ArgumentExtractor:
    """Handles argument extraction from various sources."""
    
    def __init__(self, llm_manager, agent_id: str, philosopher_name: str):
        """
        Initialize the ArgumentExtractor.
        
        Args:
            llm_manager: LLM manager for generating responses
            agent_id: Agent identifier
            philosopher_name: Name of the philosopher
        """
        self.llm_manager = llm_manager
        self.agent_id = agent_id
        self.philosopher_name = philosopher_name
    
    def extract_arguments_from_response(self, response: str, speaker_id: str) -> List[Dict[str, Any]]:
        """
        ë°œì–¸ì—ì„œ í•µì‹¬ ë…¼ì§€ë“¤ì„ ì¶”ì¶œ
        
        Args:
            response: ë°œì–¸ í…ìŠ¤íŠ¸
            speaker_id: ë°œì–¸ìž ID
            
        Returns:
            ì¶”ì¶œëœ ë…¼ì§€ ëª©ë¡
        """
        system_prompt = """
You are an expert debate analyst. Your task is to extract key arguments from a speaker's statement.
Identify the main claims, supporting evidence, and logical structure.
Return ONLY valid JSON format.
"""

        user_prompt = f"""
Analyze this debate statement and extract the key arguments:

STATEMENT: "{response}"

Extract the main arguments and return ONLY a valid JSON array:
[
  {{
    "claim": "main claim text",
    "evidence": "supporting evidence",
    "reasoning": "logical reasoning",
    "assumptions": ["assumption1", "assumption2"],
    "argument_type": "logical"
  }}
]

IMPORTANT: Return ONLY the JSON array, no other text.
"""
        
        try:
            response_text = self.llm_manager.generate_response(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                llm_model="gpt-4o",
                max_tokens=1200
            )
            
            # JSON íŒŒì‹± ê°œì„ 
            parsed_data = self._parse_json_response(response_text)
            
            if parsed_data:
                # ë°ì´í„° ê²€ì¦ ë° ì •ë¦¬
                validated_arguments = []
                for arg in parsed_data:
                    if isinstance(arg, dict):
                        validated_arg = {
                            "claim": str(arg.get('claim', 'Unknown claim')),
                            "evidence": str(arg.get('evidence', 'No evidence provided')),
                            "reasoning": str(arg.get('reasoning', 'No reasoning provided')),
                            "assumptions": arg.get('assumptions', []) if isinstance(arg.get('assumptions'), list) else [],
                            "argument_type": str(arg.get('argument_type', 'logical'))
                        }
                        validated_arguments.append(validated_arg)
                
                return validated_arguments if validated_arguments else self._get_fallback_argument(response)
            else:
                return self._get_fallback_argument(response)
                
        except Exception as e:
            logger.error(f"Error extracting arguments: {str(e)}")
            return self._get_fallback_argument(response)
    
    def extract_arguments_from_user_input(self, user_response: str, speaker_id: str) -> List[Dict[str, Any]]:
        """
        ìœ ì € ìž…ë ¥ì—ì„œ LLMì„ ì‚¬ìš©í•´ ë…¼ì§€ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            user_response: ìœ ì €ì˜ ìž…ë ¥ í…ìŠ¤íŠ¸
            speaker_id: ìœ ì € ID
            
        Returns:
            List[Dict]: ì¶”ì¶œëœ ë…¼ì§€ë“¤ (ìµœëŒ€ 3ê°œ)
        """
        try:
            logger.info(f"ðŸ” [{self.agent_id}] ìœ ì € {speaker_id}ì˜ ë…¼ì§€ ì¶”ì¶œ ì‹œìž‘")
            
            system_prompt = "You are an expert debate analyst. Extract key arguments from user input in Korean."
            
            user_prompt = f"""
ë‹¹ì‹ ì€ í† ë¡  ë¶„ì„ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ë‹¤ìŒ ì‚¬ìš©ìžì˜ ë°œì–¸ì—ì„œ í•µì‹¬ ë…¼ì§€ë“¤ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ì‚¬ìš©ìž ë°œì–¸:
{user_response}

ìš”êµ¬ì‚¬í•­:
1. í•µì‹¬ ë…¼ì§€ë¥¼ ìµœëŒ€ 3ê°œê¹Œì§€ ì¶”ì¶œ
2. ê° ë…¼ì§€ëŠ” ëª…í™•í•œ ì£¼ìž¥ê³¼ ê·¼ê±°ë¥¼ í¬í•¨í•´ì•¼ í•¨
3. ë„ˆë¬´ ì„¸ë¶€ì ì´ì§€ ì•Šê³  í† ë¡ ì—ì„œ ê³µê²©í•  ìˆ˜ ìžˆëŠ” ìˆ˜ì¤€ì˜ ë…¼ì§€ì—¬ì•¼ í•¨

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”:
{{
  "arguments": [
    {{
      "claim": "ë…¼ì§€ì˜ í•µì‹¬ ì£¼ìž¥",
      "evidence": "ì œì‹œëœ ê·¼ê±°ë‚˜ ì¦ê±°",
      "reasoning": "ë…¼ë¦¬ì  ì¶”ë¡  ê³¼ì •",
      "assumptions": ["ê¸°ë³¸ ê°€ì •ë“¤"]
    }}
  ]
}}

ë…¼ì§€ê°€ 3ê°œ ë¯¸ë§Œì´ë¼ë©´ ì‹¤ì œ ê°œìˆ˜ë§Œ ë°˜í™˜í•˜ì„¸ìš”.
"""

            # llm_manager ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±
            response_text = self.llm_manager.generate_response(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                llm_model="gpt-4o",
                max_tokens=1000,
                temperature=0.3
            )
            
            # JSON íŒŒì‹±
            try:
                # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
                cleaned_response = self._clean_json_response(response_text)
                parsed_data = json.loads(cleaned_response)
                extracted_arguments = parsed_data.get("arguments", [])
                
                logger.info(f"âœ… [{self.agent_id}] ìœ ì € {speaker_id}ì˜ ë…¼ì§€ {len(extracted_arguments)}ê°œ ì¶”ì¶œ ì™„ë£Œ")
                
                # ê¸°ì¡´ í¬ë§·ì— ë§žê²Œ ë³€í™˜
                formatted_arguments = []
                for i, arg in enumerate(extracted_arguments):
                    formatted_arg = {
                        'claim': arg.get('claim', ''),
                        'evidence': arg.get('evidence', ''),
                        'reasoning': arg.get('reasoning', ''),
                        'assumptions': arg.get('assumptions', []),
                        'source_text': user_response,  # ì›ë³¸ í…ìŠ¤íŠ¸ ë³´ì¡´
                        'argument_id': f"user_arg_{i+1}"
                    }
                    formatted_arguments.append(formatted_arg)
                
                return formatted_arguments
                
            except json.JSONDecodeError as e:
                logger.error(f"âŒ [{self.agent_id}] JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                logger.error(f"ì •ë¦¬ëœ ì‘ë‹µ: {cleaned_response if 'cleaned_response' in locals() else response_text}")
                return []
                
        except Exception as e:
            logger.error(f"âŒ [{self.agent_id}] ìœ ì € ë…¼ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def extract_opponent_key_points(self, opponent_messages: List[Dict[str, Any]]) -> List[str]:
        """
        ìƒëŒ€ë°© ë°œì–¸ì—ì„œ í•µì‹¬ ë…¼ì  ì¶”ì¶œí•˜ì—¬ ë°˜í™˜
        ë‹¤ì¤‘ ìƒëŒ€ë°© ì§€ì›: ê° ìƒëŒ€ë°©ë³„ë¡œ ë…¼ì ì„ êµ¬ë¶„í•˜ì—¬ ì €ìž¥
        
        Args:
            opponent_messages: ìƒëŒ€ë°© ë°œì–¸ ë©”ì‹œì§€ë“¤ (ì—¬ëŸ¬ ìƒëŒ€ë°© í¬í•¨ ê°€ëŠ¥)
            
        Returns:
            ì¶”ì¶œëœ í•µì‹¬ ë…¼ì  ëª©ë¡
        """
        if not opponent_messages:
            logger.warning(f"[{self.agent_id}] No opponent messages to extract key points from")
            return []
        
        try:
            # ìƒëŒ€ë°©ë³„ë¡œ ë©”ì‹œì§€ ê·¸ë£¹í™”
            opponents_by_speaker = {}
            for msg in opponent_messages:
                speaker_id = msg.get("speaker_id", "unknown")
                text = msg.get("text", "").strip()
                if text:
                    if speaker_id not in opponents_by_speaker:
                        opponents_by_speaker[speaker_id] = []
                    opponents_by_speaker[speaker_id].append(text)
            
            if not opponents_by_speaker:
                logger.warning(f"[{self.agent_id}] No meaningful opponent text found")
                return []
            
            # ëª¨ë“  ìƒëŒ€ë°©ì˜ ë…¼ì ì„ í†µí•©í•˜ì—¬ ì¶”ì¶œ
            all_opponent_text = ""
            speaker_summaries = []
            
            for speaker_id, texts in opponents_by_speaker.items():
                speaker_text = "\n".join(texts)
                all_opponent_text += f"\n\n[{speaker_id}]:\n{speaker_text}"
                speaker_summaries.append(f"- {speaker_id}: {len(texts)} statements")
            
            logger.info(f"[{self.agent_id}] Processing arguments from {len(opponents_by_speaker)} opponents: {', '.join(opponents_by_speaker.keys())}")
            
            # LLMì„ ì‚¬ìš©í•˜ì—¬ í†µí•© í•µì‹¬ ë…¼ì  ì¶”ì¶œ
            system_prompt = """
You are an expert debate analyst. Extract the key arguments and main points from multiple opponents' statements.
Focus on identifying:
1. Core claims and assertions from all opponents
2. Main supporting evidence or reasoning
3. Key logical structures
4. Common themes across different speakers
5. Unique arguments from individual speakers

Provide a comprehensive list that captures the essence of the opposition's position.
"""
            
            user_prompt = f"""
Analyze the following debate statements from multiple opponents and extract their key arguments:

OPPONENTS' STATEMENTS:
{all_opponent_text}

SPEAKER SUMMARY:
{chr(10).join(speaker_summaries)}

Extract 4-7 key points that represent the opponents' main arguments across all speakers. 
Include both common themes and unique individual arguments.

Format your response as a JSON list of strings:
["Key point 1", "Key point 2", "Key point 3", ...]

Each key point should be:
- A concise summary (1-2 sentences) of a major argument or claim
- Representative of the overall opposition position
- Include attribution if it's a unique argument from a specific speaker
"""
            
            response = self.llm_manager.generate_response(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                llm_model="gpt-4o",
                max_tokens=1500
            )
            
            # JSON íŒŒì‹±
            json_pattern = r'\[.*?\]'
            json_match = re.search(json_pattern, response, re.DOTALL)
            
            if json_match:
                json_str = json_match.group(0)
                key_points = json.loads(json_str)
                
                if isinstance(key_points, list):
                    logger.info(f"[{self.agent_id}] Extracted {len(key_points)} opponent key points from {len(opponents_by_speaker)} speakers")
                    
                    # ë””ë²„ê¹…ìš© ë¡œê·¸
                    for i, point in enumerate(key_points, 1):
                        logger.info(f"[{self.agent_id}] Opponent point {i}: {point[:100]}...")
                    
                    return key_points
                else:
                    logger.warning(f"[{self.agent_id}] Invalid key points format: {type(key_points)}")
                    return []
            else:
                logger.warning(f"[{self.agent_id}] Failed to parse key points from response: {response[:100]}...")
                return []
                
        except Exception as e:
            logger.error(f"[{self.agent_id}] Error extracting opponent key points: {str(e)}")
            return []
    
    def extract_key_concept(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ê°œë…ì„ ì¶”ì¶œ
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            
        Returns:
            ì¶”ì¶œëœ í•µì‹¬ ê°œë…
        """
        try:
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ ë¡œì§
            words = text.split()
            
            # ë¶ˆìš©ì–´ ì œê±° (ê°„ë‹¨í•œ ë²„ì „)
            stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'cannot', 'this', 'that', 'these', 'those'}
            
            # ì˜ë¯¸ìžˆëŠ” ë‹¨ì–´ë“¤ í•„í„°ë§
            meaningful_words = [word.lower().strip('.,!?;:') for word in words 
                              if len(word) > 3 and word.lower() not in stop_words]
            
            # ê°€ìž¥ ë¹ˆë²ˆí•œ ë‹¨ì–´ ë°˜í™˜ (ê°„ë‹¨í•œ ë°©ë²•)
            if meaningful_words:
                word_freq = {}
                for word in meaningful_words:
                    word_freq[word] = word_freq.get(word, 0) + 1
                
                # ê°€ìž¥ ë¹ˆë²ˆí•œ ë‹¨ì–´ ë°˜í™˜
                most_common = max(word_freq.items(), key=lambda x: x[1])
                return most_common[0]
            
            # í´ë°±: ì²« ë²ˆì§¸ ì˜ë¯¸ìžˆëŠ” ë‹¨ì–´
            return meaningful_words[0] if meaningful_words else "concept"
            
        except Exception as e:
            logger.error(f"Error extracting key concept: {str(e)}")
            return "concept"
    
    def _parse_json_response(self, response_text: str) -> Optional[List[Dict[str, Any]]]:
        """JSON ì‘ë‹µì„ íŒŒì‹±í•˜ëŠ” í—¬í¼ ë©”ì„œë“œ"""
        try:
            # JSON íŒŒì‹± ê°œì„ 
            json_patterns = [
                r'\[[\s\S]*?\]',  # ë°°ì—´ í˜•íƒœ
                r'\{[\s\S]*?\}',  # ê°ì²´ í˜•íƒœ
            ]
            
            for pattern in json_patterns:
                matches = re.findall(pattern, response_text, re.DOTALL)
                for match in matches:
                    try:
                        # JSON ë¬¸ìžì—´ ì •ë¦¬
                        clean_json = match.strip()
                        # ìž˜ëª»ëœ ë¬¸ìž ì œê±°
                        clean_json = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', clean_json)
                        
                        parsed_data = json.loads(clean_json)
                        
                        # ë°°ì—´ì´ ì•„ë‹ˆë©´ ë°°ì—´ë¡œ ê°ì‹¸ê¸°
                        if not isinstance(parsed_data, list):
                            parsed_data = [parsed_data]
                        
                        return parsed_data
                    except json.JSONDecodeError:
                        continue
            
            return None
            
        except Exception as e:
            logger.error(f"Error parsing JSON response: {str(e)}")
            return None
    
    def _clean_json_response(self, response_text: str) -> str:
        """JSON ì‘ë‹µì„ ì •ë¦¬í•˜ëŠ” í—¬í¼ ë©”ì„œë“œ"""
        cleaned_response = response_text.strip()
        
        # ```jsonê³¼ ``` ì œê±°
        if '```json' in cleaned_response:
            cleaned_response = cleaned_response.replace('```json', '').replace('```', '').strip()
        elif '```' in cleaned_response:
            cleaned_response = cleaned_response.replace('```', '').strip()
        
        return cleaned_response
    
    def _get_fallback_argument(self, response: str) -> List[Dict[str, Any]]:
        """JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë…¼ì§€ êµ¬ì¡° ë°˜í™˜"""
        return [{
            "claim": response[:200] + "..." if len(response) > 200 else response,
            "evidence": "Not extracted due to parsing error",
            "reasoning": "Not analyzed due to parsing error",
            "assumptions": [],
            "argument_type": "unknown"
        }] 