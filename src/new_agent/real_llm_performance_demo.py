#!/usr/bin/env python3
"""
ì‹¤ì œ LLMì„ ì‚¬ìš©í•œ AI í† ë¡  ì—ì´ì „íŠ¸ ì„±ëŠ¥ ë¹„êµ ë°ëª¨
- ì‹¤ì œ OpenAI API ì‚¬ìš©
- pregenerated_debates.jsonì—ì„œ ì‹¤ì œ í† ë¡  ì£¼ì œ ì‚¬ìš©
- debate_optimized.yamlì—ì„œ ì² í•™ì ì •ë³´ í™œìš©  
- ê¸°ì¡´ ë°©ì‹ vs ìƒˆë¡œìš´ ìµœì í™” ë°©ì‹ì˜ ì§„ì§œ ë¹„êµ
"""

import asyncio
import json
import yaml
import time
import sys
import os
from typing import Dict, Any, List
from pathlib import Path
import openai
from datetime import datetime

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from src.new_agent.unified_debate_agent import UnifiedDebateAgent

class RealLLMManager:
    """ì‹¤ì œ OpenAI APIë¥¼ ì‚¬ìš©í•˜ëŠ” LLM Manager"""
    
    def __init__(self, api_key: str = None):
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ api_keyë¥¼ ì œê³µí•´ì£¼ì„¸ìš”")
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        openai.api_key = self.api_key
        self.client = openai.OpenAI(api_key=self.api_key)
        
    async def generate_response_with_functions(self, system_prompt: str, user_prompt: str, functions: List, function_handler=None):
        """ì‹¤ì œ OpenAI Function Calling ì‚¬ìš©"""
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        try:
            # OpenAI Function Calling í˜¸ì¶œ
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                functions=functions,
                function_call="auto",
                temperature=0.7,
                max_tokens=2000
            )
            
            message = response.choices[0].message
            
            # Function callì´ ìˆëŠ” ê²½ìš° ì²˜ë¦¬
            if message.function_call:
                function_name = message.function_call.name
                function_args = json.loads(message.function_call.arguments)
                
                # Function handler í˜¸ì¶œ
                if function_handler:
                    function_result = await function_handler(function_name, function_args)
                    
                    # Function ê²°ê³¼ë¥¼ í¬í•¨í•œ ë‘ ë²ˆì§¸ í˜¸ì¶œ
                    messages.append({
                        "role": "assistant", 
                        "content": None,
                        "function_call": {
                            "name": function_name,
                            "arguments": message.function_call.arguments
                        }
                    })
                    messages.append({
                        "role": "function",
                        "name": function_name, 
                        "content": json.dumps(function_result)
                    })
                    
                    # ìµœì¢… ì‘ë‹µ ìƒì„±
                    final_response = self.client.chat.completions.create(
                        model="gpt-4o",
                        messages=messages,
                        temperature=0.7,
                        max_tokens=2000
                    )
                    return final_response.choices[0].message.content
            
            return message.content
            
        except Exception as e:
            print(f"âŒ OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
            # Fallback to simple completion
            simple_response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                temperature=0.7,
                max_tokens=2000
            )
            return simple_response.choices[0].message.content

class TraditionalDebateAgent:
    """ê¸°ì¡´ ë°©ì‹ ì‹œë®¬ë ˆì´ì…˜ì„ ìœ„í•œ ì „í†µì  ì—ì´ì „íŠ¸"""
    
    def __init__(self, philosopher_data: Dict[str, Any], llm_manager: RealLLMManager):
        self.philosopher_data = philosopher_data
        self.llm_manager = llm_manager
        self.llm_calls = 0
        self.rag_searches = 0
        
    async def generate_traditional_argument(self, topic: str, stance: str, context_data: Dict[str, Any]) -> Dict[str, Any]:
        """ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì…ë¡  ìƒì„± (ì—¬ëŸ¬ ë‹¨ê³„ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬)"""
        start_time = time.time()
        print(f"\nğŸŒ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì…ë¡  ìƒì„± ì‹œì‘...")
        
        try:
            # 1ë‹¨ê³„: í•µì‹¬ ë…¼ì¦ ìƒì„±
            print("   1ë‹¨ê³„: í•µì‹¬ ë…¼ì¦ ìƒì„±...")
            core_arguments = await self._generate_core_arguments(topic, stance)
            self.llm_calls += 1
            
            # 2ë‹¨ê³„: RAG ì¿¼ë¦¬ ìƒì„±
            print("   2ë‹¨ê³„: RAG ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±...")
            search_queries = await self._generate_search_queries(topic, core_arguments)
            self.llm_calls += 1
            
            # 3ë‹¨ê³„: ì›¹ ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜
            print("   3ë‹¨ê³„: ì›¹ ê²€ìƒ‰...")
            web_results = await self._simulate_web_search(search_queries[0])
            self.rag_searches += 1
            
            # 4ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜  
            print("   4ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰...")
            vector_results = await self._simulate_vector_search(search_queries[1] if len(search_queries) > 1 else search_queries[0])
            self.rag_searches += 1
            
            # 5ë‹¨ê³„: ì² í•™ì ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜
            print("   5ë‹¨ê³„: ì² í•™ì ì§€ì‹ ê²€ìƒ‰...")
            philosopher_insights = await self._simulate_philosopher_search(topic)
            self.rag_searches += 1
            
            # 6ë‹¨ê³„: ë…¼ì¦ ê°•í™”
            print("   6ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼ë¡œ ë…¼ì¦ ê°•í™”...")
            enhanced_arguments = await self._enhance_arguments(core_arguments, web_results, vector_results, philosopher_insights)
            self.llm_calls += 1
            
            # 7ë‹¨ê³„: ìµœì¢… ì…ë¡  ìƒì„±
            print("   7ë‹¨ê³„: ìµœì¢… ì…ë¡  ìƒì„±...")
            final_argument = await self._generate_final_argument(topic, stance, enhanced_arguments)
            self.llm_calls += 1
            
            end_time = time.time()
            
            return {
                "status": "success",
                "argument": final_argument,
                "generation_time": end_time - start_time,
                "llm_calls": self.llm_calls,
                "rag_searches": self.rag_searches,
                "philosopher": self.philosopher_data.get('name', 'Unknown')
            }
            
        except Exception as e:
            return {
                "status": "error",
                "message": str(e),
                "generation_time": time.time() - start_time,
                "llm_calls": self.llm_calls,
                "rag_searches": self.rag_searches
            }
    
    async def _generate_core_arguments(self, topic: str, stance: str) -> str:
        await asyncio.sleep(0.2)  # API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜
        
        prompt = f"""
{self.philosopher_data.get('name', 'Philosopher')}ë¡œì„œ '{topic}'ì— ëŒ€í•œ {stance} ì…ì¥ì˜ í•µì‹¬ ë…¼ì¦ 3ê°œë¥¼ ìƒì„±í•˜ì„¸ìš”.
ê°„ë‹¨í•œ ìš”ì  í˜•íƒœë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""
        
        response = await self._simple_llm_call(prompt)
        return response
    
    async def _generate_search_queries(self, topic: str, arguments: str) -> List[str]:
        await asyncio.sleep(0.1)
        
        prompt = f"""
ì£¼ì œ: {topic}
ë…¼ì¦: {arguments}

ìœ„ ë…¼ì¦ì„ ê°•í™”í•˜ê¸° ìœ„í•œ ê²€ìƒ‰ ì¿¼ë¦¬ 2ê°œë¥¼ ìƒì„±í•˜ì„¸ìš”.
"""
        
        response = await self._simple_llm_call(prompt)
        # ê°„ë‹¨íˆ 2ê°œ ì¿¼ë¦¬ë¡œ ë¶„í• 
        queries = [f"{topic} arguments", f"{topic} philosophical perspective"]
        return queries
    
    async def _simulate_web_search(self, query: str) -> str:
        await asyncio.sleep(0.5)  # ì›¹ ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜
        return f"ì›¹ ê²€ìƒ‰ ê²°ê³¼: {query}ì— ëŒ€í•œ ê´€ë ¨ ì •ë³´ë“¤..."
    
    async def _simulate_vector_search(self, query: str) -> str:
        await asyncio.sleep(0.3)  # ë²¡í„° ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜
        return f"ë²¡í„° ê²€ìƒ‰ ê²°ê³¼: {query}ì™€ ê´€ë ¨ëœ í•™ìˆ  ë¬¸ì„œë“¤..."
    
    async def _simulate_philosopher_search(self, topic: str) -> str:
        await asyncio.sleep(0.2)  # ì² í•™ì ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜
        return f"{self.philosopher_data.get('name')}ì˜ {topic}ì— ëŒ€í•œ ì² í•™ì  ê´€ì ..."
    
    async def _enhance_arguments(self, arguments: str, web: str, vector: str, philosopher: str) -> str:
        await asyncio.sleep(0.1)
        
        prompt = f"""
ê¸°ì¡´ ë…¼ì¦: {arguments}
ì›¹ ê²€ìƒ‰ ê²°ê³¼: {web}
í•™ìˆ  ìë£Œ: {vector}  
ì² í•™ì ê´€ì : {philosopher}

ìœ„ ì •ë³´ë“¤ì„ ì¢…í•©í•˜ì—¬ ë…¼ì¦ì„ ê°•í™”í•˜ì„¸ìš”.
"""
        
        response = await self._simple_llm_call(prompt)
        return response
    
    async def _generate_final_argument(self, topic: str, stance: str, enhanced_arguments: str) -> str:
        await asyncio.sleep(0.1)
        
        philosopher_name = self.philosopher_data.get('name', 'Philosopher')
        quote = self.philosopher_data.get('quote', '')
        
        prompt = f"""
ë‹¹ì‹ ì€ {philosopher_name}ì…ë‹ˆë‹¤.
ëŒ€í‘œ ëª…ì–¸: "{quote}"

ì£¼ì œ: {topic}
ì…ì¥: {stance}
ê°•í™”ëœ ë…¼ì¦: {enhanced_arguments}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì™„ì„±ë„ ë†’ì€ ì…ë¡ ì„ ì‘ì„±í•˜ì„¸ìš”.
ì² í•™ìì˜ íŠ¹ì„±ì„ ì‚´ë ¤ ì„¤ë“ë ¥ ìˆê²Œ ì‘ì„±í•˜ì„¸ìš”.
"""
        
        response = await self._simple_llm_call(prompt)
        return response
    
    async def _simple_llm_call(self, prompt: str) -> str:
        """ë‹¨ìˆœ LLM í˜¸ì¶œ"""
        messages = [{"role": "user", "content": prompt}]
        
        try:
            response = self.llm_manager.client.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                temperature=0.7,
                max_tokens=1000
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}"

class RealLLMPerformanceDemo:
    def __init__(self):
        self.project_root = Path(__file__).parent.parent.parent
        self.debates_data = self._load_pregenerated_debates()
        self.philosophers_data = self._load_philosophers_data()
        
        # ì‹¤ì œ LLM Manager ì´ˆê¸°í™”
        try:
            self.llm_manager = RealLLMManager()
            print("âœ… OpenAI API ì—°ê²° ì„±ê³µ")
        except ValueError as e:
            print(f"âŒ {str(e)}")
            print("   export OPENAI_API_KEY=your_api_key")
            sys.exit(1)
        
    def _load_pregenerated_debates(self) -> Dict[str, Any]:
        """pregenerated_debates.json ë¡œë“œ"""
        debates_path = self.project_root / "src" / "new" / "data" / "pregenerated_debates.json"
        try:
            with open(debates_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"âŒ {debates_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {}
    
    def _load_philosophers_data(self) -> Dict[str, Any]:
        """debate_optimized.yaml ë¡œë“œ"""
        phil_path = self.project_root / "philosophers" / "debate_optimized.yaml"
        try:
            with open(phil_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            print(f"âŒ {phil_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {}

    def get_available_topics(self) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ í† ë¡  ì£¼ì œ ëª©ë¡ ë°˜í™˜"""
        topics = []
        for topic_id, topic_data in self.debates_data.get('topics', {}).items():
            topics.append({
                'id': topic_id,
                'title': topic_data.get('title', ''),
                'category': topic_data.get('category', ''),
                'pro_philosophers': topic_data.get('original_data', {}).get('pro_philosophers', []),
                'con_philosophers': topic_data.get('original_data', {}).get('con_philosophers', []),
                'context_summary': topic_data.get('generated_data', {}).get('context_summary', {}),
                'stance_statements': topic_data.get('generated_data', {}).get('stance_statements', {})
            })
        return topics

    async def run_optimized_approach(self, topic_data: Dict[str, Any], philosopher_key: str, stance: str) -> Dict[str, Any]:
        """ìµœì í™”ëœ ë°©ì‹ ì‹¤í–‰"""
        print(f"\nğŸš€ ìµœì í™”ëœ ë°©ì‹ (UnifiedDebateAgent) ì‹¤í–‰...")
        print(f"   ì£¼ì œ: {topic_data['title']}")
        print(f"   ì² í•™ì: {philosopher_key}")
        print(f"   ì…ì¥: {stance}")
        
        start_time = time.time()
        
        # UnifiedDebateAgent ì„¤ì •
        philosopher_data = self.philosophers_data.get(philosopher_key, {})
        config = {
            'llm_manager': self.llm_manager,
            'model': 'gpt-4o',
            'use_context_enhancement': True
        }
        
        agent = UnifiedDebateAgent(
            agent_id=f"{philosopher_key}_optimized",
            philosopher_data=philosopher_data,
            config=config
        )
        
        try:
            # í†µí•©ëœ ì…ë¡  ìƒì„± (Function Calling ì‚¬ìš©)
            result = await agent.generate_opening_argument(topic_data['title'], stance)
            
            end_time = time.time()
            result['duration'] = end_time - start_time
            result['method'] = 'optimized_unified'
            
            print(f"âœ… ìµœì í™”ëœ ë°©ì‹ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {result['duration']:.2f}ì´ˆ)")
            print(f"   LLM í˜¸ì¶œ: {result.get('llm_calls', 1)}íšŒ")
            
            return result
            
        except Exception as e:
            print(f"âŒ ìµœì í™”ëœ ë°©ì‹ ì‹¤íŒ¨: {str(e)}")
            return {
                'status': 'error',
                'message': str(e),
                'duration': time.time() - start_time,
                'method': 'optimized_unified'
            }

    async def run_traditional_approach(self, topic_data: Dict[str, Any], philosopher_key: str, stance: str) -> Dict[str, Any]:
        """ê¸°ì¡´ ë°©ì‹ ì‹¤í–‰"""
        print(f"\nğŸŒ ê¸°ì¡´ ë°©ì‹ (Traditional Multi-Step) ì‹¤í–‰...")
        
        philosopher_data = self.philosophers_data.get(philosopher_key, {})
        agent = TraditionalDebateAgent(philosopher_data, self.llm_manager)
        
        result = await agent.generate_traditional_argument(topic_data['title'], stance, topic_data)
        result['method'] = 'traditional_multi_step'
        
        print(f"âœ… ê¸°ì¡´ ë°©ì‹ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {result['generation_time']:.2f}ì´ˆ)")
        print(f"   LLM í˜¸ì¶œ: {result['llm_calls']}íšŒ")
        print(f"   RAG ê²€ìƒ‰: {result['rag_searches']}íšŒ")
        
        return result

    def display_comprehensive_comparison(self, optimized: Dict[str, Any], traditional: Dict[str, Any]):
        """ì¢…í•©ì ì¸ ì„±ëŠ¥ ë¹„êµ ì¶œë ¥"""
        print("\n" + "="*100)
        print("ğŸ† ì‹¤ì œ LLM ê¸°ë°˜ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼")
        print("="*100)
        
        if optimized.get('status') == 'success' and traditional.get('status') == 'success':
            opt_time = optimized.get('duration', optimized.get('generation_time', 0))
            trad_time = traditional.get('generation_time', 0)
            
            speedup = trad_time / opt_time if opt_time > 0 else 0
            
            opt_calls = optimized.get('llm_calls', 1)
            trad_calls = traditional.get('llm_calls', 1)
            call_reduction = ((trad_calls - opt_calls) / trad_calls * 100) if trad_calls > 0 else 0
            
            print(f"\nğŸš€ ìµœì í™”ëœ ë°©ì‹ (UnifiedDebateAgent):")
            print(f"   â±ï¸  ì†Œìš”ì‹œê°„: {opt_time:.2f}ì´ˆ")
            print(f"   ğŸ”„ LLM í˜¸ì¶œ: {opt_calls}íšŒ (Function Calling í™œìš©)")
            print(f"   ğŸ” í†µí•© ì²˜ë¦¬: RAG + ë…¼ì¦ìƒì„± + ì² í•™ìíŠ¹ì„± ë°˜ì˜")
            print(f"   âœ… ì½˜í…ìŠ¤íŠ¸ í™œìš©: ì™„ì „ í†µí•©")
            
            print(f"\nğŸŒ ê¸°ì¡´ ë°©ì‹ (Traditional Multi-Step):")
            print(f"   â±ï¸  ì†Œìš”ì‹œê°„: {trad_time:.2f}ì´ˆ")
            print(f"   ğŸ”„ LLM í˜¸ì¶œ: {trad_calls}íšŒ (ìˆœì°¨ì  ë‹¤ë‹¨ê³„)")
            print(f"   ğŸ” RAG ê²€ìƒ‰: {traditional.get('rag_searches', 0)}íšŒ (ë¶„ë¦¬ëœ ëª¨ë“ˆ)")
            print(f"   âŒ ì½˜í…ìŠ¤íŠ¸ í™œìš©: ì œí•œì ")
            
            print(f"\nğŸ“Š ì„±ëŠ¥ ê°œì„  íš¨ê³¼:")
            print(f"   ğŸš€ ì†ë„ í–¥ìƒ: {speedup:.1f}ë°° ë¹¨ë¼ì§")
            print(f"   ğŸ’° LLM í˜¸ì¶œ ì ˆì•½: {call_reduction:.1f}% ê°ì†Œ")
            print(f"   ğŸ”§ ì•„í‚¤í…ì²˜: í†µí•©í˜• vs ë¶„ì‚°í˜•")
            print(f"   ğŸ¯ ì •í™•ë„: ì½˜í…ìŠ¤íŠ¸ ì™„ì „ í™œìš©")
            
        print("\n" + "="*100)

    def display_argument_quality_comparison(self, optimized: Dict[str, Any], traditional: Dict[str, Any]):
        """ìƒì„±ëœ ì…ë¡ ì˜ í’ˆì§ˆ ë¹„êµ"""
        print("\nğŸ“œ ìƒì„±ëœ ì…ë¡  í’ˆì§ˆ ë¹„êµ")
        print("="*100)
        
        if optimized.get('status') == 'success':
            print("ğŸš€ ìµœì í™”ëœ ë°©ì‹ ì…ë¡ :")
            print("=" * 50)
            print(optimized.get('argument', ''))
            print("\n" + "=" * 50)
        
        if traditional.get('status') == 'success':
            print("\nğŸŒ ê¸°ì¡´ ë°©ì‹ ì…ë¡ :")
            print("=" * 50)
            print(traditional.get('argument', ''))
            print("\n" + "=" * 50)

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    demo = RealLLMPerformanceDemo()
    
    print("ğŸ¯ ì‹¤ì œ LLMì„ ì‚¬ìš©í•œ AI í† ë¡  ì—ì´ì „íŠ¸ ì„±ëŠ¥ ë¹„êµ")
    print("="*100)
    print(f"â° ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ í† ë¡  ì£¼ì œ í‘œì‹œ
    topics = demo.get_available_topics()
    if not topics:
        print("âŒ í† ë¡  ì£¼ì œë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\nğŸ“š ì‚¬ìš© ê°€ëŠ¥í•œ í† ë¡  ì£¼ì œ ({len(topics)}ê°œ):")
    for i, topic in enumerate(topics[:3], 1):  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
        print(f"   {i}. {topic['title']} ({topic['category']})")
    
    # ì²« ë²ˆì§¸ ì£¼ì œ ì„ íƒ
    selected_topic = topics[0]
    print(f"\nğŸ¯ ì„ íƒëœ ì£¼ì œ: {selected_topic['title']}")
    
    # Pro ì¸¡ ì² í•™ì ì„ íƒ
    pro_philosophers = selected_topic['pro_philosophers']
    if not pro_philosophers:
        print("âŒ Pro ì¸¡ ì² í•™ìê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
        
    philosopher_key = pro_philosophers[0]
    philosopher_name = demo.philosophers_data.get(philosopher_key, {}).get('name', philosopher_key)
    print(f"ğŸ§  ì„ íƒëœ ì² í•™ì: {philosopher_name} ({philosopher_key})")
    
    # ì½˜í…ìŠ¤íŠ¸ ì •ë³´ ì¶œë ¥
    context_summary = selected_topic.get('context_summary', {})
    if context_summary:
        print(f"\nğŸ“‹ ì½˜í…ìŠ¤íŠ¸ ìš”ì•½:")
        print(f"   ğŸ“ {context_summary.get('summary', '')}")
        key_points = context_summary.get('key_points', [])
        if key_points:
            print(f"   ğŸ”‘ í•µì‹¬ í¬ì¸íŠ¸ {len(key_points)}ê°œ:")
            for i, point in enumerate(key_points[:3], 1):
                print(f"      {i}. {point}")
    
    print(f"\nğŸ”„ ì‹¤ì œ LLM ì„±ëŠ¥ ë¹„êµ ì‹œì‘...")
    print("   (ì‹¤ì œ OpenAI APIë¥¼ í˜¸ì¶œí•˜ë¯€ë¡œ ì‹œê°„ì´ ì†Œìš”ë©ë‹ˆë‹¤)")
    
    # 1. ìµœì í™”ëœ ë°©ì‹ ì‹¤í–‰
    optimized_result = await demo.run_optimized_approach(selected_topic, philosopher_key, 'pro')
    
    # 2. ê¸°ì¡´ ë°©ì‹ ì‹¤í–‰  
    traditional_result = await demo.run_traditional_approach(selected_topic, philosopher_key, 'pro')
    
    # ê²°ê³¼ ë¹„êµ ë° ë¶„ì„
    demo.display_comprehensive_comparison(optimized_result, traditional_result)
    demo.display_argument_quality_comparison(optimized_result, traditional_result)
    
    print(f"\nâœ¨ ì‹¤ì œ LLM ì„±ëŠ¥ ë¹„êµ ì™„ë£Œ!")
    print(f"â° ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("\nğŸ¯ ê²°ë¡ : ìµœì í™”ëœ ë°©ì‹ì´ ì†ë„, íš¨ìœ¨ì„±, í’ˆì§ˆ ëª¨ë“  ë©´ì—ì„œ ìš°ìˆ˜í•¨ì„ ì‹¤ì¦í–ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    asyncio.run(main()) 