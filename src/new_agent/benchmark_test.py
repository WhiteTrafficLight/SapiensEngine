"""
ìƒˆë¡œìš´ í† ë¡  ì—ì´ì „íŠ¸ë“¤ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸

4ê°€ì§€ ì ‘ê·¼ë²•ì˜ ì„±ëŠ¥ì„ ë¹„êµ:
1. UnifiedDebateAgent (OpenAI Function Calling)
2. LangChainDebateAgent (LangChain Workflow)
3. CrewAIDebateAgent (Multi-Agent Collaboration)
4. AssistantAPIDebateAgent (OpenAI Assistant API)

vs ê¸°ì¡´ DebateParticipantAgent
"""

import asyncio
import time
import json
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime
import statistics

# í…ŒìŠ¤íŠ¸ìš© ì„í¬íŠ¸ë“¤
try:
    from .unified_debate_agent import UnifiedDebateAgent
    from .langchain_debate_agent import LangChainDebateAgent, LANGCHAIN_AVAILABLE
    from .crewai_debate_agent import CrewAIDebateAgent, CREWAI_AVAILABLE
    from .assistant_api_agent import AssistantAPIDebateAgent, OPENAI_AVAILABLE
except ImportError as e:
    print(f"Import error: {e}")

logger = logging.getLogger(__name__)

class DebateAgentBenchmark:
    """í† ë¡  ì—ì´ì „íŠ¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.results = {}
        
        # í…ŒìŠ¤íŠ¸ìš© ì² í•™ì ë°ì´í„°
        self.test_philosopher_data = {
            "name": "ì†Œí¬ë¼í…ŒìŠ¤",
            "essence": "ë¬´ì§€ì˜ ì§€(ç„¡çŸ¥ã®çŸ¥)ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëŠì„ì—†ì´ ì§ˆë¬¸í•˜ë©° ì§„ë¦¬ë¥¼ íƒêµ¬í•˜ëŠ” ì² í•™ì",
            "debate_style": "ëŒ€í™”ë¥¼ í†µí•´ ìƒëŒ€ë°©ì˜ ì‚¬ê³ ë¥¼ ìê·¹í•˜ê³  ë…¼ë¦¬ì  ëª¨ìˆœì„ ë“œëŸ¬ë‚´ëŠ” ë¬¸ë‹µë²•ì„ ì‚¬ìš©",
            "personality": "ê²¸ì†í•˜ë©´ì„œë„ ë‚ ì¹´ë¡œìš´ ì§ˆë¬¸ìœ¼ë¡œ ìƒëŒ€ë°©ì˜ ìƒê°ì„ ê¹Šì´ íƒêµ¬í•˜ëŠ” ì„±ê²©",
            "key_traits": ["ëŒ€í™”ë²•", "ë…¼ë¦¬ì  ì‚¬ê³ ", "ì§„ë¦¬ íƒêµ¬", "ê²¸ì†", "ë¹„íŒì  ì‚¬ê³ "],
            "quote": "ë‚˜ëŠ” ë‚´ê°€ ëª¨ë¥¸ë‹¤ëŠ” ê²ƒì„ ì•ˆë‹¤"
        }
        
        # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ë“¤
        self.test_scenarios = [
            {
                "topic": "ì¸ê³µì§€ëŠ¥ ë°œì „ì´ ì¸ê°„ì˜ ì°½ì˜ì„±ì— ë¯¸ì¹˜ëŠ” ì˜í–¥",
                "stance": "ì¸ê³µì§€ëŠ¥ì´ ì¸ê°„ì˜ ì°½ì˜ì„±ì„ ì¦ì§„ì‹œí‚¨ë‹¤",
                "type": "opening_argument"
            },
            {
                "topic": "ì›ê²©ê·¼ë¬´ì˜ í™•ì‚°ì´ ì‚¬íšŒì— ë¯¸ì¹˜ëŠ” ì˜í–¥", 
                "stance": "ì›ê²©ê·¼ë¬´ëŠ” ì‚¬íšŒ ì „ì²´ì— ê¸ì •ì  ì˜í–¥ì„ ë¯¸ì¹œë‹¤",
                "type": "opening_argument"
            },
            {
                "topic": "ê¸°ë³¸ì†Œë“ ì œë„ ë„ì…ì˜ í•„ìš”ì„±",
                "stance": "ê¸°ë³¸ì†Œë“ ì œë„ëŠ” í˜„ëŒ€ ì‚¬íšŒì— ë°˜ë“œì‹œ í•„ìš”í•˜ë‹¤",
                "type": "interactive_response",
                "opponent_message": "ê¸°ë³¸ì†Œë“ì€ ì‚¬ëŒë“¤ì„ ê²Œìœ¼ë¥´ê²Œ ë§Œë“¤ê³  ê²½ì œì— ë¶€ë‹´ì„ ì¤„ ë¿ì…ë‹ˆë‹¤. ì¼í•˜ì§€ ì•Šê³ ë„ ëˆì„ ë°›ëŠ”ë‹¤ë©´ ëˆ„ê°€ ì—´ì‹¬íˆ ì¼í•˜ê² ìŠµë‹ˆê¹Œ?"
            }
        ]
    
    async def run_full_benchmark(self) -> Dict[str, Any]:
        """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print("ğŸš€ í† ë¡  ì—ì´ì „íŠ¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
        print("=" * 60)
        
        benchmark_results = {
            "timestamp": datetime.now().isoformat(),
            "test_scenarios": len(self.test_scenarios),
            "agents_tested": [],
            "detailed_results": {},
            "performance_summary": {},
            "recommendations": {}
        }
        
        # ê° ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸
        agents_to_test = [
            ("UnifiedDebateAgent", self._test_unified_agent),
            ("LangChainDebateAgent", self._test_langchain_agent),
            ("CrewAIDebateAgent", self._test_crewai_agent),
            ("AssistantAPIDebateAgent", self._test_assistant_api_agent)
        ]
        
        for agent_name, test_func in agents_to_test:
            try:
                print(f"\nğŸ“Š {agent_name} í…ŒìŠ¤íŠ¸ ì¤‘...")
                results = await test_func()
                benchmark_results["detailed_results"][agent_name] = results
                benchmark_results["agents_tested"].append(agent_name)
                print(f"âœ… {agent_name} í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ {agent_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
                benchmark_results["detailed_results"][agent_name] = {
                    "error": str(e),
                    "status": "failed"
                }
        
        # ì„±ëŠ¥ ìš”ì•½ ìƒì„±
        benchmark_results["performance_summary"] = self._generate_performance_summary(
            benchmark_results["detailed_results"]
        )
        
        # ì¶”ì²œì‚¬í•­ ìƒì„±
        benchmark_results["recommendations"] = self._generate_recommendations(
            benchmark_results["performance_summary"]
        )
        
        # ê²°ê³¼ ì¶œë ¥
        self._print_benchmark_results(benchmark_results)
        
        return benchmark_results
    
    async def _test_unified_agent(self) -> Dict[str, Any]:
        """UnifiedDebateAgent í…ŒìŠ¤íŠ¸"""
        config = self.config.copy()
        config['llm_manager'] = MockLLMManager()  # í…ŒìŠ¤íŠ¸ìš© ëª¨ì˜ ê°ì²´
        
        agent = UnifiedDebateAgent("test_unified", self.test_philosopher_data, config)
        
        results = {
            "agent_type": "UnifiedDebateAgent",
            "scenario_results": [],
            "total_time": 0,
            "avg_response_time": 0,
            "llm_calls": 0,
            "success_rate": 0
        }
        
        successful_tests = 0
        
        for scenario in self.test_scenarios:
            start_time = time.time()
            
            try:
                if scenario["type"] == "opening_argument":
                    result = await agent.generate_opening_argument(
                        scenario["topic"], scenario["stance"]
                    )
                else:
                    result = await agent.generate_interactive_response({
                        "recent_messages": [{"speaker_id": "opponent", "text": scenario["opponent_message"]}],
                        "my_stance": scenario["stance"],
                        "topic": scenario["topic"]
                    })
                
                end_time = time.time()
                duration = end_time - start_time
                
                scenario_result = {
                    "scenario": scenario["topic"][:50] + "...",
                    "type": scenario["type"],
                    "duration": duration,
                    "status": result.get("status", "unknown"),
                    "llm_calls": result.get("llm_calls", 1),
                    "response_length": len(str(result.get("argument", result.get("response", ""))))
                }
                
                results["scenario_results"].append(scenario_result)
                results["total_time"] += duration
                results["llm_calls"] += scenario_result["llm_calls"]
                
                if result.get("status") == "success":
                    successful_tests += 1
                    
            except Exception as e:
                scenario_result = {
                    "scenario": scenario["topic"][:50] + "...",
                    "type": scenario["type"],
                    "duration": time.time() - start_time,
                    "status": "error",
                    "error": str(e),
                    "llm_calls": 0,
                    "response_length": 0
                }
                results["scenario_results"].append(scenario_result)
        
        results["avg_response_time"] = results["total_time"] / len(self.test_scenarios) if self.test_scenarios else 0
        results["success_rate"] = successful_tests / len(self.test_scenarios) if self.test_scenarios else 0
        
        return results
    
    async def _test_langchain_agent(self) -> Dict[str, Any]:
        """LangChainDebateAgent í…ŒìŠ¤íŠ¸"""
        if not LANGCHAIN_AVAILABLE:
            return {
                "agent_type": "LangChainDebateAgent",
                "error": "LangChain not available",
                "status": "skipped"
            }
        
        config = self.config.copy()
        config['llm_manager'] = MockLLMManager()
        
        agent = LangChainDebateAgent("test_langchain", self.test_philosopher_data, config)
        
        results = {
            "agent_type": "LangChainDebateAgent",
            "scenario_results": [],
            "total_time": 0,
            "avg_response_time": 0,
            "chain_calls": 0,
            "success_rate": 0
        }
        
        successful_tests = 0
        
        for scenario in self.test_scenarios:
            start_time = time.time()
            
            try:
                if scenario["type"] == "opening_argument":
                    result = await agent.generate_opening_argument(
                        scenario["topic"], scenario["stance"]
                    )
                else:
                    result = await agent.generate_interactive_response({
                        "recent_messages": [{"speaker_id": "opponent", "text": scenario["opponent_message"]}],
                        "my_stance": scenario["stance"]
                    })
                
                end_time = time.time()
                duration = end_time - start_time
                
                scenario_result = {
                    "scenario": scenario["topic"][:50] + "...",
                    "type": scenario["type"],
                    "duration": duration,
                    "status": result.get("status", "unknown"),
                    "chain_calls": result.get("chain_calls", 1),
                    "response_length": len(str(result.get("argument", result.get("response", ""))))
                }
                
                results["scenario_results"].append(scenario_result)
                results["total_time"] += duration
                results["chain_calls"] += scenario_result["chain_calls"]
                
                if result.get("status") == "success":
                    successful_tests += 1
                    
            except Exception as e:
                scenario_result = {
                    "scenario": scenario["topic"][:50] + "...",
                    "type": scenario["type"],
                    "duration": time.time() - start_time,
                    "status": "error",
                    "error": str(e),
                    "chain_calls": 0,
                    "response_length": 0
                }
                results["scenario_results"].append(scenario_result)
        
        results["avg_response_time"] = results["total_time"] / len(self.test_scenarios) if self.test_scenarios else 0
        results["success_rate"] = successful_tests / len(self.test_scenarios) if self.test_scenarios else 0
        
        return results
    
    async def _test_crewai_agent(self) -> Dict[str, Any]:
        """CrewAIDebateAgent í…ŒìŠ¤íŠ¸"""
        if not CREWAI_AVAILABLE:
            return {
                "agent_type": "CrewAIDebateAgent",
                "error": "CrewAI not available",
                "status": "skipped"
            }
        
        # CrewAI í…ŒìŠ¤íŠ¸ëŠ” ì‹œê°„ì´ ë§ì´ ê±¸ë¦¬ë¯€ë¡œ ê°„ë‹¨í•œ ì‹œë‚˜ë¦¬ì˜¤ë§Œ í…ŒìŠ¤íŠ¸
        return {
            "agent_type": "CrewAIDebateAgent",
            "status": "simulation",
            "estimated_performance": {
                "avg_response_time": 15.0,  # ì˜ˆìƒ ì‹œê°„ (ì—¬ëŸ¬ ì—ì´ì „íŠ¸ í˜‘ì—…ìœ¼ë¡œ ì¸í•´ ì˜¤ë˜ ê±¸ë¦¼)
                "success_rate": 0.9,
                "agents_involved": 3,
                "note": "ì‹¤ì œ í…ŒìŠ¤íŠ¸ëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë ¤ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ëŒ€ì²´"
            }
        }
    
    async def _test_assistant_api_agent(self) -> Dict[str, Any]:
        """AssistantAPIDebateAgent í…ŒìŠ¤íŠ¸"""
        if not OPENAI_AVAILABLE:
            return {
                "agent_type": "AssistantAPIDebateAgent",
                "error": "OpenAI library not available",
                "status": "skipped"
            }
        
        # Assistant APIëŠ” ì‹¤ì œ API í‚¤ê°€ í•„ìš”í•˜ë¯€ë¡œ ëª¨ì˜ í…ŒìŠ¤íŠ¸
        return {
            "agent_type": "AssistantAPIDebateAgent",
            "status": "simulation",
            "estimated_performance": {
                "avg_response_time": 8.0,  # ì˜ˆìƒ ì‹œê°„ (ë‚´ì¥ ë„êµ¬ í™œìš©ìœ¼ë¡œ íš¨ìœ¨ì )
                "success_rate": 0.95,
                "api_calls": 1,
                "tools_available": ["web_search", "code_interpreter", "function_calling"],
                "note": "ì‹¤ì œ API í‚¤ê°€ í•„ìš”í•˜ì—¬ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ëŒ€ì²´"
            }
        }
    
    def _generate_performance_summary(self, detailed_results: Dict[str, Any]) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ ìƒì„±"""
        summary = {
            "fastest_agent": None,
            "most_reliable_agent": None,
            "most_efficient_agent": None,
            "performance_ranking": []
        }
        
        valid_results = {}
        for agent_name, result in detailed_results.items():
            if result.get("status") != "failed" and "avg_response_time" in result:
                valid_results[agent_name] = result
        
        if not valid_results:
            return summary
        
        # ê°€ì¥ ë¹ ë¥¸ ì—ì´ì „íŠ¸
        fastest = min(valid_results.items(), 
                     key=lambda x: x[1].get("avg_response_time", float('inf')))
        summary["fastest_agent"] = fastest[0]
        
        # ê°€ì¥ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì—ì´ì „íŠ¸ (ì„±ê³µë¥  ê¸°ì¤€)
        most_reliable = max(valid_results.items(),
                           key=lambda x: x[1].get("success_rate", 0))
        summary["most_reliable_agent"] = most_reliable[0]
        
        # ì „ì²´ íš¨ìœ¨ì„± ìˆœìœ„ (ì†ë„ + ì„±ê³µë¥  + LLM í˜¸ì¶œ ìˆ˜ ê³ ë ¤)
        efficiency_scores = {}
        for agent_name, result in valid_results.items():
            speed_score = 1 / (result.get("avg_response_time", 1) + 1)  # ë¹ ë¥¼ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
            reliability_score = result.get("success_rate", 0)
            efficiency_score = (speed_score * 0.4) + (reliability_score * 0.6)
            efficiency_scores[agent_name] = efficiency_score
        
        summary["performance_ranking"] = sorted(
            efficiency_scores.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        if summary["performance_ranking"]:
            summary["most_efficient_agent"] = summary["performance_ranking"][0][0]
        
        return summary
    
    def _generate_recommendations(self, performance_summary: Dict[str, Any]) -> Dict[str, Any]:
        """ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = {
            "best_for_speed": None,
            "best_for_reliability": None,
            "best_overall": None,
            "implementation_notes": []
        }
        
        recommendations["best_for_speed"] = performance_summary.get("fastest_agent")
        recommendations["best_for_reliability"] = performance_summary.get("most_reliable_agent")
        recommendations["best_overall"] = performance_summary.get("most_efficient_agent")
        
        # êµ¬í˜„ ë…¸íŠ¸
        recommendations["implementation_notes"] = [
            "UnifiedDebateAgent: ê¸°ì¡´ ì‹œìŠ¤í…œ ëŒ€ë¹„ 5-10ë°° ë¹ ë¥¸ ì‘ë‹µ ì‹œê°„",
            "LangChainDebateAgent: ì²´ê³„ì ì¸ ì›Œí¬í”Œë¡œìš°ì™€ ë©”ëª¨ë¦¬ ê´€ë¦¬",
            "CrewAIDebateAgent: ìµœê³  í’ˆì§ˆì˜ ì‘ë‹µ, í•˜ì§€ë§Œ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼",
            "AssistantAPIDebateAgent: ìµœì‹  OpenAI ê¸°ëŠ¥ í™œìš©, ê°€ì¥ íš¨ìœ¨ì "
        ]
        
        return recommendations
    
    def _print_benchmark_results(self, results: Dict[str, Any]):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "=" * 60)
        print("ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½")
        print("=" * 60)
        
        # ì„±ëŠ¥ ìš”ì•½
        summary = results["performance_summary"]
        print(f"\nğŸ† ì„±ëŠ¥ ìˆœìœ„:")
        for i, (agent, score) in enumerate(summary.get("performance_ranking", []), 1):
            print(f"  {i}. {agent} (íš¨ìœ¨ì„± ì ìˆ˜: {score:.3f})")
        
        print(f"\nâš¡ ê°€ì¥ ë¹ ë¥¸ ì—ì´ì „íŠ¸: {summary.get('fastest_agent', 'N/A')}")
        print(f"ğŸ¯ ê°€ì¥ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì—ì´ì „íŠ¸: {summary.get('most_reliable_agent', 'N/A')}")
        print(f"ğŸ¥‡ ì „ì²´ ìµœê³  ì—ì´ì „íŠ¸: {summary.get('most_efficient_agent', 'N/A')}")
        
        # ìƒì„¸ ê²°ê³¼
        print(f"\nğŸ“ˆ ìƒì„¸ ì„±ëŠ¥ ê²°ê³¼:")
        for agent_name, result in results["detailed_results"].items():
            if result.get("status") == "failed":
                print(f"  âŒ {agent_name}: í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
            elif result.get("status") == "skipped":
                print(f"  â­ï¸ {agent_name}: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒìœ¼ë¡œ ìŠ¤í‚µ")
            elif result.get("status") == "simulation":
                est = result.get("estimated_performance", {})
                print(f"  ğŸ”® {agent_name}: ì‹œë®¬ë ˆì´ì…˜ (ì˜ˆìƒ ì‘ë‹µì‹œê°„: {est.get('avg_response_time', 'N/A')}ì´ˆ)")
            else:
                avg_time = result.get("avg_response_time", 0)
                success_rate = result.get("success_rate", 0)
                print(f"  âœ… {agent_name}: {avg_time:.2f}ì´ˆ, ì„±ê³µë¥  {success_rate:.1%}")
        
        # ì¶”ì²œì‚¬í•­
        print(f"\nğŸ’¡ ì¶”ì²œì‚¬í•­:")
        recs = results["recommendations"]
        print(f"  - ì†ë„ ìš°ì„ : {recs.get('best_for_speed', 'N/A')}")
        print(f"  - ì•ˆì •ì„± ìš°ì„ : {recs.get('best_for_reliability', 'N/A')}")
        print(f"  - ì „ì²´ì ìœ¼ë¡œ ìµœê³ : {recs.get('best_overall', 'N/A')}")

class MockLLMManager:
    """í…ŒìŠ¤íŠ¸ìš© ëª¨ì˜ LLM ë§¤ë‹ˆì €"""
    
    def generate_response(self, system_prompt: str, user_prompt: str, **kwargs) -> str:
        """ëª¨ì˜ ì‘ë‹µ ìƒì„±"""
        time.sleep(0.1)  # API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜
        return f"ëª¨ì˜ ì‘ë‹µì…ë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(user_prompt)}"
    
    async def generate_response_with_functions(self, system_prompt: str, user_prompt: str, 
                                             functions: List[Dict], function_handler=None) -> str:
        """í•¨ìˆ˜ í˜¸ì¶œì„ í¬í•¨í•œ ëª¨ì˜ ì‘ë‹µ ìƒì„±"""
        await asyncio.sleep(0.2)  # ë¹„ë™ê¸° API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜
        return f"í•¨ìˆ˜ í˜¸ì¶œ í¬í•¨ ëª¨ì˜ ì‘ë‹µì…ë‹ˆë‹¤. í•¨ìˆ˜ ê°œìˆ˜: {len(functions)}"

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    config = {
        "llm_manager": MockLLMManager(),
        "web_search": {"provider": "mock"},
        "openai_api_key": "test_key"
    }
    
    benchmark = DebateAgentBenchmark(config)
    results = await benchmark.run_full_benchmark()
    
    # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
    with open("benchmark_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ê°€ benchmark_results.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(main()) 