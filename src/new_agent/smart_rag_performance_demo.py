#!/usr/bin/env python3
"""
ì§€ëŠ¥í˜• RAG ì„±ëŠ¥ ë¹„êµ ë°ëª¨
- ê¸°ì¡´ ë°©ì‹: ëª¨ë“  ë…¼ì§€ì— ëŒ€í•´ ì¼ê´„ RAG ê²€ìƒ‰
- ìƒˆë¡œìš´ ë°©ì‹: í•„ìš”í•  ë•Œë§Œ ì§€ëŠ¥ì ìœ¼ë¡œ RAG/ì›¹ì„œì¹˜ ì‚¬ìš©
"""

import asyncio
import json
import yaml
import time
import sys
import os
from typing import Dict, Any, List
from pathlib import Path
from datetime import datetime

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from src.new_agent.smart_rag_unified_agent import SmartRAGUnifiedAgent
from src.new_agent.real_llm_performance_demo import RealLLMManager, TraditionalDebateAgent

class SmartRAGPerformanceDemo:
    def __init__(self):
        self.project_root = Path(__file__).parent.parent.parent
        self.debates_data = self._load_pregenerated_debates()
        self.philosophers_data = self._load_philosophers_data()
        
        # ì‹¤ì œ LLM Manager ì´ˆê¸°í™”
        try:
            self.llm_manager = RealLLMManager()
            print("âœ… OpenAI API ì—°ê²° ì„±ê³µ")
        except ValueError as e:
            print(f"âŒ {str(e)}")
            print("   export OPENAI_API_KEY=your_api_key")
            sys.exit(1)
    
    def _load_pregenerated_debates(self) -> Dict[str, Any]:
        """pregenerated_debates.json ë¡œë“œ"""
        debates_path = self.project_root / "src" / "new" / "data" / "pregenerated_debates.json"
        try:
            with open(debates_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"âŒ {debates_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {}
    
    def _load_philosophers_data(self) -> Dict[str, Any]:
        """debate_optimized.yaml ë¡œë“œ"""
        phil_path = self.project_root / "philosophers" / "debate_optimized.yaml"
        try:
            with open(phil_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            print(f"âŒ {phil_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {}

    def get_test_scenarios(self) -> List[Dict[str, Any]]:
        """RAG í•„ìš”ì„±ì— ë”°ë¥¸ ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤"""
        return [
            {
                'name': 'ğŸŒ ì›¹ì„œì¹˜ í•„ìš” ì£¼ì œ',
                'topic': 'Should AI be regulated by government policies in 2024?',
                'category': 'current_events',
                'expected_rag': ['web_search'],
                'reasoning': 'ìµœì‹  ì •ë³´ì™€ ì •ë¶€ ì •ì±…ì´ í•„ìš”'
            },
            {
                'name': 'ğŸ“Š ë°ì´í„° ì¤‘ì‹¬ ì£¼ì œ', 
                'topic': 'Do statistics show that remote work increases productivity?',
                'category': 'data_heavy',
                'expected_rag': ['web_search', 'vector_search'],
                'reasoning': 'í†µê³„ì™€ ì—°êµ¬ ë°ì´í„°ê°€ í•„ìš”'
            },
            {
                'name': 'ğŸ¤” ì² í•™ì  ì£¼ì œ',
                'topic': 'Is consciousness purely a product of brain activity?',
                'category': 'philosophical_depth', 
                'expected_rag': ['philosopher_search', 'vector_search'],
                'reasoning': 'ì² í•™ì  ê¹Šì´ì™€ ì‚¬ìƒê°€ë“¤ì˜ ê²¬í•´ê°€ í•„ìš”'
            },
            {
                'name': 'ğŸ’­ ë‹¨ìˆœ ìœ¤ë¦¬ ì£¼ì œ',
                'topic': 'Is it wrong to lie to protect someone\'s feelings?',
                'category': 'simple_ethics',
                'expected_rag': [],
                'reasoning': 'ê¸°ë³¸ì ì¸ ë…¼ì¦ë§Œìœ¼ë¡œ ì¶©ë¶„, RAG ë¶ˆí•„ìš”'
            }
        ]

    async def run_smart_rag_approach(self, scenario: Dict[str, Any], philosopher_key: str, stance: str) -> Dict[str, Any]:
        """ì§€ëŠ¥í˜• RAG ë°©ì‹ ì‹¤í–‰"""
        print(f"\nğŸ§  ì§€ëŠ¥í˜• RAG ë°©ì‹ ({scenario['name']}) ì‹¤í–‰...")
        print(f"   ì£¼ì œ: {scenario['topic']}")
        print(f"   ì˜ˆìƒ RAG: {scenario['expected_rag']}")
        
        start_time = time.time()
        
        philosopher_data = self.philosophers_data.get(philosopher_key, {})
        config = {
            'llm_manager': self.llm_manager,
            'model': 'gpt-4o',
            'use_context_enhancement': True
        }
        
        agent = SmartRAGUnifiedAgent(
            agent_id=f"{philosopher_key}_smart_rag",
            philosopher_data=philosopher_data,
            config=config
        )
        
        try:
            # ì§€ëŠ¥í˜• ì…ë¡  ìƒì„±
            result = await agent.generate_intelligent_opening_argument(
                topic=scenario['topic'], 
                stance=stance
            )
            
            result['scenario'] = scenario['name']
            result['expected_vs_actual'] = {
                'expected_rag': scenario['expected_rag'],
                'actual_web_searches': result.get('web_searches', 0),
                'actual_vector_searches': result.get('vector_searches', 0)
            }
            
            print(f"âœ… ì§€ëŠ¥í˜• RAG ë°©ì‹ ì™„ë£Œ")
            print(f"   â±ï¸  ì†Œìš”ì‹œê°„: {result['generation_time']:.2f}ì´ˆ")
            print(f"   ğŸ”„ LLM í˜¸ì¶œ: {result['llm_calls']}íšŒ")
            print(f"   ğŸŒ ì›¹ ê²€ìƒ‰: {result['web_searches']}íšŒ")
            print(f"   ğŸ“š ë²¡í„° ê²€ìƒ‰: {result['vector_searches']}íšŒ")
            
            return result
            
        except Exception as e:
            print(f"âŒ ì§€ëŠ¥í˜• RAG ë°©ì‹ ì‹¤íŒ¨: {str(e)}")
            return {
                'status': 'error',
                'message': str(e),
                'generation_time': time.time() - start_time,
                'scenario': scenario['name']
            }

    async def run_traditional_heavy_rag_approach(self, scenario: Dict[str, Any], philosopher_key: str, stance: str) -> Dict[str, Any]:
        """ê¸°ì¡´ ë°©ì‹ (ëª¨ë“  ë…¼ì§€ì— ëŒ€í•´ RAG ê²€ìƒ‰) ì‹¤í–‰"""
        print(f"\nğŸŒ ê¸°ì¡´ Heavy RAG ë°©ì‹ ({scenario['name']}) ì‹¤í–‰...")
        
        start_time = time.time()
        philosopher_data = self.philosophers_data.get(philosopher_key, {})
        
        # Enhanced Traditional Agent with Heavy RAG
        agent = EnhancedTraditionalAgent(philosopher_data, self.llm_manager)
        
        result = await agent.generate_heavy_rag_argument(
            topic=scenario['topic'], 
            stance=stance, 
            context_data=scenario
        )
        
        result['scenario'] = scenario['name']
        result['method'] = 'traditional_heavy_rag'
        
        print(f"âœ… ê¸°ì¡´ Heavy RAG ë°©ì‹ ì™„ë£Œ")
        print(f"   â±ï¸  ì†Œìš”ì‹œê°„: {result['generation_time']:.2f}ì´ˆ")
        print(f"   ğŸ”„ LLM í˜¸ì¶œ: {result['llm_calls']}íšŒ") 
        print(f"   ğŸ” RAG ê²€ìƒ‰: {result['rag_searches']}íšŒ (í•„ìˆ˜)")
        
        return result

    def display_smart_rag_analysis(self, smart_results: List[Dict[str, Any]], traditional_results: List[Dict[str, Any]]):
        """ì§€ëŠ¥í˜• RAG ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*120)
        print("ğŸ¯ ì§€ëŠ¥í˜• RAG vs ê¸°ì¡´ Heavy RAG ë°©ì‹ ë¹„êµ ë¶„ì„")
        print("="*120)
        
        total_smart_time = sum(r.get('generation_time', 0) for r in smart_results if r.get('status') == 'success')
        total_trad_time = sum(r.get('generation_time', 0) for r in traditional_results if r.get('status') == 'success')
        
        total_smart_web = sum(r.get('web_searches', 0) for r in smart_results)
        total_smart_vector = sum(r.get('vector_searches', 0) for r in smart_results)
        total_smart_llm = sum(r.get('llm_calls', 0) for r in smart_results)
        
        total_trad_rag = sum(r.get('rag_searches', 0) for r in traditional_results)
        total_trad_llm = sum(r.get('llm_calls', 0) for r in traditional_results)
        
        print(f"\nğŸ“Š ì „ì²´ ì„±ëŠ¥ ìš”ì•½:")
        print(f"   ğŸ§  ì§€ëŠ¥í˜• RAG ë°©ì‹:")
        print(f"      â±ï¸  ì´ ì†Œìš”ì‹œê°„: {total_smart_time:.2f}ì´ˆ")
        print(f"      ğŸ”„ ì´ LLM í˜¸ì¶œ: {total_smart_llm}íšŒ")
        print(f"      ğŸŒ ì´ ì›¹ ê²€ìƒ‰: {total_smart_web}íšŒ")
        print(f"      ğŸ“š ì´ ë²¡í„° ê²€ìƒ‰: {total_smart_vector}íšŒ")
        print(f"      ğŸ¯ ê²€ìƒ‰ íš¨ìœ¨ì„±: í•„ìš”ì‹œì—ë§Œ ì„ íƒì  ì‹¤í–‰")
        
        print(f"\n   ğŸŒ ê¸°ì¡´ Heavy RAG ë°©ì‹:")
        print(f"      â±ï¸  ì´ ì†Œìš”ì‹œê°„: {total_trad_time:.2f}ì´ˆ")
        print(f"      ğŸ”„ ì´ LLM í˜¸ì¶œ: {total_trad_llm}íšŒ")
        print(f"      ğŸ” ì´ RAG ê²€ìƒ‰: {total_trad_rag}íšŒ (ëª¨ë“  ë…¼ì§€ì— ëŒ€í•´ í•„ìˆ˜)")
        print(f"      ğŸ’¸ ê²€ìƒ‰ ë¹„íš¨ìœ¨ì„±: ë¶ˆí•„ìš”í•œ ê²½ìš°ì—ë„ ì¼ê´„ ì‹¤í–‰")
        
        if total_trad_time > 0:
            speedup = total_trad_time / total_smart_time if total_smart_time > 0 else 0
            search_reduction = ((total_trad_rag - (total_smart_web + total_smart_vector)) / total_trad_rag * 100) if total_trad_rag > 0 else 0
            
            print(f"\nğŸš€ ì„±ëŠ¥ ê°œì„  íš¨ê³¼:")
            print(f"   âš¡ ì†ë„ í–¥ìƒ: {speedup:.1f}ë°° ë¹¨ë¼ì§")
            print(f"   ğŸ’° ê²€ìƒ‰ ì ˆì•½: {search_reduction:.1f}% ê²€ìƒ‰ íšŸìˆ˜ ê°ì†Œ")
            print(f"   ğŸ¯ ì§€ëŠ¥ì„±: ìƒí™©ì— ë§ëŠ” ì ì‘í˜• RAG")
            print(f"   ğŸ”§ íš¨ìœ¨ì„±: ë¶ˆí•„ìš”í•œ ì‘ì—… ì œê±°")
        
        print(f"\nğŸ“ ì‹œë‚˜ë¦¬ì˜¤ë³„ RAG ì‚¬ìš© ë¶„ì„:")
        for i, (smart, traditional) in enumerate(zip(smart_results, traditional_results)):
            if smart.get('status') == 'success':
                expected = smart.get('expected_vs_actual', {})
                print(f"   {i+1}. {smart.get('scenario', 'Unknown')}")
                print(f"      ì˜ˆìƒ RAG: {expected.get('expected_rag', [])}")
                print(f"      ì‹¤ì œ ì‹¤í–‰: ì›¹={expected.get('actual_web_searches', 0)}íšŒ, ë²¡í„°={expected.get('actual_vector_searches', 0)}íšŒ")
                print(f"      ê¸°ì¡´ ë°©ì‹: RAG={traditional.get('rag_searches', 0)}íšŒ (ë¬´ì¡°ê±´)")
                print(f"      íš¨ìœ¨ì„±: {'âœ… ì ì ˆ' if len(expected.get('expected_rag', [])) > 0 else 'âœ… ë¶ˆí•„ìš”í•œ ê²€ìƒ‰ íšŒí”¼'}")

class EnhancedTraditionalAgent(TraditionalDebateAgent):
    """ê¸°ì¡´ ë°©ì‹ì„ Heavy RAGë¡œ ê°•í™”í•œ ì—ì´ì „íŠ¸"""
    
    async def generate_heavy_rag_argument(self, topic: str, stance: str, context_data: Dict[str, Any]) -> Dict[str, Any]:
        """Heavy RAG ë°©ì‹ ì…ë¡  ìƒì„± (ëª¨ë“  ë…¼ì§€ì— ëŒ€í•´ RAG í•„ìˆ˜)"""
        start_time = time.time()
        print(f"   ğŸ” ëª¨ë“  ë…¼ì§€ì— ëŒ€í•´ í•„ìˆ˜ RAG ê²€ìƒ‰ ì‹œì‘...")
        
        try:
            # 1ë‹¨ê³„: í•µì‹¬ ë…¼ì¦ ìƒì„±
            core_arguments = await self._generate_core_arguments(topic, stance)
            self.llm_calls += 1
            
            # 2ë‹¨ê³„: ëª¨ë“  ë…¼ì§€ì— ëŒ€í•´ í•„ìˆ˜ RAG (3ê°€ì§€ ì†ŒìŠ¤)
            print("   ğŸŒ í•„ìˆ˜ ì›¹ ê²€ìƒ‰ 1...")
            web_search_1 = await self._simulate_web_search(f"{topic} current data")
            self.rag_searches += 1
            
            print("   ğŸŒ í•„ìˆ˜ ì›¹ ê²€ìƒ‰ 2...")
            web_search_2 = await self._simulate_web_search(f"{topic} expert opinions")  
            self.rag_searches += 1
            
            print("   ğŸ“š í•„ìˆ˜ ë²¡í„° ê²€ìƒ‰ 1...")
            vector_search_1 = await self._simulate_vector_search(f"{topic} academic research")
            self.rag_searches += 1
            
            print("   ğŸ“š í•„ìˆ˜ ë²¡í„° ê²€ìƒ‰ 2...")
            vector_search_2 = await self._simulate_vector_search(f"{topic} philosophical analysis")
            self.rag_searches += 1
            
            print("   ğŸ§  í•„ìˆ˜ ì² í•™ì ê²€ìƒ‰...")
            philosopher_search = await self._simulate_philosopher_search(topic)
            self.rag_searches += 1
            
            # 3ë‹¨ê³„: ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ í†µí•©
            print("   ğŸ”„ ëª¨ë“  RAG ê²°ê³¼ í†µí•©...")
            integrated_rag = await self._integrate_all_rag_results(
                core_arguments, web_search_1, web_search_2, 
                vector_search_1, vector_search_2, philosopher_search
            )
            self.llm_calls += 1
            
            # 4ë‹¨ê³„: ìµœì¢… ì…ë¡  ìƒì„±
            print("   âœï¸  ìµœì¢… ì…ë¡  ìƒì„±...")
            final_argument = await self._generate_final_argument(topic, stance, integrated_rag)
            self.llm_calls += 1
            
            end_time = time.time()
            
            return {
                "status": "success",
                "argument": final_argument,
                "generation_time": end_time - start_time,
                "llm_calls": self.llm_calls,
                "rag_searches": self.rag_searches,
                "philosopher": self.philosopher_data.get('name', 'Unknown'),
                "rag_strategy": "heavy_mandatory"
            }
            
        except Exception as e:
            return {
                "status": "error",
                "message": str(e),
                "generation_time": time.time() - start_time,
                "llm_calls": self.llm_calls,
                "rag_searches": self.rag_searches
            }
    
    async def _integrate_all_rag_results(self, core_args: str, web1: str, web2: str, vec1: str, vec2: str, phil: str) -> str:
        """ëª¨ë“  RAG ê²°ê³¼ í†µí•©"""
        await asyncio.sleep(0.3)  # í†µí•© ì²˜ë¦¬ ì‹œê°„
        
        prompt = f"""
í•µì‹¬ ë…¼ì¦: {core_args}
ì›¹ ê²€ìƒ‰ 1: {web1}
ì›¹ ê²€ìƒ‰ 2: {web2}  
ë²¡í„° ê²€ìƒ‰ 1: {vec1}
ë²¡í„° ê²€ìƒ‰ 2: {vec2}
ì² í•™ì ê²€ìƒ‰: {phil}

ìœ„ ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ í†µí•©ëœ ë…¼ì¦ì„ ìƒì„±í•˜ì„¸ìš”.
"""
        
        response = await self._simple_llm_call(prompt)
        return response

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    demo = SmartRAGPerformanceDemo()
    
    print("ğŸ¯ ì§€ëŠ¥í˜• RAG vs ê¸°ì¡´ Heavy RAG ë°©ì‹ ì„±ëŠ¥ ë¹„êµ")
    print("="*120)
    print(f"â° ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
    scenarios = demo.get_test_scenarios()
    philosopher_key = 'nietzsche'  # ë‹ˆì²´ ì„ íƒ
    stance = 'pro'
    
    print(f"\nğŸ“š í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ({len(scenarios)}ê°œ):")
    for i, scenario in enumerate(scenarios, 1):
        print(f"   {i}. {scenario['name']}: {scenario['topic']}")
        print(f"      â†’ {scenario['reasoning']}")
    
    philosopher_name = demo.philosophers_data.get(philosopher_key, {}).get('name', philosopher_key)
    print(f"\nğŸ§  ì„ íƒëœ ì² í•™ì: {philosopher_name}")
    print(f"ğŸ“ RAG ì¹œí™”ë„: {demo.philosophers_data.get(philosopher_key, {}).get('rag_affinity', 0.5)}")
    
    print(f"\nğŸ”„ ê° ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„±ëŠ¥ ë¹„êµ ì‹œì‘...")
    
    smart_results = []
    traditional_results = []
    
    # ê° ì‹œë‚˜ë¦¬ì˜¤ë³„ë¡œ ë‘ ë°©ì‹ ë¹„êµ
    for i, scenario in enumerate(scenarios, 1):
        print(f"\n" + "="*80)
        print(f"ğŸ“‹ ì‹œë‚˜ë¦¬ì˜¤ {i}: {scenario['name']}")
        print(f"   ì£¼ì œ: {scenario['topic']}")
        print(f"   ì¹´í…Œê³ ë¦¬: {scenario['category']}")
        print("="*80)
        
        # ì§€ëŠ¥í˜• RAG ë°©ì‹
        smart_result = await demo.run_smart_rag_approach(scenario, philosopher_key, stance)
        smart_results.append(smart_result)
        
        # ê¸°ì¡´ Heavy RAG ë°©ì‹  
        traditional_result = await demo.run_traditional_heavy_rag_approach(scenario, philosopher_key, stance)
        traditional_results.append(traditional_result)
        
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ ë¹„êµ
        if smart_result.get('status') == 'success' and traditional_result.get('status') == 'success':
            smart_time = smart_result.get('generation_time', 0)
            trad_time = traditional_result.get('generation_time', 0)
            speedup = trad_time / smart_time if smart_time > 0 else 0
            
            print(f"\n   ğŸ“Š ì‹œë‚˜ë¦¬ì˜¤ {i} ê²°ê³¼:")
            print(f"      ğŸ§  ì§€ëŠ¥í˜•: {smart_time:.2f}ì´ˆ, LLM {smart_result.get('llm_calls', 0)}íšŒ")
            print(f"      ğŸŒ ê¸°ì¡´í˜•: {trad_time:.2f}ì´ˆ, LLM {traditional_result.get('llm_calls', 0)}íšŒ")
            print(f"      ğŸš€ ê°œì„ : {speedup:.1f}ë°° ë¹¨ë¼ì§")
    
    # ì¢…í•© ë¶„ì„ ê²°ê³¼
    demo.display_smart_rag_analysis(smart_results, traditional_results)
    
    print(f"\nâœ¨ ì§€ëŠ¥í˜• RAG vs Heavy RAG ë¹„êµ ì™„ë£Œ!")
    print(f"â° ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("\nğŸ¯ ê²°ë¡ : ì§€ëŠ¥í˜• RAGê°€ ìƒí™©ì— ë§ëŠ” íš¨ìœ¨ì  ê²€ìƒ‰ìœ¼ë¡œ ì„±ëŠ¥ê³¼ ë¹„ìš©ì„ ëª¨ë‘ ìµœì í™”í–ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    asyncio.run(main()) 